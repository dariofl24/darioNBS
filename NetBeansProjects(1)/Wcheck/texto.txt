#input{./title.tex}
$$ $$
#newpage
Este texto presenta un estudio detallado del art#'iculo #emph{Entanglement creation in low-energy scattering}, autor#'ia del Dr. Ricardo Weder Zaninovich, publicado en Diciembre de 2011 en la revista #emph{Physical Review} [1].##

El art#'iculo en cuesti#'on se centra en el estudio de la creaci#'on de entrelazamiento cu#'antico en procesos de dispersi#'on a bajas energ#'ias. Como es bien sabido, la dispersi#'on  es proceso din#'amico b#'asico cuyo estudio es esencial en todas las #'areas de la f#'isica, tanto cl#'asica como moderna, mientras que el entrelazamiento cu#'antico es la noci#'on central de las teor#'ias cu#'anticas modernas, en particular es la base fundamental de la teor#'ias de la informaci#'on y de la computaci#'on cu#'antica, dos #'areas de gran importancia para el futuro desarrollo tecnol#'ogico. ##

El entrelazamiento cu#'antico es un fen#'omeno que solo se da en sistemas compuestos (sistemas formados por dos o m#'as subsistemas cu#'anticos), por ejemplo como los que son descritos en [13]. Cuando se habla de entrelazamiento cu#'antico o de un sistema compuesto cu#'anticamente entrelazado, nos referimos a un sistema en el que entre sus componentes (los subsistemas que lo forman) se han creado correlaciones como resultado de las interacciones dentro del sistema completo, haciendo que sea imposible dar una descripci#'on del estado cu#'antico de cada uno de los subsistemas sin tomar en cuenta los estados de los dem#'as. De esta forma cada subsistema deja de ser #emph{cu#'anticamente independiente} incluso cuando la interacci#'on entre los subsistemas halla cesado y estos hallan sido separados f#'isicamente.##

En este texto vamos a estudiar creaci#'on de entrelazamiento cu#'antico en un sistema compuesto por dos part#'iculas sin spin, de masas $m_{1}$ y $m_{2}$ en el limite no relativista de bajas energ#'ias en tres dimensiones.## 
Para hacer esto, primero vamos a considerar que la interacci#'on entre las part#'iculas esta en funci#'on de la #emph{posici#'on relativa} entre ellas y que esta interacci#'on se hace despreciable cuando la #emph{separaci#'on entre las part#'iculas es grande}. Vamos a comenzar suponiendo que a tiempos en el pasado remoto (esto es tiempos en el limite $t #rar -#infty$) las part#'iculas se encuentran muy alejadas una de la otra, haciendo que la interacci#'on entre ellas sea despreciable y que por tanto las podamos considerar como sistemas cu#'anticos independientes, mas aun, en este punto podemos describir el estado cu#'antico de cada part#'icula a trav#'es de una soluci#'on apropiada de la ecuaci#'on libre de Schr#"odinger en tres dimensiones, de esta forma la din#'amica del sistema completo esta dada por el producto de estos dos estados independientes. Despu#'es, el sistema evoluciona, las part#'iculas se acercan, interact#'uan compartiendo informaci#'on cu#'antica entre ellas provocando que se entrelacen cu#'anticamente, para despu#'es dispersarse y #emph{alejarse}. Para tiempos en el futuro lejano (en el limite $t #rar #infty$) la interacci#'on entre las part#'iculas vuelve a ser despreciable y la din#'amica del sistema se puede describir por medio de una soluci#'on apropiada de la ecuaci#'on libre de Schr#"odinger en seis dimensiones, sin embargo en este punto al estar entrelazadas las part#'iculas debido al proceso de dispersi#'on, el estado del sistema completo ya no puede ser descrito por el producto de dos soluciones de la ecuaci#'on libre de Schr#"odinger en tres dimensiones. A este ultimo estado del sistema completo lo conocemos como el #emph{estado asint#'otico final libre} y es en donde realizaremos la #emph{medici#'on del entrelazamiento} que se creo entre las part#'iculas, a trav#'es de la pureza $P$ del sistema. Debemos hacer notar que los procesos de dispersi#'on son quiz#'a la manera mas simple de entrelazar cu#'anticamente a un conjunto de part#'iculas.##

Desde la proposici#'on de su existencia en 1935 ([2] y [3]) y aun despu#'es de su confirmaci#'on te#'orica en la d#'ecada de los sesenta [4] y de su observaci#'on experimental en las d#'ecadas de los setenta y ochenta ([5] y [6]), el entrelazamiento cu#'antico sigue siendo un tema controversial en la f#'isica y se puede debatir ampliamente acerca de su existencia o no. Sin embargo, este texto no pretende ser una discusi#'on acerca de esta controversia. En este trabajo enfocamos al entrelazamiento cu#'antico como un objeto mas de estudio de la f#'isica matem#'atica te#'orica, ya que como es bien sabido este efecto es hasta cierto punto te#'oricamente posible.##

Este texto se divide principalmente en tres partes: {#bf Preliminares}, {#bf Creaci#'on de entrelazamiento} y {#bf Ap#'endice}.##

En la primera parte #emph{Preliminares}, empezamos dando una #emph{cl#'asica} introducci#'on a las herramientas y conceptos b#'asicos que se utilizan en la f#'isica matem#'atica. En el primer capitulo comenzamos desde las definiciones fundamentales de espacios m#'etricos y la topolog#'ia que se genera en estos, para despu#'es introducir a los espacios normados completos o de Banach. Para estos #'ultimos espacios introducimos a los operadores lineales acotados y mostramos con todo detalle que el conjunto que forman estos es de nuevo un espacio de Banach, bajo la definici#'on de una norma apropiada.##

En el capitulo dos introducimos a los espacios con producto interno y estudiamos detalladamente como se puede generar una funci#'on norma a trav#'es de este producto, para despu#'es introducir a los espacios de Hilbert, los cuales son una clase de espacios de Banach. Los espacios de Hilbert tienen un papel fundamental en la mec#'anica cu#'antica ya que como se ve mas adelante en esta parte del texto, los estados cu#'anticos de un sistema se expresan en t#'erminos de funciones, las cuales son elementos de un espacio de Hilbert apropiado. En el resto de este capitulo nos dedicamos a estudiar las propiedades mas importantes de los espacios de Hilbert, a saber, centramos nuestra atenci#'on en lo que en muchos textos se refieren como #emph{la geometr#'ia de los espacios de Hilbert} ya que el concepto de producto escalar y la norma que surge de este, permite hacer algunas analog#'ias de car#'acter geom#'etrico con los espacios euclidianos de dimensi#'on finita (ya que los espacios de Hilbert pueden ser de dimensi#'on infinita). Algunas de estas propiedades #emph{geom#'etricas} son el #emph{Teorema de Pit#'agoras}, las desigualdades de Bessel y de Schwarz, la ley del paralelogramo, el teorema de proyecci#'on y el teorema de representaci#'on de Riez. Despu#'es continuamos el capitulo con un estudio detallado de las #emph{Bases ortonormales} en estos espacios, en donde establecemos que todo espacio de Hilbert tiene una base ortonormal, la cual puede llegar a ser un conjunto no numerable. Luego, introducimos a los #emph{espacios de Hilbert separables}, los cuales son los que tienen una base ortonormal a lo mas numerable.## 
Con los conceptos anteriores establecidos, concluimos este capitulo con el estudio del  producto tensorial de espacios de Hilbert, con lo cual mostramos que se puede generar un nuevo espacio de este tipo a trav#'es de del producto tensorial de los elementos de un conjunto de espacios de Hilbert. Esta parte de nuestro estudio de los espacios de Hilbert es de vital importancia, ya que mas adelante en el texto veremos que el espacio donde se encuentran las funciones que describen los estados cu#'anticos de un sistema formado por dos o mas part#'iculas, resulta ser el producto tensorial de los espacios de Hilbert donde se encuentran los estados individuales de cada part#'icula.##

En el capitulo tres nos dedicamos a desarrollar un ejemplo concreto de un espacio de Hilbert, a saber el espacio $L^{2}(#R^{n})$. El estudio de este espacio es importante ya que los estados cu#'anticos con los que trabajaremos se encuentran en los espacios $L^{2}(#R^{3})$ y $L^{2}(#R^{6})$. Comenzamos este capitulo considerando al conjunto $#L^{2}(#R^{n})$ el cual esta formado por todas las funciones medibles (en el sentido de Lebesgue) de $#R^{n}$ a $#C$, las cuales tienen la propiedad de que su modulo al cuadrado es integrable en el sentido de Lebesgue; es bien sabido que este conjunto resulta ser un espacio vectorial sobre $#C$. Despu#'es introducimos a la funci#'on $ ( #cdot ,#cdot ): #L^{2}(#R^{n}) #times #L^{2}(#R^{n}) #rar #R $, dada por:
$$ ( f,g )= #int_{#R^{n}} f(#x)#, #ol{g(#x)} #, d#x $$
Esta funci#'on se presta como candidata a ser un producto interno en el espacio $#L^{2}(#R^{n})$, sin embargo esta falla en ser #emph{positiva definida}. Para corregir esto mostramos detalladamente como generar un espacio de clases de equivalencia de funciones, identificando a las funciones de $#L^{2}(#R^{n})$ que son #emph{iguales en casi todo punto} en el sentido de la medida de Lebesgue. Despu#'es definimos al espacio $L^{2}(#R^{n})$ como este espacio de clases de equivalencia y vemos con todo detalle que la funci#'on: 
$$ ([f],[g])= ( f,g )= #int_{#R^{n}} f(#x)#, #ol{g(#x)} #, d#x $$
en donde $f,g #in #L^{2}(#R^{n})$ son los representantes de las clases de equivalencia $[f]$ y $[g]$ respectivamente, resulta ser un producto interno de donde obtenemos inmediatamente la definici#'on de una funci#'on norma. En el resto de este capitulo nos dedicamos a mostrar detalladamente que el espacio $L^{2}(#R^{n})$ es completo bajo la norma generada por el producto interno y por tanto es en efecto un espacio de Hilbert. As#'i tambi#'en, mostramos que $L^{2}(#R^{n})$ es de hecho un espacio separable  y que por tanto podemos encontrar una base ortonormal numerable para este.##
En la practica se suele tratar al espacio $L^{2}(#R^{n})$ como un espacio de funciones aunque formalmente es un espacio de Hilbert de clases de equivalencia. Sin embargo no hay ninguna ambiguedad en hacer esto ya que como hemos mencionado el producto escalar de dos clases de equivalencia se da en t#'erminos de dos representantes de estas, las cuales resultan ser funciones y ya que las dem#'as propiedades que hacen de $L^{2}(#R^{n})$ un espacio de Hilbert dependen de la definici#'on del producto escalar, podemos #emph{moderar} nuestra visi#'on de este espacio y trabajar con el como si se tratara de un espacio vectorial de dimensi#'on infinita cuyos elementos son funciones.##

El capitulo cuatro se dedica al estudio de las herramientas b#'asicas del an#'alisis de Fourier que se utilizan ampliamente en la mec#'anica cu#'antica. Comenzamos definiendo las #emph{sumas de Fourier} complejas para despu#'es introducir a las series de Fourier de funciones complejas de cuadrado integrable en intervalos de longitud finita $[-L,L]$. Despu#'es consideramos el caso limite en donde el intervalo $[-L,L]$ tiende a la recta real $( -#infty,#infty )$; haciendo esto vemos como las sumas involucradas en la serie de Fourier de una funci#'on suave se pueden comenzar a aproximar a trav#'es de integrales, con lo cual llegamos a la definici#'on de la #emph{transformada de Fourier} de una funci#'on suave, para despu#'es extender este concepto a funciones de cuadrado integrable. Una vez establecido lo anterior, damos la definici#'on de la transformada de Fourier para funciones complejas suaves con dominio en $#R^{n}$. Es aqu#'i en donde introducimos al #emph{espacio de momentos} y establecemos el teorema de inversi#'on de Fourier. A trav#'es del lema de #emph{Riemannn - Lebesgue} y del teorema de #emph{Fourier - Plancherel} establecemos algunas de las propiedades mas importantes de la transformada de Fourier que son de vital importancia en la teor#'ia cu#'antica. Concluimos este capitulo con la extensi#'on de la transformada de Fourier al espacio $L^{2}(#R^{n})$, el cual es el verdadero espacio en donde utilizaremos la teor#'ia desarrollada en este capitulo.##

En el capitulo cinco, con toda la teor#'ia matem#'atica establecida hasta este punto, comenzamos con la introducci#'on a la teor#'ia cu#'antica.##
Este capitulo comienza con una discusi#'on acerca de la dualidad onda - part#'icula. Para hacer esto lo mas claro posible, esta discusi#'on la llevamos a cabo sobre el marco hist#'orico del nacimiento de la mec#'anica cu#'antica, comenzando con un breve repaso acerca de los conceptos de onda y los fen#'omenos ondulatorios, para despu#'es discutir el efecto fotoel#'ectrico y como la explicaci#'on que se dio de este en el a#~no de 1905, se dio en t#'erminos de una nueva #emph{visi#'on} acerca de la naturaleza de los fen#'omenos ondulatorios, tales como la propagaci#'on de las ondas electromagn#'eticas. Despu#'es discutimos las #emph{ondas de materia}, introducidas por Louis de Broglie en 1924. Aqu#'i es donde discutimos por primera vez la dualidad onda part#'icula y como a un fen#'omeno corpuscular como la propagaci#'on de un haz de electrones se le puede asociar un fen#'omeno ondulatorio y hacemos notar que esta propuesta es el reciproco de la explicaci#'on del efecto fotoel#'ectrico. Despu#'es describimos el experimento de Davison y Germer, el cual da la confirmaci#'on experimental de las ondas de materia propuestas por Louis de Broglie. Luego, considerando ondas planas de la forma $u(#x,t)= #exp ( i #k #cdot #x - iwt )$ (con $#x #in #R^{n}$), establecemos la {#bf ecuaci#'on libre de Sch#"odinger} simplemente como una ecuaci#'on diferencial cuyas soluciones pueden ser dadas en t#'erminos de las ondas planas $u(#x,t)$. En el resto del capitulo introducimos los paquetes de ondas planas y discutimos la linealidad de la ecuaci#'on de Sch#"odinger e introducimos la #emph{distribuci#'on de momento} de una funci#'on de onda a trav#'es de su transformada de Fourier. El capitulo concluye con la interpretaci#'on f#'isica de la funci#'on de onda, a la cual interpretamos como la funci#'on cuyo modulo cuadrado da la #emph{densidad de probabilidad} de encontrar a la part#'icula que esta describe en un cierto punto del espacio. As#'i tambi#'en discutimos la #emph{condici#'on de normalizaci#'on} que deben de cumplir estas funciones para que su interpretaci#'on probabil#'istica quede establecida sin ambiguedad y establecemos como calcular la probabilidad de encontrar a una part#'icula en cierta regi#'on del espacio a un cierto tiempo.##

El capitulo seis comienza con una discusi#'on de lo que es el estado de un sistema en general para la f#'isica. Hacemos notar que el estado de un sistema f#'isico debe de ser una colecci#'on de propiedades que sean capaces de dar una descripci#'on completa del sistema, en el sentido de que con esta informaci#'on se pueda determinar el estado de este a todo tiempo, tanto en el pasado como en el futuro. Tambi#'en hacemos notar que el desarrollo de un sistema en el tiempo esta dado por una ley din#'amica; en la mec#'anica cl#'asica esta ley din#'amica esta dada por las ecuaciones de Hamilton, mientras que en la mec#'anica cu#'antica esta dada por la ecuaci#'on de Sch#"odinger cuyas soluciones van a resultar ser funciones que contienen toda la informaci#'on necesaria de un sistema cu#'antico para determinar su estado a todo tiempo. ##
Despu#'es en este capitulo introducimos el concepto de #emph{Observable}. Un observable es una cantidad f#'isica como la posici#'on, el momento y la energ#'ia, la cual se puede medir experimentalmente. Luego introducimos a los operadores de posici#'on y de momento con los cuales podemos extraer informaci#'on acerca de estas dos cantidades de las funciones que describen el estado de un sistema cu#'antico. As#'i tambi#'en, ya que lo anterior solo se da en t#'erminos de probabilidades definimos los #emph{valores esperados} o valores medios de un observable f#'isico. Luego introducimos la regla de sustituci#'on como una herramienta para obtener observables de estados cu#'anticos. Es aqu#'i en donde introducimos al Hamiltoniano cu#'antico para una particula, que en unidades f#'isicas tiene la forma:

$$ H= #frac{#P^{2}}{2m} +V(#x), $$
en donde $#mathbf{p}:= -i #hbar #nabla$ es el operador de momento, $#hbar$ es la #emph{constante de Plank} y $m$ es la masa de la particula. Este operador resulta ser el observable de la energ#'ia total de un sistema cu#'antico.##

En el capitulo siete, damos la definici#'on de #emph{funci#'on propia y valor propio} para operadores lineales en espacios de Hilbert. As#'i tambi#'en damos la relaci#'on que existe entre los observables f#'isicos y los operadores lineales, definimos a los #emph{estados propios} y discutimos como obtener los valores esperados de los observables descritos por operadores lineales. Luego, prestamos atenci#'on a las funciones propias del Hamiltoniano cu#'antico, para establecer a la ecuaci#'on de Sch#"odinger estacionaria y al problema de valores iniciales que esta implica, para despu#'es analizar la evoluci#'on temporal de estas funciones propias, a las cuales definimos como #emph{estados estacionarios} del sistema cu#'antico que describen.

En el capitulo ocho nos dedicamos a estudiar a los operadores lineales en espacios de Hilbert y a formalizar algunos de los conceptos introducidos en el capitulo anterior. Comenzamos estableciendo la {#bf ecuaci#'on general de Sch#"odinger} dependiente del tiempo:
$$ i #hbar #, #frac{#partial}{#partial t} #psi(t) = H#psi(t) $$
en donde $H$ es Hamiltoniano cu#'antico. Definimos a los operadores unitarios $U:H #rar H$, los cuales son operadores lineales acotados sobreyectivos con la caracter#'istica de que preservan la norma del elemento del espacio de Hilbert al que son aplicados:
$$ #| U #psi #|=#| #psi #|  #,#,,#, #forall #psi #in H $$
En esta parte hacemos notar que la transformada de Fourier es de hecho un operador unitario en el espacio de Hilbert $L^{2}(#R^{n})$.##
Luego, discutimos la evoluci#'on temporal de las soluciones de la ecuaci#'on general de Sch#"odinger dependiente del tiempo en t#'erminos de operadores unitarios. Para esto definimos el #emph{generador infinitesimal} para operadores de este tipo y damos la definici#'on de los #emph{grupos unitarios de un par#'ametro}. ##
Despu#'es definimos a los operadores sim#'etricos en espacios de Hilbert y estudiamos sus propiedades mas importantes, para despu#'es dar la definici#'on del adjunto de un operador. Con lo anterior establecido damos la definici#'on de los operadores #emph{autoadjuntos} y enunciamos el teorema de Stone, con lo que concluimos nuestra discusi#'on de la evoluci#'on temporal de los estados cu#'anticos. Terminamos este capitulo con la definici#'on formal de un {#bf Observable f#'isico} en el sentido de la mec#'anica cu#'antica. Un observable es un operador auto adjunto cuyo conjunto de funciones propias forma una base ortonormal del espacio de Hilbert en donde este act#'ua.##

En los cap#'itulos anteriores nos hemos dedicado a estudiar algunos de los aspectos matem#'aticos mas importantes que dan forma y solides a la teor#'ia cu#'antica, sin profundizar demasiado en el sentido f#'isico que estos tienen. En el capitulo nueve nos dedicaremos a crear la conexi#'on entre los aspectos matem#'aticos mas abstractos de la teor#'ia cu#'antica, con lo que en verdad se puede apreciar en el mundo real por medio de mediciones y experimentos. Esta conexi#'on queda establecida a trav#'es de los seis postulados de la mec#'anica cu#'antica. En estos se postula que a todo tiempo el estado de un sistema cu#'antico queda determinado por un elemento de un espacio de Hilbert apropiado (en nuestro caso en los espacios $L^{2}(#R^{n})$), que en un sistema cu#'antico las cantidades f#'isicas son descritas por operadores autoadjuntos que act#'uan en el espacio de Hilbert del sistema en cuesti#'on y que los #'unicos resultados posibles de las mediciones de estas cantidades son los valores propios de los operadores autoadjuntos.##
As#'i tambi#'en postulamos el #emph{Principio de descomposici#'on espectral}, la #emph{Reducci#'on de paquete de ondas} de un estado cu#'antico despu#'es de una medici#'on y postulamos finalmente a la ecuaci#'on general de Sch#"odinger dependiente del tiempo como la ley din#'amica de los sistemas cu#'anticos.##

Hasta este punto todo lo que se ha discutido acerca de la teor#'ia cu#'antica (con excepci#'on de los seis postulados) es valido para sistemas formados por una sola part#'icula. En los cap#'itulos diez, once y doce nos dedicamos principalmente a generalizar los conceptos desarrollados en los cap#'itulos anteriores para sistemas compuestos, en el caso particular de sistemas cu#'anticos formados por dos part#'iculas (sistemas binarios). Comenzamos estableciendo que el espacio de Hilbert de un sistema binario es el producto tensorial de los correspondientes espacios de Hilbert de las dos part#'iculas que lo forman. Establecemos la ecuaci#'on general de Sch#"odinger dependiente del tiempo, para este tipo de sistemas. As#'i tambi#'en introducimos el producto tensorial de operadores lineales para definir el concepto de observable para sistemas binarios.##

Luego nos dedicamos a la definici#'on del #emph{operador de densidad} y sus principales propiedades. Al mismo tiempo introducimos las nociones de #emph{entrelazamiento cu#'antico} y algunos conceptos del an#'alisis funcionas como los operadores de traza finita  para despu#'es estudiar como a trav#'es del operador de densidad podemos obtener informaci#'on acerca de los subsistemas que forman al sistema binario. Es en el capitulo doce en donde nos dedicamos al estudio de los estados puros y entrelazados. Aqu#'i definimos la #emph{forma normal} del vector de estado del sistema binario a trav#'es de la forma can#'onica del operador de densidad. Concluimos este capitulo con la definici#'on de la pureza del estado de un sistema binario. La pureza $P$ es una medida del entrelazamiento que existe en el estado de un sistema compuesto. Mostramos que la Pureza $P$ es igual a uno, si y solo si, el estado del sistema es no entrelazado y que es estrictamente menor que uno en cualquier otro caso. En este capitulo concluimos la parte de la introducci#'on que tiene que ver con los conceptos b#'asicos de la mec#'anica cu#'antica que utilizaremos en el resto del texto.##

En el capitulo trece comenzamos nuestra discusi#'on acerca de la teor#'ia de dispersi#'on. En este capitulo abordamos de manera breve pero general, las caracter#'isticas fundamentales de los sistemas f#'isicos de dispersi#'on. Discutimos como la dispersi#'on en sistemas de muchas part#'iculas se modela a trav#'es de la interacci#'on que existe entre estas y por medio de #emph{estados libres} tanto el pasado remoto como en el futuro distante, considerando que a estos tiempos la separaci#'on de las part#'iculas es tan grande que la interacci#'on entre estas es despreciable; es aqu#'i en donde obtenemos la primera formulaci#'on de la #emph{Hip#'otesis asint#'otica} sobre la cual desarrollaremos los conceptos de dispersi#'on en el sentido de la mec#'anica cu#'antica.##

Es en el capitulo catorce en donde comenzamos nuestra discusi#'on acerca de la teor#'ia cu#'antica de la dispersi#'on dependiente del tiempo. Comenzamos formulando la condici#'on asint#'otica en t#'erminos de la mec#'anica cu#'antica, a trav#'es de estados cu#'anticos libres (potencial igual a cero) y estados cu#'anticos perturbados (potencial diferente de cero). Es aqu#'i en donde definimos a los operadores de onda $#O_{#pm}$ y establecemos algunas de sus propiedades mas importantes para despu#'es poder introducir al operador de dispersi#'on sin ninguna ambiguedad; vemos que los operadores de onda son #emph{isometr#'ias parciales} y que su inversa esta dada por el adjunto de cada uno de estos, respectivamente; esto ultimo en general solo es valido cuando nos restringimos al subespacio cerrado $M_{#infty}(H_{0})$ de estados de dispersi#'on del sistema, esto es, el subespacio formado por todos los estados del Hamiltoniano libre que pueden representar asint#'oticamente en el tiempo a un estado del Hamiltoniano perturbado; estos #'ultimos estados forman al conjunto $M_{#infty}(H)$.##
Luego definimos al operador de dispersi#'on $#S = #O_{+}^{*} #O_{-}$ y vemos con todo detalle que este es tambi#'en una isometr#'ia parcial. En particular cuando los rangos de los operadores de onda $#O_{#pm}$ son iguales, diremos que nuestra teor#'ia es #emph{asint#'oticamente completa} y en este caso la restricci#'on del operador de dispersi#'on al subespacio $M_{#infty}(H_{0})$ resulta ser un operador unitario.##

En el capitulo quince retomamos el estudio de conceptos del an#'alisis funcional. Aqu#'i estudiamos la representaci#'on que se puede hacer de un cierto espacio de Hilbert $H$ a trav#'es de funciones medibles del tipo:
$$ f: #Lambda#subset #R #rar H_{0} $$
en donde $#Lambda$ es un sub conjunto medible (en el sentido de Lebesgue) y $H_{0}$ es un espacio de Hilbert apropiado.##
Vemos con todo detalle como el espacio de Hilbert $L^{2}(#R^{3})$ se puede descomponer en el espacio de funciones del tipo $ f:[0,#infty) #rar L^{2}(S^{2}) $ en donde $L^{2}(S^{2})$ es el espacio de todas las funciones de cuadrado integrable sobre la esfera unitaria del espacio $#R^{3}$. Hacemos esto con el objetivo de llegar a la descomposici#'on espectral del operador de dispersi#'on $#S$, con respecto a $#R^{+}$ y a $L^{2}(S^{2})$. La descomposici#'on espectral de este operador se puede ver como $ #{ #S(#lambda) #}_{#lambda #geq 0} $ en donde $#S(#lambda)$ es un operador acotado en particular parcialmente isom#'etrico, del espacio $L^{2}(S^{2})$, para cada $ #lambda #geq 0 $. A esta representaci#'on del operador $#S$, se conoce como la #emph{matriz de dispersi#'on}.##

La teor#'ia cu#'antica de dispersi#'on desarrollada en el capitulo catorce se puede aplicar de forma directa a un sistema cu#'antico de una sola part#'icula, la cual es dispersada por un potencial $V(#x)$. En el capitulo diecis#'eis mostramos como el problema de la dispersi#'on de un sistema cu#'antico formado por dos part#'iculas, las cuales interact#'uan mediante un potential que depende de la distancia relativa entre estas, puede ser reducido al problema de la dispersi#'on de una sola part#'icula, como en el capitulo catorce, simplemente haciendo el cambio de coordenadas al sistema de referencia del centro de masa de las part#'iculas , tal y como se hace en la teor#'ia cl#'asica de campo central. Aqu#'i cambiamos la factorizaci#'on del espacio de Hilbert del sistema de dos part#'iculas $H_{1} #pr H_{2}$ por una factorizaci#'on en t#'erminos del espacio de Hilbert del centro de masa del sistema y de la variable de posici#'on relativa $H_{CM} #pr H_{rel}$.##

En el capitulo diecisiete planteamos las condiciones del problema especifico que vamos a abordar en el resto del texto. Como ya hemos mencionado vamos a estudiar la creaci#'on de entrelazamiento cu#'antico en un proceso de dispersi#'on de dos part#'iculas sin spin, de masas $m_{1}$ y $m_{2}$ respectivamente. En este caso el espacio de configuraci#'on del sistema es $H=L^{2}(#R^{6})$ el cual se puede factorisar como $H=L^{2}(#R^{3}) #pr L^{2}(#R^{3})$. Vamos a considerar la ecuaci#'on de Sch#"odinger para un sistema de dos part#'iculas en #emph{unidades f#'isicas}:
$$ i #hbar #, #frac{#partial}{ #partial t } #psi (#x_{1} , #x_{2},t)= - #left( #frac{#hbar^{2}}{2m_{1}} #Delta_{1} + #frac{#hbar^{2}}{2m_{2}} #Delta_{2} #right) #, #psi (#x_{1},#x_{2},t)  + V(#x_{1}-#x_{2}) #psi $$
en donde el potencial $V$ depende del vector de posici#'on relativo de las dos part#'iculas $#x_{1}-#x_{2}$. Aqu#'i cambiamos al sistema de coordenadas del centro de masa de las dos part#'iculas y realizamos la factorizaci#'on del espacio de Hilbert $H=H_{CM} #pr H_{CM} =L^{2}(#R^{3}) #pr L^{2}(#R^{3})=L^{2}(#R^{6})$. Luego realizamos la factorizaci#'on del operador de dispersi#'on $#S = I_{cm} #pr #S_{rel}$, en donde $I_{cm}$ es el operador identidad del espacio de Hilbert del centro de masa $H_{CM}=L^{2}(#R^{3})$ y $#S_{rel}$ es el operador de dispersi#'on actuando sobre el espacio de Hilbert de la coordenada relativa $H_{rel}= L^{2}(#R^{3})$. Luego pasamos al espacio de momentos, en donde ahora este se factoriza en el espacio de Hilbert del momento del centro de masa del sistema y del momento relativo de las part#'iculas $#hat{H}= #hat{H}_{CM} #pr #hat{H}_{rel}$. En el espacio de momentos definimos al operador de dispersi#'on como:
$$ #hat{#S}= I_{CM} #pr #hat{#S}_{rel} = I_{CM} #pr #F#, #S_{rel} #F^{-1} $$
en donde $#F$ y $#F^{-1}$ denotan a la transformada de Fourier y a su inversa actuando en el espacio de Hilbert $#hat{H}_{rel}= L^{2}(#R^{3})$. En este espacio y en unidades f#'isicas, la matriz de dispersi#'on tiene toma la forma:
$$ #hat{#S}_{rel}= #{ #S(#P^{2}/2m) #} $$
con $#P$ el momento relativo y $m$ la masa reducida del sistema. Despu#'es ya que estamos interesados en el caso de dispersi#'on en el l#'imite de bajas energ#'ias (esto es, en el l#'imite cuando $|#P / #hbar| #rar 0$) enunciamos el teorema del #emph{Kato-Jensen}[18] el cual da una expansi#'on de la matriz de dispersi#'on en este limite, en t#'erminos de operadores de rango finito, bajo la condici#'on de que para el potencial $V(#x)$ ($#x #in #R^{3}$), del Hamiltoniano perturbado, exista una constante $#beta >0$, tal que $ (1+|#x|)^{#beta}#, V(#x) $ sea un operador compacto del espacio de Sobolev $H^{1}$ al espacio de Sobolev $H^{-1}$.##
La expansi#'on del operador de dispersi#'on de este teorema esta dada por:
$$ #S(#P^{2}/2m)= I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} + o(|#P/#hbar|^{2}) $$
para $#beta >5$. Si $#beta >7$, entonces $o(|#P/#hbar|^{2})$ se puede reemplazar por $O(|#P/#hbar|^{3})$.##
Si $#varphi(#P_{1} , #P_{2}) #in L^{2}(#R^{6})$ es un estado puro de un sistema de dos part#'iculas, su pureza esta dada por:
$$ P(#varphi)= #int_{#R^{12}} #varphi(#P_{1},#P_{2}) #, #varphi(#P_{1}',#P_{2}') #, #ol{#varphi(#P_{1}',#P_{2})} #, #ol{#varphi(#P_{1},#P_{2}')}#,#, d#P_{1}#,d#P_{2}#, d#P_{1}' #,d#P_{2}' $$
 
En la segunda parte del texto, bas#'andonos en la expansi#'on de la matriz de dispersi#'on del teorema de Kato - Jensen, derivamos una expresi#'on para la pureza con cota de error para la dispersi#'on de dos part#'iculas sin spin, cuyo potencial de interacci#'on no tiene necesariamente simetr#'ia esf#'erica, como suele suponerse en muchos casos de estudio de dispersi#'on. A saber, nuestra #'unica suposici#'on sobre el potencial $V(#x)$ (con $#x=#x_{1} - #x_{2}$ la coordenada relativa de las part#'iculas), es la misma que se utiliza en el teorema de #emph{Kato - Jensen}.##

Para hacer esto, comenzamos suponiendo que los estados asint#'oticos iniciales de las dos part#'iculas est#'an dados por un par de funciones Gaussianas normalizadas, con la misma varianza pero con momento promedio opuesto: 
$$ #varphi_{#P_{0}}(#P_{1})= #frac{1}{( #sigma^{2}#pi )^{3/4}}#, #exp( -( #P_{1}-#P_{0} )^{2} / 2 #sigma^{2} ) #,#,#,;#,#,#, #varphi_{-#P_{0}}(#P_{2})= #frac{1}{( #sigma^{2}#pi )^{3/4}}#, #exp( -( #P_{2}+#P_{0} )^{2} / 2 #sigma^{2} ) $$
de esta forma, el estado asint#'otico inicial del sistema esta dado por:
$$ #varphi_{in,#P_{0}}( #P_{1},#P_{2} )= #varphi_{#P_{0}}(#P_{1}) #, #varphi_{-#P_{0}}(#P_{2}) $$

Suponemos que el proceso de dispersi#'on se lleve a cabo en el origen al tiempo $t=0$ y por esta raz#'on la posici#'on promedio de las dos part#'iculas es cero en el estado asint#'otico inicial $ #F^{-1}#,#varphi_{in,#P_{0}}$. Una vez que el proceso de dispersi#'on termina, el estado de las part#'iculas queda determinado por el estado asint#'otico final $#varphi_{out,#P_{0}}(#P_{1} , #P_{2}) =#left[ #S(#P^{2} /2m ) #varphi_{in,#P_{0}}  #right] ( #P_{1},#P_{2} )  $. Despu#'es nos dedicamos a calcular la pureza del estado final $P(#varphi_{out,#P_{0}})$ para comprobar la creaci#'on de entrelazamiento entre las part#'iculas debido al proceso de dispersi#'on por el que han pasado. Sin embargo notamos que este estado final debe de ser entrelazado, ya que esta dado en t#'erminos de la matriz de dispersi#'on la cual depende directamente del momento relativo $#P$ del sistema. ##
Debemos mencionara que para estar en el limite de bajas energ#'ias, necesitamos que el momento relativo promedio $#P_{0}$ sea peque#~no, pero tambi#'en necesitamos que la varianza $#sigma$ de los estados Gaussianos iniciales tambi#'en sea peque#~na, ya que de los contrario el estado asint#'otico inicial $#varphi_{ in,#P_{0} }$ tendr#'ia una gran probabilidad de tener un momento grande, incluso si $#P_{0}$ tiene un valor peque#~no.##
Denotamos por $#varphi_{in}$ al estado asint#'otico inicial con momento relativo promedio $#P_{0}=0$, y de notamos $ #varphi_{out}=#S( #P^{2}/2m ) #varphi_{in} $, al estado asint#'otico de salida correspondiente. As#'i tambi#'en definimos:
$$ #mu_{1} = #frac{m_{1}}{m_{1}+m_{2}} #,#,#,;#,#,#, #mu_{2} = #frac{m_{2}}{m_{1}+m_{2}} $$
a las fracciones de la masa de las part#'iculas uno y dos con respecto a la masa total.##
En los teoremas 18.3 y 18.5 damos una demostraci#'on rigurosa de los siguientes resultados acerca de los t#'erminos de orden dominante de la pureza del estado final asint#'otico $#varphi_{out,#P_{0}}$, en el l#'imite de bajas energ#'ias:
$$ P (#vp_{out,#P_{0}})= P (#vp_{out})+O(|#P_{0}/#hbar|) $$
en donde $O(|#P_{0}/#hbar|)$ es uniforme con respecto a $#sigma$, para $#sigma$ en conjuntos acotados. M#'as aun:
$$ #P(#vp_{out})= 1 - #left(#frac{c_{0} #, #sigma}{#hbar} #right)^{2}#E(#mu_{1}) +#begin{cases} #begin{matrix} o(|#sigma/#hbar|^{2}) & si#; #beta >5 ## O(|#sigma/#hbar|^{3}) & si#; #beta > 7 #end{matrix} #end{cases} $$
en donde $#beta$ es la misma constante que aparece en la suposici#'on del potencial de interacci#'on, la cual controla como decae este en el infinito. $c_{0}$ es la #emph{longitud de dispersi#'on} la cual se define en la ecuaci#'on #eqref{scat-l}. $#E(#mu_{1})$ es el  #emph{coeficiente de entrelazamiento}, el cual esta dado por:
$$ #E(#mu_{1})= #frac{16}{#pi [1+(2#mu_{1}-1)^{2}]} + #frac{4}{(2#mu_{1}-1)^{2}} #cdot #frac{[1+(2#mu_{1}-1)^{2}]^{3/2} - 1}{#sqrt{1+(2#mu_{1}-1)^{2}}} $$
$$ - 8#, J(#mu_{1},1-#mu_{1}) - 8 #, J(1-#mu_{1},#mu_{1}) $$
con:
$$J(#mu_{1},#mu_{2})= $$
#begin{align}
#frac{1}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}}
|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, |#q_{1}+#q_{2}|^{2} - |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2}-#frac{|#q_{1}|^{2}}{2}#, #right]
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{2}}}
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#,d#q_{1} #right]^{2} d#q_{2} #nonumber
#end{align}

Observamos que $#E(#mu_{1})=#E(1-#mu_{1})$, ya que $P( #varphi_{out} )$ debe de ser invariante bajo el intercambio de las part#'iculas uno y dos. Tambi#'en observamos que no hay ning#'un termino de orden $#frac{#sigma}{#hbar}$ en la expresi#'on de $P(#varphi_{out})$.##

La tercera parte del texto es un ap#'endice en donde a trav#'es de c#'alculos expl#'icitos se establecen algunas ecuaciones que se utilizan en el capitulo de creaci#'on de entrelazamiento. Tambi#'en calculamos expl#'icitamente el valor de $J(1/2 , 1/2)$ y de $J(1,0)$.
As#'i tambi#'en, es en este ap#'endice en donde damos una generalizaci#'on del teorema de Kato - Jensen ya que este originalmente se enuncia para part#'iculas de masa $1/2$ en unidades en las que $#hbar=1$.


#part{Preliminares}

#chapter{Espacios m#'etricos y normados}
El ejemplo m#'as sencillo y natural de un espacio vectorial es sin duda $#R^{n}$, en donde es f#'acil e intuitivo el desarrollar conceptos geom#'etricos tales como la norma de un vector, el producto escalar  y la distancia entre puntos, de donde obtenemos las importantes nociones de limite, convergencia y continuidad. Sin embargo, estos conceptos no son #'unicos de $#R^{n}$ ya que es posible desarrollarlos sobre conjuntos y espacios vectoriales m#'as generales, incluso de dimensi#'on infinita.  En este capitulo comenzaremos estudiando las nociones de distancia y norma, en conjuntos y espacios vectoriales en general, mediante la introducci#'on de los espacios m#'etricos y los espacios normados.    

#vspace{10 mm}

#begin{definition}
Un espacio m#'etrico es un conjunto $M$ y una funci#'on real $ d(#cdot , #cdot) $ sobre $ M #times M $ que satisface:
#begin{enumerate}
#item $ d(x,y) #geq 0 $.
#item $ d(x,y)=0 $, si y solo si, $x=y$.
#item $ d(x,y)=d(y,x) $.
#item $ d(x,z) #leq d(x,y) + d(y,z) $ (desigualdad del triangulo).
#end{enumerate}
En donde a la funci#'on $d(#cdot , #cdot )$ se le conoce como la m#'etrica de $M$. 
#end{definition}

#n Hay que notar que un espacio m#'etrico es un conjunto $M$ junto con una funci#'on $d$. En general, un conjunto $X$ se puede convertir en un espacio m#'etrico de diferentes maneras, empleado diferentes m#'etricas. Cuando no sea del todo claro con que m#'etrica estamos trabajando en un cierto espacio m#'etrico $M$ usaremos la notaci#'on $ #la M , d #ra $ la cual muestra expl#'icitamente a la m#'etrica.##

#n Ahora introducimos el importante concepto de convergencia en espacios m#'etricos.

#begin{definition}
Se dice que una sucesi#'on de elementos $ #{x_{n}  #}_{n=1}^{#infty} $ de un espacio m#'etrico $#la M , d #ra$ converge a un elemento $x #in M$, si $ d(x,x_{n}) #rightarrow 0 $, cuando $ n #rar #infty $. Este hecho lo denotamos por,
$$ #lim_{n #rar #infty} x_{n}= x $$
#end{definition}
#n Un tipo muy importante de sucesiones en espacios m#'etricos son las sucesiones de Cauchy.
#begin{definition}
Una sucesi#'on de elementos $#{ x_{n} #}$ de un espacio m#'etrico $#la M , d #ra$ es llamada una {#bf sucesi#'on de Cauchy}, si para todo $#epsilon > 0$, existe $N #in #mathbb{N}$, tal que si, $ n,m #geq N $ ($n,m #in #mathbb{N}$) entonces, $d(x_{n},x_{m}) < #epsilon$.  
#end{definition}

#begin{proposition}
Toda sucesi#'on convergente es una sucesi#'on de Cauchy.
#end{proposition}
#n {#bf Demostraci#'on}##
Sea $#{ x_{n} #}$ una sucesi#'on de elementos de un espacio m#'etrico $#la M, d #ra$ tal que, converge a $x #in M$, entonces, dada $#epsilon >0 $, podemos encontrar un $ N #in #mathbb{N} $ tal que si, $ n #geq N $ entonces $ d(x_{n},x) < #frac{#epsilon}{2} $. Luego si $ m,n > N $, tenemos que, 
$$ d(x_{n},x_{m}) #leq d(x_{n},x) + d(x_{m},x) < #epsilon $$
Por tanto, $#{ x_{n} #}$ es una sucesi#'on de Cauchy. ## #qed

#vspace{5 mm}

Aunque toda sucesi#'on convergente es una sucesi#'on de Cauchy, el regreso de esta proposici#'on no es necesariamente cierto para todo espacio m#'etrico, como veremos a continuaci#'on. Sea $#mathbb{Q}$ el conjunto de los n#'umeros racionales con la m#'etrica $d(x,y)= |x-y|$ y sea $x^{*}$ un n#'umero irracional, es decir, $x^{*} #in #R #setminus #mathbb{Q}$. Luego, existe una sucesi#'on de racionales $#{x_{n}#}$ tal que, $#lim_{n #rar #infty} x_{n}= x^{*}$ en $#R$. Entonces $#{x_{n}#}$ es una sucesi#'on de Cauchy de n#'umeros en $#mathbb{Q}$ que no converge a ning#'un $y #in #mathbb{Q}$, ya que de suceder esto se tendr#'ia que, $#lim_{n #rar #infty} x_{n}= y$ en $#mathbb{Q}$ y $#lim_{n #rar #infty} x_{n}= x^{*}$ en $#R$ teniendo como consecuencia $x^{*}=y$.##
El ejemplo anterior motiva la introducci#'on de un tipo especial de espacios m#'etricos en donde toda sucesi#'on de Cauchy es convergente.
#begin{definition}
Un espacio m#'etrico en el que todas las sucesiones de Cauchy convergen es llamado {#bf espacio m#'etrico completo}.
#end{definition}

El ejemplo mas simple de un espacio m#'etrico completo es $#R^{n}$ (para un demostraci#'on rigurosa de este hecho nos referimos a [7]. Del hecho de que $#R$ es un espacio m#'etrico completo, es f#'acil ver que $#C$ tambi#'en lo es.

#begin{definition}
Un conjunto $B$ en un espacio m#'etrico $M$ es llamado {#bf Denso} si cada $m #in M$ es un limite de elementos de $B$. 
#end{definition}
 Por ejemplo $#R$ es un espacio m#'etrico completo mientras que $#mathbb{Q}$ no lo es. Este ejemplo de $#mathbb{Q}$ y $#R$ sugiere lo que se debe de hacer en un espacio m#'etrico incompleto $X$ para hacerlo completo. Es necesario agrandar a $X$ agregando todos los limites posibles de todas las sucesiones de Cauchy, as#'i el espacio original $X$ va a ser un conjunto denso dentro de un espacio m#'etrico completo m#'as grande $#tilde{X}$. Esto es posible hacerlo siempre que, el espacio m#'etrico incompleto se encuentre contenido en un espacio m#'etrico completo m#'as grande (como con $#mathbb{Q}$ y $#R$) en el caso de que esto no suceda, no es claro que significa #emph{el agregar todos los posibles limites}. El que sea posible completar todo espacio m#'etrico es el contenido de un teorema para el cual primero necesitamos establecer las siguientes definiciones:
#begin{definition}
Una funci#'on de un espacio m#'etrico $ #la X , d #ra $ a otro espacio m#'etrico $#la Y , #r #ra$ es llamada {#bf continua} en $x #in X$ si , $#r (f(x_{n}),f(x)) #rar 0$, siempre que, $ d(x_{n},x) #rar 0 $ cuando $n #rar #infty$.
#end{definition}

#begin{definition}
Una biyecci#'on $h$ de $ #la X,d #ra $ a $ #la Y , #r #ra $ que preserva la m#'etrica,
$$ #r (h(x),h(y)) = d(x,y) $$ 
es llamada una {#bf isometr#'ia} la cual es autom#'aticamente continua. Los espacios $#la X , d #ra$ y $#la Y , #r #ra$ son llamados {#bf isom#'etricos} si existe una isometr#'ia entre ellos.
#end{definition}

Los espacios isom#'etricos son esencialmente id#'enticos como espacios m#'etricos; cualquier teorema que solo tenga que ver con la estructura m#'etrica de $ #la X,d #ra  $ se va a cumplir en todos los espacios isom#'etricos a este.##
El siguiente teorema establece en que sentido un espacio m#'etrico incompleto puede ser agrandado a un espacio m#'etrico completo.
#begin{theorem}
Si $#la M , d #ra$ es un espacio m#'etrico incompleto, es posible encontrar un espacio m#'etrico completo $#tilde{M}$, tal que, $M$ es isom#'etrico a un sub conjunto denso de $#tilde{M}$.
#end{theorem}
#n {#bf Demostraci#'on (generalidades)}##
Consideremos a todas las sucesiones de Cauchy $#{ x_{n} #}$ de $M$. Digamos que dos sucesiones $#{x_{n}#}$, $#{y_{n}#}$ son equivalentes, si y solo si, $#lim_{n #rar #infty} d(x_{n},y_{n})=0$. Sea $#tilde{M}$ la familia de clases de equivalencia de las sucesiones de Cauchy bajo la relaci#'on de equivalencia anterior. Luego, para cualquiera dos sucesiones de Cauchy el limite $ #lim_{n #rar #infty} d(x_{n},y_{n}) $ existe y depende solo de la clase de equivalencia de $#{ x_{n} #}$ y $#{ y_{n} #}$. Este limite define una m#'etrica en $#tilde{M}$, luego $#tilde{M}$ es completo ya que toda sucesi#'on de Cauchy $ #{ #xi_{n} #} #subset #tilde{M} $,  en donde $ #xi_{n}= #left[ #{x_{j,n}#}_{j=1}^{#infty} #right] $, tiene como limite a la clase de equivalencia $ #xi= #left[ #{ x_{j,j} #}_{j=1}^{#infty} #right] $. Finalmente mapeamos a $M$ sobre $#tilde{M}$ asociando a cada $x #in M$ con la sucesi#'on constante $ #{x#} #in #tilde{M} $. Luego $M$ es denso en $#tilde{M}$ y el mapeo es isom#'etrico.## #qed

#vspace{5 mm}

Para completar esta discusi#'on sobre espacios m#'etricos, introduciremos las nociones de conjuntos abiertos y cerrados.
#begin{definition}
Sea $#la X,d #ra$ un espacio m#'etrico:
#begin{enumerate}
#item Un conjunto de la forma $#{ x #in X | d(x,y)<r #}$ es llamado una {#bf una bola abierta}, $B(y;r)$, de radio $r$ alrededor del punto $y$. 

#item Un conjunto $ O #subset X $ es llamado {#bf abierto}, si $#forall y #in O$, existe un  $ r >0$ tal que, $B(y;r) #subset O$.

#item Un conjunto $N #subset X$ es llamado una {#bf vecindad} de $y #in N$ si existe una bola $B(y;r) #subset N$ para alg#'un $r>0$.

#item Sea $E #subset X$. Un punto $x #in X$ es llamado un punto limite de $E$, si $#, #forall r >0$, $ B(x;r) #cap (E #setminus #{x#}) #neq #emptyset $, esto es, $x$ es un punto limite de $E$ si este conjunto contiene otros puntos diferentes de $x$ arbitrariamente cerca de $x$.

#item Un conjunto $F #subset X$ es llamado {#bf cerrado} si $F$ contiene a todos sus puntos limite.

#item Si $G #subset X$, $x #in G$ es llamado {#bf punto interior} de $G$, si existe $r>0$, tal que, $ B(x;r) #subset G $.

#end{enumerate}
#end{definition}  

#vspace{5 mm}

#n Los resultados que se establecen en el siguientes dos teoremas son consecuencia inmediata de los resultados anteriormente mencionados (ver [8]).

#vspace{5 mm}

#begin{theorem}
Sea $#la X , d #ra$ un espacio m#'etrico, entonces:
#begin{enumerate}
#item Un conjunto $O$ es abierto, si y solo si, $X #setminus O$ es cerrado.
#item $#lim_{m #rar #infty} x_{m}=x $, si y solo si, para toda vecindad $N$ de $x$ existe un $M #in #mathbb{N}$, tal que, $m #geq M$ implica, $x_{m} #in N$. 
#item El conjunto de puntos interiores de un conjunto es abierto.
#item La uni#'on de un conjunto $E$ con el conjunto de sus puntos limite, es un conjunto cerrado, denotado por $#overline{E}$ y es llamado la {#bf cerradura} de $E$.
#item Un conjunto es abierto, si y solo si, es una vecindad de cada uno de sus puntos.
#end{enumerate}
#end{theorem}

#vspace{5 mm}

#begin{theorem}
Una funci#'on $f(#cdot)$ de un espacio m#'etrico $X$ a otro espacio m#'etrico $Y$ es continua, si y solo si, para todo conjunto abierto $O #subset Y$, la imagen inversa $ f^{-1}[O] $ es un conjunto abierto.
#end{theorem}

#vspace{5 mm}
 
Completamos este capitulo con una discusi#'on acerca de dos conceptos centrales del an#'alisis funcional, los espacios normados y las transformaciones lineales acotadas.

#vspace{5 mm}

#begin{definition}
Un espacio normado es un espacio vectorial, $V$ sobre $#mathbb{C}$ (o $#R$) y una funci#'on $#| #cdot #|$, de $V$ a $#R$ que satisface:
#begin{enumerate}
#item $#| v #| #geq 0$ para todo $v #in V$.
#item $#| v #|=0$, si y solo si, $v=0$.
#item $#| #alpha#, v #|=|#alpha|#, #| v #|$, para todo $v #in V$ y $#alpha #in #mathbb{C}$ (o en $#R$).
#item $#| v+w #| #leq #| v #| + #| w #|$, para toda $v$ y $w$ en $V$.
#end{enumerate}
#end{definition}

#vspace{3 mm}

#n La desigualdad del triangulo implica de forma inmediata a la {#bf desigualdad del triangulo inversa}:
$$ #Big| #|u#| - #|v#| #Big|#leq #|u-v#| $$

#begin{definition}
Una {#bf transformaci#'on lineal acotada} u {#bf operador lineal acotado}, de un espacio normado $#la V_{1},#| #cdot #|_{1} #ra$ a otro espacio normado $#la V_{2},#| #cdot #|_{2} #ra$, es una funci#'on, $T$, de $V_{1}$ a $V_{2}$, que satisface:
#begin{enumerate}
#item $T( #alpha #, v + #beta #, w )= #alpha#, T(v) + #beta #, T(w)$, $#forall v,w #in V_{1}$, $#forall #alpha,#beta #in #mathbb{C}$ o $#R$.
#item Para alg#'un $#, C #geq 0$, $#| T(v) #|_{2} #leq C#,#| v #|_{1} $.
#end{enumerate} 
#end{definition}

#vspace{3 mm}

Al conjunto de todas las transformaciones lineales acotadas, entre los espacios normados $#la V_{1},#| #cdot #|_{1} #ra$ y $#la V_{2},#| #cdot #|_{2} #ra$, lo denotamos como $B(V_{1},V_{2})$.##

La constante $C$ mas peque#~na que satisface la segunda condici#'on de la definici#'on anterior, es conocida como la #emph{norma} de $T$ y la denotaremos como $#| T #|$:
#begin{equation}#label{ops-norm}
#| T #|= #sup_{ #|v#|_{1}=1 } #| Tv #|_{2}
#end{equation}
Mas aun, la funci#'on $ #| #cdot #| : B(V_{1},V_{2}) #rar [0,#infty) $, dada por la expresi#'on anterior, es de hecho una funci#'on norma.##

#begin{proposition}
Sean $#la V_{1},#| #cdot #|_{1} #ra$ y $#la V_{2},#| #cdot #|_{2} #ra$, espacios normados. Entonces el conjunto $B(V_{1},V_{2})$ es un espacio normado, bajo la norma #eqref{ops-norm}.
#end{proposition}
#n {#bf Demostraci#'on}##
Sin perdida de generalidad podemos suponer que $V_{1}$ y $V_{2}$ son espacios vectoriales sobre $#C$. Como es usual en la teor#'ia de funciones, definimos la suma de dos elementos $T_{1}$ y $T_{2}$ de $B(V_{1},V_{2})$ como la funci#'on que act#'ua de la forma:
$$ (T_{1}+T_{2})(v)= T_{1}(v) + T_{2}(v) $$
Y la multiplicaci#'on por escalar como:
$$ ( #gamma #, T)(v) = #gamma #, T(v)  $$
$ #forall v #in V_{1} $, $#forall #gamma #in #C$. Es inmediato ver que la funci#'on $#gamma #, T$ es lineal, ya que:
$$ ( #gamma #, T)(#alpha v + #beta w) = #gamma #, T(#alpha v + #beta w) =#gamma #,( #alpha #, T(v) + #beta #, T(w) ) = #alpha #,(#gamma#, T)(v) + #beta (#gamma #, T)(w) $$

La funci#'on, $#mathcal{O}: V_{1} #rar 0 $, en donde $0$ es el neutro aditivo del espacio $V_{2}$, esta en $B(V_{1},V_{2})$ ya que claramente es lineal y acotada, por tanto es un operador lineal. Mas aun, $#mathcal{O}$ es el neutro aditivo del espacio $B(V_{1},V_{2})$ ( $T+#mathcal{O} = T$, $ #forall T #in B(V_{1},V_{2}) $). Al operador $#mathcal{O}$ se le conoce com#'unmente como el #emph{operador lineal cero}.## 
Por la linealidad de $T_{1}$ y $T_{2}$, tenemos que:
$$ (T_{1}+T_{2})(#alpha v + #beta w)= T_{1}(#alpha v + #beta w) + T_{1}(#alpha v + #beta w) = ( #alpha #, T_{1}(v) + #beta #, T_{1}(w) )+( #alpha #, T_{2}(v) + #beta #, T_{2}(w) ) $$
$$= #alpha #,(T_{1}+T_{2})(v) + #beta #,(T_{1}+T_{2})(w) $$
$#forall v,w #in V_{1}$, $#forall #alpha,#beta #in #mathbb{C}$. as#'i, la funci#'on $T_{1}+T_{2}$ es lineal.##

Ahora tomemos $ #alpha_{0},#beta_{0} #in C $, luego para $T_{1},T_{2} #in #C$ tenemos que:
$$ #| (#alpha_{0}#,T_{1}+#beta_{0}#,T_{2} )(v) #|_{2}=#|#alpha_{0}#,T_{1}(v) + #beta_{0}#,T_{2}(v) #|_{2}$$ 
$$#leq |#alpha_{0}|#, #| T_{1}(v) #|_{2} + |#beta_{'}| #, #|T_{2}(v)#|_{2} #leq (|#alpha_{0}|#, #| T_{1} #| + |#beta_{0}| #, #|T_{2}#|) #, #|v#|_{1} $$
Para toda $v #in V_{1}$.## 
De esta forma la funci#'on $#alpha_{0}#,T_{1}+#beta_{0}#,T_{2} $ es acotada y por tanto $#alpha_{0}#,T_{1}+#beta_{0}#,T_{2} #in B(V_{1},V_{2})$, $ #forall #alpha_{0},#beta_{0} #in #C$, $ T_{1}, T_{2} #in B(V_{1},V_{2})$, lo que hace de $B(V_{1},V_{2})$ un espacio lineal.##

#n Ahora veamos que efectivamente la funci#'on $#| #cdot #| : B(V_{1},V_{2}) #rar [0,#infty)$ es una norma.##

#n Ya que $#la V_{2},#| #cdot #|_{2} #ra$ es un espacio normado, tenemos que para $ #hat{v} #in V_{1} $ con $ #| #hat{v} #|_{1}=1 $:
$$ 0 #leq #| T#hat{v} #|_{2} #leq #sup_{#|v#|_{1}=1} #|Tv#|_{2}=#|T#| $$
Por tanto $ 0 #leq #|T#| $, $ #forall T #in B(V_{1},V_{2}) $. Para el operador lineal cero, $#mathcal{O} #in B(V_{1},V_{2})$, y para toda $v #in V_{1}$, tenemos que $ #| #mathcal{O}(v) #|_{2}=0 $, por tanto es claro que:
$$ #| #mathcal{O} #|=0 $$
Por otro lado, supongamos que para un operador lineal $ T^{*} #in B(V_{1},V_{2}) $, tenemos que $ #|T^{*}#|=0 $, esto implica que:
$$ 0 #leq #| T^{*}v #|_{2} #leq #| T^{*} #| #, #|v#|_{1}=0 $$
as#'i, $#| T^{*}v #|_{2}=0$, $#forall v #in V_{1}$, pero ya que $#| #cdot #|_{2}$ es una norma, esto sucede, si y solo si, $ T^{*}v =0$ para toda $v #in V_{1}$ (en donde este ultimo $0$ denota al neutro aditivo de $V_{2}$). De esta forma tenemos que $T^{*}= #mathcal{O}$.##
Luego, concluimos que
$$ #| T #|=0 #, #sss #, T=#mathcal{O} $$

#n Ahora tomemos $#alpha #in #C$ y $T #in B(V_{1},V_{2})$. Luego, ya que $|#alpha| #geq 0$, tenemos que:
$$ #| #alpha #,T #|= #sup_{#|u#|_{1}=1}#| #alpha #, Tv #|_{2}= #sup_{#|u#|_{1}=1}|#alpha|#, #|Tv #|_{2} = |#alpha| #, #sup_{#|u#|_{1}=1} #|Tv #|_{2} = |#alpha|#, #|Tv#|_{2} $$

#n Tomemos ahora $T_{1},T_{2} #in B(V_{1},V_{2})$, luego ya que $#| #cdot #|_{2} #geq 0$:
$$ #| T_{1}+T_{2} #|= #sup_{#|u#|_{1}=1} #| (T_{1}+T_{2})(u) #|_{2} #leq #sup_{#|u#|_{1}=1} (#|T_{1}u#|_{2} + #|T_{2}u#|_{2}) $$
$$ #leq #sup_{#|u#|_{1}=1} #|T_{1}u#|_{2} + #sup_{#|u#|_{1}=1} #|T_{2}u#|_{2} =#|T_{1}#| + #|T_{2}#| $$

#n Y de esta forma concluimos que $B(V_{1},V_{2})$ es un espacio normado.
#qed

#vspace{5 mm}

#n Se debe observar que cualquier espacio normado $ #la V,#| #cdot #| #ra $ es un espacio m#'etrico, definiendo a la funci#'on distancia como, $ d(v,w)= #| v-w #| $; cuando en un espacio normado la m#'etrica este dada de esta forma la identificaremos como la {#bf m#'etrica inducida}. 

#begin{proposition}
En un espacio normado $#la X, #| #cdot #| #ra$ la funci#'on definida por, $ d(x,y):= #|x-y #| $, es una funci#'on distancia.
#end{proposition}
#n {#bf Demostraci#'on}##
Ya que $#| #cdot #|$ es una norma, tenemos:
#begin{enumerate}
#item $ d(x,y)=#| x-y #|#geq 0 $.
#item $ d(x,y)=#| x-y #|=0$ $#sss$ $ x-y=0 $ $#sss$ $x= y$.
#item $ d(x,y)=#| x-y #|= |-1|#, #| y-x #|=#| y-x #|= d(y,x) $.
#item $ d(x,y)=#| x-y #|=#| x -z+z -y #| #leq #|x-z#| + #| z-y #|=d(x,z)+d(z,y) $.
#end{enumerate}
#qed

#vspace{5 mm}

Hay entonces una noci#'on de continuidad de funciones en los espacios normados y para las funciones lineales, esta propiedad es capturada precisamente por los operadores lineales acotados. 
#begin{theorem}#label{T-ac-con}
Sea $T$ una transformaci#'on lineal entre dos espacios normados. Los siguientes enunciados son equivalentes:
#begin{enumerate}
#item $T$ es continua en un punto.
#item $T$ es continua en todo punto.
#item $T$ es acotada.
#end{enumerate}
#end{theorem}
#n {#bf Demostraci#'on}##
$ 1) #Rightarrow 2) : $##
Supongamos que $T$ es continua en el punto $x_{0}$ del espacio normado $ #la X, #| #cdot #|_{1} #ra $. Sea $#{x_{n}#} #subset X $ una sucesi#'on tal que, $ x_{n} #rar x_{0} $, entonces dado $#epsilon >0$, existe $ N #in #mathbb{N} $, tal que si $n  #geq N$, 
$$ #| T(x_{n})-T(x_{0}) #|_{2} < #epsilon $$ 
Sea $#hat{x} #in X$, entonces existe $ x^{*} #in X $, tal que, $ #hat{x}= x_{0}+x^{*} $. as#'i tenemos que, $ x_{n}+x^{*} #rar x_{0}+x^{*}=#hat{x} $. Luego,
$$ #| T(x_{n}+x^{*}) - T(#hat{x}) #|_{2}= #| T(x_{n})+T(x^{*}) - T(x^{*})-T(x_{0}) #|_{2}= #| T(x_{n}) - T(x_{0}) #|_{2} < #epsilon $$
para $n #geq N$. Ahora, si $#{y_{n}#}$ es una sucesi#'on, tal que $ y_{n} #rar #hat{x} $, tenemos que, $ y_{n}-x^{*} #rightarrow #hat{x} - x^{*} = x_{0} $. as#'i, por el argumento anterior, dada $#epsilon >0$, existe $ N #in #mathbb{N} $, tal que,
$$ #| T(y_{n}) - T(#hat{x}) #|_{2}= #| T((y_{n}-x_{0})+x_{0}) - T(#hat{x}) #|_{2} < #epsilon $$
para $ n #geq N $. Por tanto, ya que $#hat{x} #in X$ es arbitrario, $T$ es continua en $X$. ##
Es evidente que $ 2) #Rightarrow 1)  $.##

#n $ 2) #Rightarrow 3) : $##
Supongamos que $T$ no es acotada. Entonces existe una sucesi#'on $#{ u_{n} #} #subset X$, con $#| u_{n} #|_{1}=1$, tal que, $ #| Tu_{n} #|_{2} > n $, $ #forall n #in #mathbb{N} $. Luego la sucesi#'on $#left#{ #frac{1}{n}#, u_{n}#right#}$, converge a cero, ya que, $ #left#| #frac{1}{n}#, u_{n} #right#|_{1}= #frac{1}{n} $, pero:
$$ #left#| T #left( #frac{1}{n}#, u_{n} #right) #right#|_{2}= #frac{1}{n} #left#| T #left( u_{n} #right) #right#|_{2}> 1 $$
$ #forall n #in #mathbb{N} $, lo cual contradice la continuidad de $T$. Por tanto $T$ es acotada.##

#n $ 3) #Rightarrow 2) : $##
Sup#'onganos que $ x_{n} #rar x $. as#'i, dado $#epsilon > 0 $ existe $n #in #mathbb{N}$, tal que, $ #| x_{n}-x #| < #epsilon/#|T#| $, para $ n > N $. Luego ya que $T$ es acotada,
$$ #| T(x_{n})-T(x) #|_{2}= #| T(x_{n}-x) #|_{2} #leq #|T#|#cdot #|x_{n}-x#|_{1} < #epsilon$$
$ #forall n > N $. Por tanto, $T$ es continua en $X$. ## #qed

#vspace{5 mm}

De la desigualdad del triangulo inversa es f#'acil ver que la norma $#| #cdot #|$, es una funci#'on continua en el espacio normado $#la V, #| #cdot #| #ra$.##

as#'i tambi#'en, la m#'etrica inducida proporciona la noci#'on de espacio completo en los espacios normados.##

#begin{definition}
Decimos que un espacio normado $#la V, #| #cdot #| #ra$ es {#bf completo}, si este es completo como espacio m#'etrico bajo la m#'etrica inducida, $d(x,y)=#| x-y #|$.
#end{definition}

#n Un espacio normado completo es conocido como un {#bf espacio de Banach}. En particular, el conjunto de operadores lineales acotados entre los espacios de Banach $V_{1}$ y $V_{2}$, $B(V_{1},V_{2})$ es un espacio de Banach bajo la norma de los operadores definida en #eqref{ops-norm}.##

#begin{proposition}
Sea $#la V_{1},#| #cdot #|_{1} #ra$ un espacio normado y sea $#la V_{2},#| #cdot #|_{2} #ra$ un espacio de Banach. Entonces el espacio de operadores lineales acotados, $B(V_{1},V_{2})$ es un espacio de Banach, bajo la norma #eqref{ops-norm}.
#end{proposition}
#n {#bf Demostraci#'on}##
Sea $#{ T_{n} #}_{n=1}^{#infty} #subset B(V_{1},V_{2})$, una sucesi#'on de Cauchy de operadores lineales acotados. Entonces, para $#epsilon >0$, existe $N #in #mathbb{N}$, tal que:
#begin{equation}#label{p-nn}
#| T_{m}-T_{n} #| < #epsilon
#end{equation}
$#forall m,n > N$. Luego, para $v #in V_{1}$ lo anterior implica que:
#begin{equation}#label{p1-nn}
#| T_{m}v - T_{n}v #|_{2} #leq #| T_{m}-T_{n} #| #cdot #|v#|_{1} < #epsilon #, #|v#|_{1}
#end{equation}
$#forall m,n > N$. Esto implica que la sucesi#'on $#{ T_{n}v #}_{n=1}^{#infty}$, es de Cauchy en $V_{2}$. Ya que $V_{2}$ es de Banach, existe un #'unico elemento en este espacio, al cual la sucesi#'on anterior converge; denotamos a este elemento como $Tv$. Pero ya que esto sucede para todo $v #in V_{1}$, definimos a la funci#'on $T: V_{1} #rar V_{2}$ como:
$$ Tv= #lim_{n #rar #infty} T_{n}v $$
Por la linealidad del limite, es f#'acil ver que:
$$ T(#alpha v + #beta u)= #lim_{n #rar #infty} T_{n}(#alpha v + #beta u) = #lim_{n #rar #infty} #left( #alpha #, T_{n}v + #beta #, T_{n}u #right)$$ 
$$= #alpha #, #lim_{n #rar #infty} T_{n} v + #beta #, #lim_{n #rar #infty} T_{n}u = #alpha #, Tv + #beta #, Tu $$
$ #forall u,v #in V_{1}$, $#forall #alpha , #beta #in #C$. Por tanto $T$ es una funci#'on lineal. La expresi#'on #eqref{p1-nn} y la desigualdad del triangulo inversa implican que:
$$ #Big|#, #|T_{m}v#|_{2} - #|T_{n}v#|_{2}  #,#Big| #leq #epsilon #, #|v#|_{1} #,#,,#,#, #forall n,m > N #,,#, #forall v #in V_{1} $$
tomando el limite cuando $m #rar #infty$ y tomando el hecho de que la norma $ #| #cdot #|_{2} $ es una funci#'on continua, tenemos que:
$$0 #leq #Big|#, #|Tv#|_{2} - #|T_{n}v#|_{2}  #,#Big| #leq #epsilon #, #|v#|_{1} #,#,,#,#,#, #forall n > N #,,#, #forall v #in V_{1} $$
Luego,
$$0 #leq #|Tv#|_{2} #leq #epsilon #, #|v#|_{1} + #|T_{n}v#|_{2} #leq (#epsilon + #|T_{n}#|)#, #|v#|_{1} #,#,,#, #forall n > N #,,#, #forall v #in V_{1} $$
Por tanto $T #in B(V_{1},V_{2})$. De esta forma, tomando la expresi#'on #eqref{p-nn}, el limite cuando $m #rar #infty$ y el hecho de que la norma $#| #cdot #|$ es una funci#'on continua, tenemos que:
$$ #| T - T_{n} #| < #epsilon $$
$#forall n > N$. Por tanto:
$$ #lim_{n #rar #infty} T_{n} = T $$
en la norma del espacio de operadores. De esta forma $B(V_{1},V_{2})$ es un espacio de Banach. ##
#qed

#vspace{5 mm}
 
Si $#la X,#| #cdot #| #ra$ es un espacio normado, sabemos que $X$ se puede completar como espacio m#'etrico por medio de la m#'etrica inducida. Usando el hecho de que $X$ es denso en $#tilde{X}$ es f#'acil ver a trav#'es del siguiente teorema y su demostraci#'on, que los operadores acotados del espacio $X$ se pueden extender naturalmente y de una sola manera, en operadores acotados (y con la misma cota) del espacio completo $#tilde{X}$.

#begin{theorem}
Sea $T$ un operador acotado de un espacio normado $ #la V_{1},#| #cdot #|_{1} #ra $ a un espacio normado completo (espacio de Banach) $ #la V_{2},#| #cdot #|_{2} #ra $. Entonces $T$ puede ser extendido de manera #'unica a un operador acotado (con la misma cota), $#tilde{T}$, del espacio m#'etrico completo $#tilde{V}_{1}$ a $#la V_{2},#| #cdot #|_{2} #ra$.
#end{theorem}
#n {#bf Demostraci#'on}##
Sea $#tilde{V}_{1}$ el espacio m#'etrico completo que proviene de completar $V_{1}$. Para cada $x #in #tilde{V}_{1}$ hay una sucesi#'on de elementos $#{x_{n}#} #subset V_{1}$ tal que, $ x_{n} #rar x $, cuando $ n #rar #infty $. Ya que $x_{n}$ converge, esta es de Cauchy, entonces, dado $#epsilon >0$, existe $N #in #mathbb{N}$, tal que si $ n,m > N $ implica, $ #| x_{n}-x_{m} #|_{1} < #epsilon / #|T#| $. Luego, 
$$ #| Tx_{n}-Tx_{m} #|_{2}=#| T(x_{n}-x_{m}) #| #leq #|T#| #cdot #| x_{n}-x_{m} #|_{1} < #epsilon $$
Esto muestra que $#{Tx_{n}#} $ es una sucesi#'on de Cauchy en $V_{2}$ y ya que este espacio es completo, existe $ y #in V_{2} $, tal que, $ Tx_{n} #rar y $. Definamos $ #tilde{T}x=y $. Mostremos que esta definici#'on es independiente de la sucesi#'on $ x_{n} #rar x $. Si $x_{n} #rar x $ y $x'_{n} #rar x $, entonces la sucesi#'on $ x_{1},x'_{1},x_{2},x'_{2},x_{3},x'_{3},... #rar x $, luego $ Tx_{1},Tx'_{1},Tx_{2},Tx'_{2},Tx_{3},Tx'_{3},... #rar #hat{y}$, para alg#'un $#hat{y} #in V_{2}$ por el argumento anterior. Entonces,
$$ #lim_{n #rar #infty}Tx'_{n}= #hat{y}= #lim_{n #rar #infty}Tx_{n}= y$$
Mas aun, $#tilde{T}$ es acotada con modulo $#|T#|$, ya que,
$$ #| #tilde{T}x #|_{2}=#lim_{n #rar #infty} #| Tx_{n} #|_{2} #leq #overline{#lim_{n #rar #infty}} #|T#| #cdot #|x_{n}#|_{1}= #|T#|#cdot #|x#|_{1} $$ 
Es claro que de la definici#'on de $#tilde{T}$,
$$ #tilde{T}(#alpha #, x)=#lim_{n #rar #infty} T(#alpha#, x_{n}) = #alpha #, #lim_{n #rar #infty} T( x_{n})= #alpha #, #tilde{T}(x) $$
por tanto $#tilde{T}$ es lineal. Mas aun $#tilde{T}$ es #'unica, ya que si, $ #hat{T}(x)= #lim_{n #rar #infty} Tx_{n} $, en donde $ x_{n} #rar x $, tenemos que,
$$ #| #tilde{T}(x) - #hat{T}(x) #| = #| #tilde{T}(x)-T(x_{n})+T(x_{n})-#hat{T}(x) #|#leq #| #tilde{T}(x)-T(x_{n})#| + #|#hat{T}(x)-T(x_{n}) #| #rar 0 $$
cuando $n #rar #infty$. ## #qed

#vspace{5 mm}

Para concluir esta secci#'on y nuestra discusi#'on acerca de los operadores acotados en espacios normados, introduciremos dos diferentes nociones de convergencia de una sucesi#'on de operadores.

#begin{definition}
Sea $#{ T_{n} #}$ una sucesi#'on de operadores acotados entre los espacios de Banach,## $#la V_{1},#| #cdot #|_{1} #ra $ y $ #la V_{2},#| #cdot #|_{2} #ra $. Diremos que la sucesi#'on $#{ T_{n} #}$ {#bf converge fuertemente} al operador acotado $T$ (T: $ V_{1} #rar V_{2} $), si para cada $x #in V_{1}$ la sucesi#'on de vectores $#{ T_{n}x #}$ converge en $V_{2}$ al vector $Tx$, esto es:
#begin{equation}#label{s-conv}
#lim_{n #rar #infty} #| T_{n}x - Tx #|_{2} =0 #,#,,#,#,#,#, #forall x #in V_{1}
#end{equation}
a este tipo de convergencia la denotamos simplemente como, $T_{n} #rar T$ o de forma mas precisa:
#begin{equation}#label{s-lim}
T = #slim_{n #rar #infty} T_{n}
#end{equation}
#end{definition}

#vspace{3 mm}

La noci#'on de convergencia fuerte para una sucesi#'on de operadores es bastante general, en particular esta se puede utilizar tambi#'en en el caso de los operadores no acotados, los cuales no est#'an definidos necesariamente en todo el espacio normado $V_{1}$. En este caso solo hay que considerar a los vectores que est#'en  en la intersecci#'on de todos los operadores $T_{n}$ y del operador $T$.

#begin{definition}
Sea $#{ T_{n} #}$ una sucesi#'on de operadores acotados entre los espacios de Banach,## $#la V_{1},#| #cdot #|_{1} #ra $ y $ #la V_{2},#| #cdot #|_{2} #ra $. Diremos que la sucesi#'on $#{ T_{n} #}$ {#bf converge uniformemente} al operador acotado $T$ (T: $ V_{1} #rar V_{2} $), si esta converge en la norma $#| #cdot #|$ del espacio de Banach operadores acotados (ec. #eqref{ops-norm}), es esto es:
#begin{equation}
#lim_{n #rar #infty} #| T_{n} - T #| =0 
#end{equation}
a este tipo de convergencia la denotamos como:
#begin{equation}
T= #ulim_{n #rar #infty} T_{n} 
#end{equation}
#end{definition}  

#vspace{3 mm}

Es f#'acil ver que si una sucesi#'on de operadores acotados $#{T_{n}#}$, ($ T_{n}:V_{1} #rar V_{2} $, $#forall n $) converge uniformemente a al operador $T$, entonces esta tambi#'en converge fuertemente, ya que:
$$ #| Tx - T_{n}x #|_{2} #leq #|  T-T_{n} #| #cdot #| x #|_{1} #rar 0 $$
cuando $ n #rar #infty $, $ #forall x #in V_{1} $.##

%newpage

#chapter{Espacios de Hilbert}
En el capitulo anterior hemos dado una generalizaci#'on de los conceptos de distancia, limite, continuidad y norma para espacios vectoriales, gracias a la introducci#'on de los espacios m#'etricos y los espacios normados. Existen sin embargo, seg#'un el algebra lineal, propiedades lineales, m#'etricas y geom#'etricas de los espacios vectoriales de dimensi#'on finita, cuya generalizaci#'on a espacios vectoriales de dimensi#'on infinita es vital para dar un fundamento matem#'atico solido a teor#'ias f#'isicas basadas por completo en espacios de este tipo, tal como sucede con la mec#'anica cu#'antica basada en espacios de funciones de cuadrado integrable.## 
En este capitulo estudiaremos espacios vectoriales con producto interno, una generalizaci#'on del producto punto usual en espacios vectoriales de dimensi#'on finita. Las propiedades geom#'etricas de estos espacios se siguen de la noci#'on de #'angulo, la cual esta impl#'icita en la definici#'on del producto interno.

#vspace{10 mm}

#begin{definition}
Un espacio vectorial $V$ sobre $#C$ es llamado un {#bf espacio con producto interno} si existe una funci#'on compleja $(#cdot,#cdot)$  en $V #times V$ que satisfaga las siguientes cuatro condiciones para todo $ x,y,z #in V $ y $#alpha #in #C$.
#begin{enumerate}
#item $(x,x) #geq 0$ y $ (x,x)=0 $, si y solo si, $x=0$ (positiva definida).
#item $(x,y+z)=(x,y)+(x,z)$.
#item $ ( x,#alpha #, y )= #alpha #, (x,y) $.
#item $ (x,y)= #overline{(y,x)} $ (anti linealidad).
#end{enumerate}
La funci#'on $ (#cdot,#cdot) $ es llamada un {#bf producto interno}.
#end{definition}
Se debe notar que las condiciones $2)$, $3)$, y $4)$ de la definici#'on anterior, implican que, 
$$ (x,#alpha y+#beta z)= #alpha (x,y) + #beta (x,z) $$ 
y que, 
$$ ( #alpha#, x,y )= #overline{#alpha}#, (x,y) $$
De la linealidad de las entradas del producto interno, es f#'acil ver que:
$$ (x,0)=(0,x)=0 #,#,#,#,#, #forall x #in V $$
en donde $0$, dentro del producto interno se refiere a la neutro aditivo de $V$ y $0$ en el lado derecho de la igualdad, es el $0$ de $#C$. No importando en que campo estemos trabajando, al neutro aditivo de este siempre lo podemos denotar como $0$ sin ambiguedad.

#n En la siguiente definici#'on se establece la primera noci#'on geom#'etrica de los espacios vectoriales con producto interno.

#begin{definition}
Se dice que dos vectores $x$, $y$ de un espacio con producto interno $V$, son {#bf ortogonales} si $(x,y)=0$. Una colecci#'on de vectores $#{x_{i}#}$ de $V$ es llamada un {#bf conjunto ortonormal} si $( x_{i},x_{i} )=1$ para toda $i$, $(x_{i},x_{j}) =0$ si $ i #neq j $.
#end{definition}

Por el momento denotaremos por $#|x#| := #sqrt{(x,x)}$ y mas adelante mostraremos que en efecto, $ #| #cdot #| $ es una norma.

#begin{theorem}
{#bf (Teorema de Pit#'agoras)}  Sea $#{x_{n}#}_{n=1}^{N}$ un conjunto ortonormal en un espacio con producto interno $V$. Entonces para toda $x #in V$,
$$ #|x#|^{2}= #sum_{n=1}^{N} |(x,x_{n})|^{2} + #left#| x - #sum_{n=1}^{N} (x_{n},x)x_{n} #right#|^{2} $$
#end{theorem} 
#n {#bf Demostraci#'on}##
Escribimos a $x$ como,
$$ x= #sum_{n=1}^{N} |(x,x_{n})|^{2}#, x_{n} + #left( x - #sum_{n=1}^{N} |(x,x_{n})|^{2}#, x_{n} #right) $$
Luego, tenemos que
$$ #left(#sum_{n=1}^{N} (x_{n},x)#,x_{n}#, , #, x - #sum_{n=1}^{N}(x_{n},x)#,x_{n}  #right)= #left( #sum_{n=1}^{N} (x_{n},x)#,x_{n}#, , #, x #right) - #left( #sum_{n=1}^{N} (x_{n},x)#,x_{n}#, , #, #sum_{n=1}^{N} (x_{n},x)#,x_{n} #right) $$
$$ = #sum_{n=1}^{N} #overline{(x_{n},x)}#, (x_{n},x) - #sum_{n=1}^{N} #overline{(x_{n},x)}#, (x_{n},x)=0 $$
por tanto, $#sum_{n=1}^{N} (x_{n},x)#,x_{n}$ y $ x - #sum_{n=1}^{N} |(x,x_{n})|^{2}#, x_{n} $, son ortogonales.##
Entonces,
$$ #|x#|^{2}= (x,x)= #left#| #sum_{n=1}^{N} |(x,x_{n})|^{2}#, x_{n} #right#|^{2} + #left#| x - #sum_{n=1}^{N} |(x,x_{n})|^{2}#, x_{n} #right#|^{2} $$
$$ = #sum_{n=1}^{N} |(x_{n},x)|^{2} + #left#| x - #sum_{n=1}^{N} |(x,x_{n})|^{2}#, x_{n} #right#|^{2} $$
## #qed

#vspace{5 mm}

#n Ya que la norma de un vector es siempre un n#'umero positivo es f#'acil ver el siguiente resultado.

#begin{corollary}
{#bf (Desigualdad de Bessel)} Sea $#{x_{n}#}_{n=1}^{N}$ un conjunto ortonormal de un espacio con producto interno $V$. Entonces para toda $x #in V$,
$$ #| x #|^{2} #geq #sum_{n=1}^{N} |(x_{n},x)|^{2} $$ 
#end{corollary}

#begin{corollary}
{#bf (Desigualdad de Schwarz)} Sea $V$ un espacio con producto interno y sean $x,y #in V$. Entonces,
$$ |(x,y)| #leq #|x#| #, #|y#| $$ 
#end{corollary}
#n {#bf Demostraci#'on}##
El caso $y=0$ es inmediato, entonces supongamos que $y #neq 0$. El vector $ #frac{y}{#|y#|} $ forma un conjunto ortonormal por si mismo, entonces aplicando la desigualdad de Bessel a cualquier $x #in V$ obtenemos,
$$ #|x#|^{2} #geq  #left| #left( x , #frac{y}{#|y#|} #right) #right|^{2}= #frac{|(x,y)|^{2}}{#|y#|^{2}} $$
de donde obtenemos, $ |(x,y)| #leq #|x#| #, #|y#| $.## #qed

#vspace{5 mm}

#n Es interesante ver cuando la desigualdad de Schwarz se hace una igualdad.
#begin{proposition}#label{scheq}
Sea $V$ un espacio con producto interno y sean $x,y #in V$. Entonces,
$$ |(x,y)| = #|x#| #, #|y#| $$ 
si y solo si, $ y= #alpha #,x  $, con $#alpha #in #C$.
#end{proposition}
#n {#bf Demostraci#'on}##
Los casos, $x=0$ y/o $y=0$, son inmediatos. Supongamos entonces $ x #neq 0 $, $ y #neq 0 $.##

#n $ #Leftarrow ] $ Si $#alpha #in #C$, es claro que por la propiedades del producto escalar,
$$ |(x,y)|= |(x,#alpha#,x)|= |#alpha|#cdot #|x#|#cdot #|x#|= #|x#| #cdot #| #alpha #, x #|= #|x#| #cdot #|y#| $$

#n$ #Rightarrow ] $ Supongamos que $|(x,y)| = #|x#| #, #|y#|$. Definimos a los vectores,
#begin{align}
y_{#parallel} := &#left( #frac{x}{#|x#|},y #right)#, #frac{x}{#|x#|}##
y_{#perp} := y - &#left( #frac{x}{#|x#|},y #right)#, #frac{x}{#|x#|}
#end{align}
es claro que, $y = y_{#parallel} + y_{#perp}$ y que, $ (y_{#parallel} , y_{#perp})=0 $. Entonces por el teorema de Pit#'agoras,
$$ #|y#|^{2}= #|y_{#parallel}#|^{2} + #|y_{#perp}#|^{2} = #left| #left( #frac{x}{#|x#|},y #right) #right|^{2} + #left#| y - #left( #frac{x}{#|x#|},y #right) #frac{x}{#|x#|} #right#|^{2} $$
Luego, ya que por hip#'otesis tenemos $ #|x#|^{2} #, #|y#|^{2}=|(x,y)|^{2} $,
$$ #|x#|^{2}#, #|y#|^{2} = |(x,y)|^{2} + #|x#|^{2}#, #left#| y - #left( #frac{x}{#|x#|},y #right) #frac{x}{#|x#|} #right#|^{2} = |(x,y)|^{2} $$
esto implica que, 
$$ #left#| y - #left( #frac{x}{#|x#|},y #right) #frac{x}{#|x#|} #right#|=0 $$ 
pero esto sucede si y solo si,
$$ y= #left( #frac{x}{#|x#|},y #right) #frac{x}{#|x#|} = #left( #frac{x}{#|x#|},#frac{y}{#|x#|} #right)#, x $$ ## #qed

#vspace{5 mm}

Otra identidad geom#'etrica muy #'util es la {#bf ley del paralelogramo}, cuya demostraci#'on surge inmediatamente de la definici#'on de $#| #cdot #|$:
#begin{equation}
#|x+y#|^{2} + #|x-y#|^{2}= 2#, #|x#|^{2} + 2#, #|y#|^{2}
#end{equation}

En el capitulo anterior observamos que todo espacio normado es un espacio m#'etrico. El siguiente teorema muestra que todo espacio con producto interior es un espacio normado.

#newpage

#begin{theorem}#label{pi-nor}
Todo espacio con producto interior $V$ es un espacio normado, con la norma## $#|x#|= (x,x)^{1/2}$.
#end{theorem}
#n {#bf Demostraci#'on}##
Ya que $V$ es un espacio vectorial , solo necesitamos verificar que la funci#'on $#|#cdot #|$ tiene todas las propiedades de una norma. Todas estas propiedades se siguen inmediatamente de las propiedades $1)$ a $4)$ del producto interno, con excepci#'on de la desigualdad del triangulo. Sean $x,y #in V$, entonces por la desigualdad de Schwarz,
$$ #|x+y#|^{2}= (x,x)+(x,y)+(y,x)+(y,y) $$  
$$ = (x,x) + 2#, #hbox{Re}(x,y) +(y,y) $$
$$ #leq (x,x) + 2#, |(x,y)| +(y,y) $$
$$ #leq (x,x) + 2#,(x,x)^{1/2}(y,y)^{1/2}+(y,y) $$
Luego,
$$ #|x+y#| #leq  #|x#| + #|y#|  $$
Es interesante ver cuando la desigualdad del triangulo es una igualdad. Para esto supongamos que $ #|x+y#|= #|x#|+#|y#| $. Luego desarrollando el cuadrado de esta igualdad, tenemos que:
$$ #| x+y #|^{2}= (x+y,x+y)= #|x#|^{2} + 2#, #hbox{Re}(x,y) + #|y#|^{2} $$
$$( #|x#|+ #|y#| )^{2}= #|x#|^{2} + 2#, #|x#|#cdot #|y#| + #|y#|^{2} $$
esto implica que:
$$ #|x#| #cdot #|y#|= #hbox{Re}(x,y) $$
mas aun, por la desigualdad de Schwarz:
$$0 #leq |#hbox{Re}(x,y)| #leq |(x,y)|#leq #|x#| #cdot #|y#| = #hbox{Re}(x,y) $$
tomando el hecho de que $#hbox{Re}(x,y) #leq |#hbox{Re}(x,y)|$, tenemos que:
$$0 #leq |(x,y)|= #|x#|#cdot #|y#| =#hbox{Re}(x,y) $$
esto implica que $(x,y) #in #R$. Luego tenemos que:
$$ 0 #leq (x,y) = #|x#| #cdot #|y#| $$
Pero por la proposici#'on #ref{scheq} tenemos que esto sucede, si y solo si, $y = #alpha x$, en este caso con $#alpha$ un real positivo ya que de lo contrario lo anterior no se cumplir#'ia. El reciproco de esta proposici#'on es inmediato. Por tanto la desigualdad del triangulo es una igualdad, si y solo si, $y=#alpha x$ con $0#leq #alpha$. ## #qed

#vspace{5 mm}

El teorema anterior muestra que los espacios con producto interior poseen una m#'etrica natural,
$$ d(x,y)= #sqrt{ (x-y,x-y) } $$
entonces, tenemos las nociones de convergencia, completitud y densidad definidas para espacios m#'etricos. Mas aun, con la funci#'on distancia anterior y la desigualdad de Schwarz podemos mostrar que la funci#'on $(#cdot ,#cdot)$ es continua en V.

#begin{proposition}#label{p1}
Para todo $x #in V$ el producto escalar es una funci#'on continua de su primera entrada $ (#cdot,x) $, de su segunda entrada $(x,#cdot)$ y de ambas $(#cdot , #cdot)$.
#end{proposition}
#n {#bf Demostraci#'on}##
El caso en el que $x=0$ es inmediato. Sea $ x #in V $ tal que, $x #neq 0$ y sea $#{ y_{n} #} #subset V$, tal que, $ y_{n} #rar y #in V $ cuando $n #rar #infty$. Luego por la desigualdad de Schwarz tenemos que,
#begin{align}
| (y_{n},x)-(y,x) |&=| (y_{n}-y,x) | #leq #| y_{n}-y #| #cdot #|x#| #rar 0 ##
| (x,y_{n})-(x,y) |&=| (x,y_{n}-y) | #leq #|x#| #cdot #| y_{n}-y #| #rar 0
#end{align}
cuando $n #rar #infty$. Por tanto, el producto escalar es una funci#'on continua de su primera entrada y su segunda entrada.##
Sea $ #{x_{n}#} #subset V $, tal que, $ x_{n} #rar x #in V$ cuando $n #rar #infty$. De la desigualdad del triangulo tenemos que,
$$ #Big| #|x_{n}#| - #|x#| #Big| #leq #| x_{n} - x #| $$
de donde es f#'acil ver que entonces existe un real $C #geq 0$, tal que, $ C= #sup_{n}#{ #|x_{n}#| #} $. Luego por la desigualdad de Schwarz tenemos que,
$$ |(x_{n},y_{n})-(x,y)|= |(x_{n},y_{n})-(x_{n},y)+(x_{n},y)-(x,y)| = |(x_{n},y_{n}-y) + (x_{n}-x,y)|  $$ 
$$ #leq |(x_{n},y_{n}-y)| + |(x_{n}-x,y)| #leq #|x_{n}#|#cdot #|y_{n}-y#|+#|x_{n}-x#| #cdot #|y#| $$
$$ #leq C #cdot #|y_{n}-y#|+#|x_{n}-x#| #cdot #|y#| #rar 0 $$
cuando $n #rar #infty$. Por tanto, el producto escalar es una funci#'on continua de ambas de sus entradas. 
## #qed

#vspace{5 mm}

Otra propiedad importante de la norma generada por un producto interno, es la identidad de polarizaci#'on, la cual nos permite recuperar al producto interno directamente de la norma:
#begin{equation}#label{pol-}
(x,y)= #frac{1}{4} #left[ #left( #| x+y #|^{2} -#| x-y #|^{2} #right) -i ( #|x+iy#|^{2} -  #| x-iy #|^{2})  #right]
#end{equation}

En particular, siempre podemos completar a $V$ en un espacio normado $#tilde{V}$ en el que $V$ es isom#'etrico a un sub conjunto denso de este. De hecho $#tilde{V}$ es tambi#'en un espacio con producto interior ya que el producto interno de $V$ puede ser extendido a $#tilde{V}$ por continuidad.  

#begin{proposition}
Sea $V$ un espacio con producto interno y sea $#tilde{V}$ el espacio que resulta de completar a $V$ como espacio m#'etrico, entonces $#tilde{V}$ es un espacio con producto interno.
#end{proposition}
#n {#bf Demostraci#'on}##
Sean $x,y #in #tilde{V}$ entonces, ya que $V$ es isom#'etricamente denso en $#tilde{V}$ existen dos sucesiones de elementos de $V$, $#{x_{n}#}$ y $#{y_{n}#}$ convergentes (y por tanto de Cauchy) en $#tilde{V}$, tales que $ x_{n} #rar x $, $ y_{n} #rar y $, cuando $n #rar #infty$. La desigualdad del triangulo implica,
$$ #Big| #|x_{n}#| - #|x_{m}#| #Big|#leq #| x_{n} - x_{m} #| $$
$$ #Big| #|y_{n}#| - #|y_{m}#| #Big|#leq #| y_{n} - y_{m} #| $$
Por tanto las sucesiones de n#'umeros reales $#{ #|x_{n}#| #}$, $#{ #|y_{n}#| #}$
son de Cauchy y acotadas, es decir existen reales positivos $C_{x}$ y $C_{y}$ tales que, $ C_{x}= #sup_{n} #{ #|x_{n}#| #} $ y $ C_{y}= #sup_{n} #{ #|y_{n}#| #} $. Luego por la desigualdad de Schwarz tenemos que,
$$ | (x_{n},y_{n})-(x_{m},y_{m}) | =| (x_{n},y_{n})- (x_{m},y_{n}) + (x_{m},y_{n}) -(x_{m},y_{m}) | $$
$$ #leq #| x_{n} - x_{m} #| #cdot #|y_{n}#| + #|x_{m}#| #cdot #|y_{n}-y_{m}#| $$
$$ #leq C_{y} #cdot #| x_{n} - x_{m} #| + C_{x} #cdot #|y_{n}-y_{m}#| $$
as#'i, la sucesi#'on de n#'umeros complejos $(x_{n},y_{n})$ es de Cauchy y por tanto convergente. De esta forma definimos, para $x,y #in #tilde{V}$:
$$ (x,y)= #lim_{n #rar #infty} (x_{n},y_{n}) $$
Mas aun, el valor de $(x,y)$ es independiente de la sucesi#'on. Para ver esto supongamos que existen dos sucesiones de elementos de $V$, $#{#hat{x}_{n}#}$ y $#{#hat{y}_{n}#}$ convergentes (y por tanto de Cauchy) en $#tilde{V}$, tales que $ #hat{x}_{n} #rar x $, $ #hat{y}_{n} #rar y $, cuando $n #rar #infty$. Luego por la desigualdad de Schwarz y la desigualdad del triangulo:

$$ | (x_{n},y_{n})-(#hat{x}_{n},#hat{y}_{n}) |= | (x_{n},y_{n}) -(x_{n},#hat{y}_{n})+ ( x_{n},#hat{y}_{n} ) -(#hat{x}_{n},#hat{y}_{n}) | $$
$$ #leq #| x_{n} #| #cdot #| y_{n}-#hat{y}_{n} #| + #| y_{n} #| #cdot #| x_{n}-#hat{x}_{n} #| #leq C_{x} #cdot #| y_{n}-#hat{y}_{n} #| + C_{y} #cdot #| x_{n}-#hat{x}_{n} #| $$
$$ #leq C_{x}( #|y_{n}-y#|+#|#hat{y}_{n}-y#| ) + C_{y}( #|x_{n}-x#|+#|#hat{x}_{n} - x#| ) #rar 0 $$
cuando $n #rar #infty$.##
Ahora verifiquemos que $(#cdot , #cdot)$ cumple con las cuatro propiedades del producto interno en $#tilde{V}$. ## 

#n$1)$ Ya que para $x #in #tilde{V}$ el valor de $(x,x)$ no depende de la sucesi#'on de elementos $#{x_{n}#} #subset V$, podemos hacer $(x,x)= #lim_{n #rar #infty} (x_{n},x_{n}) $ y ya que $ (x_{n},x_{n}) #geq 0 $, $#forall x_{n} #in V$, tenemos que: 
$$ 0 #leq #lim_{n #rar #infty} (x_{n},x_{n})= ( x,x ) $$
por tanto $0 #leq (x,x)$, $ #forall x #in #tilde{V} $.##
Sea $z #in #tilde{V}$, tal que, $z #neq 0$. Entonces, ya que los espacios m#'etricos son espacios Hausdorff, existe una bola de radio $r >0$ con centro en $z$, para la cual  $ 0 #notin #overline{B_{r}(z)}$, esto implica que para alg#'un $#epsilon >0$ existe una bola alrededor de cero tal que, $ B_{#sqrt{#epsilon}}(0) #cap B_{r}(z)= #emptyset $. Sea $#{ z_{n} #} #subset V$ una sucesi#'on tal que, $z_{n} #rar z $, cuando $ n #rar #infty $; sin perdida de generalidad podemos suponer que $ z_{n} #in B_{r}(z) $ $#forall n$. as#'i, tenemos que, $ (z_{n},z_{n})= #|z_{n}#|^{2}= #|z_{n}-0#|^{2}=d^{2}(0,z_{n}) >#epsilon > 0$. Luego,
$$ 0 < #epsilon < #lim_{n #rar #infty} (z_{n},z_{n})= (z,z) $$
por tanto $ (z,z)>0 $, para $z #in #tilde{V}$ tal que, $z#neq 0$. Ya que $V$ es un espacio vectorial isom#'etricamente denso en $#tilde{V}$, $V$ es isom#'etrico a un subespacio vectorial de $#tilde{V}$ y por tanto para $0 #in #tilde{V}$, tenemos $(0,0)=0$. ## 
Verificar las propiedades $2),3)$ y $4)$ es inmediato de la definici#'on del producto interno y las propiedades elementales de limites de n#'umeros complejos. ## #qed

#vspace{5 mm}

#begin{definition}
Un espacio con producto interno y completo es llamado un {#bf espacio de Hilbert}. Los espacios con producto interno son llamados algunas veces espacios pre-Hilbert.
#end{definition}

#vspace{5 mm}

De la definici#'on anterior se debe de notar que todo espacio de Hilbert es un espacio de Banach. Sin embargo el teorema de Jordan - von Newman muestra que lo contrario no es siempre cierto [9].

#vspace{5 mm}

#begin{definition}#label{unita}
Decimos que dos espacios de Hilbert $#H_{1}$ y $#H_{2}$ son {#bf isomorfos} si existe un operador lineal biyectivo $U$ de $#H_{1}$ a $#H_{2}$, tal que, $ (Ux,Uy)_{#H_{2}}=(x,y)_{#H_{1}} $, para todo $x,y #in #H_{1}$. Tal operador es conocido como un {#bf operador unitario}.
#end{definition}

En particular, observamos que si $U$ es un operador unitario entonces:
$$ #| Ux #|_{2} = #| x #|_{1} #forall x #in H_{1} $$
De esta forma $U$ es tambien un operador {#bf isom#'etrico}.##

El ejemplo mas sencillo de un espacio de Hilbert es, de nueva cuenta, $#R^{n}$ as#'i como $#C^{n}$, ambos de dimensi#'on finita, a saber $n$. Un ejemplo mas interesante es el espacio vectorial de las funciones de cuadrado integrable en el sentido de Lesbegue en $#R^{n}$, denotado por $L^{2}(#R^{n})$, el cual es de dimensi#'on infinita. Mas adelante mostraremos que este espacio es en efecto un espacio de Hilbert, adem#'as de discutir el papel crucial que este juega en la mec#'anica cu#'antica.

#vspace{5 mm}

#section{El lema de Riesz}
Consideremos a un subespacio cerrado $#M$, de un espacio de Hilbert $#H$. Es f#'acil ver que $#M$ es un espacio de Hilbert por si mismo, bajo el producto interno que hereda de forma natural de $#H$. Denotamos por $#M^{#perp}$ al conjunto de vectores en $#H$ que son ortogonales a todos los vectores de $#M$; $#M^{#perp}$ es llamado el {#bf complemento ortogonal} de $#M$.##

#begin{proposition}
$#M^{#perp}$ es un espacio de Hilbert y $#M #cap #M^{#perp}= 0$.
#end{proposition}
#n {#bf Demostraci#'on}##
Sean $ x,y #in #M^{#perp} $ , $z#in #M$ y sean $#alpha , #beta #in #C$. Entonces por la linealidad del producto interno, tenemos que,
$$ (z,#alpha x + #beta y)= (z,#alpha x) + (z,#beta y)= #alpha (z,x) + #beta (z,y)=0 $$
y ya que $(#alpha x + #beta y,z)= #ol{(z,#alpha x + #beta y)}=0 $, tenemos que $ #alpha x + #beta y #in #M^{#perp} $. Ya que $(0,z)=0$, $#forall z #in #M$, concluimos que $0 #in #M^{#perp}$. Por tanto $#M^{#perp}$ es un subespacio vectorial de $#H$.## 

Sea $x^{*} #in #H$ un punto limite de $#M^{#perp}$. Entonces existe una sucesi#'on $#{ x_{n} #} #subset #M^{#perp}$, tal que, $ x_{n} #rar x^{*} $, cuando $n #rar #infty$. Luego, por la continuidad en las entradas del producto escalar, tenemos que:
$$ (x^{*},z) = #left(#lim_{n #rar #infty} x_{n},z  #right) = #lim_{n#rar #infty}(x_{n},z)=0 $$  
as#'i, $ x^{*} #in #M^{#perp} $. Por tanto $#M^{#perp}$ contiene a todos sus puntos limite y ya que $#H$ es completo, toda sucesi#'on de Cauchy de elementos de $#M^{#perp}$ tiene limite en $#H$, a saber, en un punto limite de $#M^{#perp}$.##
Por tanto $#M^{#perp}$ es completo. ##

Por otro lado, sea $ x #in #M #cap #M^{#perp} $, entonces $ (x,z)=0 $, $#forall z #in #M $, en particular para $z=x$, luego $ (x,x)= #|x#|^{2}=0 $, pero esto sucede si y solo si, $x=0$.## #qed

#vspace{5 mm}

Debemos hacer notar que en la demostraci#'on anterior no recurrimos a la propiedad de que $#M$ sea un subespacio cerrado de $#H$. Por tanto $#M^{#perp}$ sera un subespacio cerrado y $#M #cap #M^{#perp}=0$ incluso cuando $#M$ sea solamente un sub conjunto de $#H$.##

El siguiente teorema muestra que para cualquier subespacio propio y cerrado de $#H$, siempre hay vectores perpendiculares a este, de hecho muestra que hay tantos que podemos representar a $#H$ como:
$$ #H = #M #oplus #M^{#perp} := #{ x+y #,|#, x#in#M #,,#, y #in #M^{#perp} #} $$

#n Para hacer esto, establecemos primero el siguiente lema.

#begin{lemma}
Sea $#H$ un espacio de Hilbert, sea $#M$ un subespacio cerrado de $#H$ y supongamos que $x #in H$. Entonces existe en $#M$ un #'unico elemento $z$, el cual es el mas cercano a $x$.
#end{lemma}
#n {#bf Demostraci#'on}##
El caso en el que $x #in #M$ es inmediato haciendo $z=x$. Supongamos que $x #notin #M$.##
Sea $d= #inf_{y #in #M} #|x-y#|$. Tomemos una sucesi#'on $ #{y_{n}#} #subset #M $, tal que,
$$ #| x-y_{n} #| #rar d $$
Entonces,
$$ #| y_{n}-y_{m} #|^{2}= #| (y_{n}-x) - (y_{m}-x) #|^{2} $$  
$$ = 2#,#| y_{n}-x #|^{2} + 2#,#| y_{m}-x #|^{2} - #| -2x+y_{n}+y_{m} #|^{2} $$
$$ = 2#,#| y_{n}-x #|^{2} + 2#,#| y_{m}-x #|^{2} - 4#,#left#| x- (1/2)(y_{n}+y_{m}) #right#|^{2} $$
$$ #leq 2#,#| y_{n}-x #|^{2} + 2#,#| y_{m}-x #|^{2} - 4 d^{2} $$
$$ #longrightarrow 2d^{2} + 2d^{2} - 4d^{2} =0  $$
cuando $n #rar #infty$ y $m #rar #infty$. 
La segunda igualdad se sigue de la ley del paralelogramo y la desigualdad del hecho que $ (1/2)(y_{n}+y_{m}) #in #M $. Entonces $#{y_{n}#}$ es de Cauchy y ya que $#M$ es cerrado, $#{y_{n}#}$ converge a un elemento $z $ de $#M$. Por la continuidad de la norma, 
$$ #|x-z#|= #| x - #lim_{n #rar #infty} y_{n} #|= #lim_{n #rar #infty} #|x-y_{n}#|= d $$ 
Mas aun, $z$ es #'unico. Supongamos que existe otro $z^{*} #in #M$ tal que, $#|x-z^{*}#|=d$, luego: 
$$ #frac{z+z^{*}}{2} #in #M $$
Supongamos que existe $#alpha #geq 0$ tal que, $ x-z=#alpha(x-z^{*}) $. Esto implica que $x = #frac{z-#alpha z^{*}}{1-#alpha} #in #M$, lo cual es una contradicci#'on ya que $#M$ es un espacio vectorial y $x #notin #M$, por tanto no existe tal $#alpha #geq 0$. Luego, tenemos una desigualdad estricta en la siguiente aplicaci#'on de la desigualdad del triangulo:
$$ #left#| x- #frac{z+z^{*}}{2} #right#| = #frac{1}{2} #| 2x-z-z^{*} #| < #frac{1}{2} ( #|x-z#| + #|x-z^{*}#| )= d $$
as#'i, $#left#| x- #frac{z+z^{*}}{2} #right#| < d$, para $ #frac{z+z^{*}}{2} #in #M $, lo cual es claramente una contradicci#'on. Por tanto $z$ es #'unico. ## #qed

#vspace{5 mm}

#begin{theorem}
{#bf (Teorema de proyecci#'on)} Sea $#H$ un espacio de Hilbert y $#M$ un subespacio cerrado. Entonces todo $x #in #H$ puede ser escrito de forma #'unica como $x= z+w$, en donde $z #in #M$ y $w #in #M^{#perp}$.   
#end{theorem}
#n {#bf Demostraci#'on}##
Sea $x #in #H$. Entonces por el lema anterior hay un #'unico elemento $z #in #M$ el cual es el mas cercano a $x$. Definimos $w= x-z$, entonces $x=z+w$. Sea $y #in #M$ y $ t #in #R$. Si $ d= #|x-z#| $, entonces:
$$ d^{2} #leq #| x-(z+ty) #|^{2} = #| w-ty #|^{2}= d^{2} -2t#hbox{Re}(w,y) + t^{2} #|y#|^{2} $$ 
Entonces, $ -2t#hbox{Re}(w,y) + t^{2} #|y#|^{2} $ para toda $t$, esto implica que $#hbox{Re}(w,y)=0$. De forma similar usando $it$ en lugar de $t$ mostramos que $#hbox{Im}(w,y)=0$. Por tanto, $w #in #M^{#perp}$.##
Sean $ z_{2}#in #M $ y $w_{2} #in #M^{#perp} $. Supongamos que $ x= z+w=z_{2}+w_{2} $, luego: 
$$ z-z_{2}=w_{2}-w #in #M #cap #M^{#perp} $$ 
implicando que $ z-z_{2}=w_{2}-w=0 $. Por tanto la representaci#'on de $x=z+w$ es #'unica. ## #qed 

#vspace{5 mm}

En el capitulo anterior ya hemos estudiado a los operadores lineales acotados entre espacios normados. Denotaremos por $#L(#H_{1},#H_{2})$ al conjunto de operadores lineales acotados entre los espacios de Hilbert $#H_{1}$ y $#H_{2}$. Un caso particular de este conjunto es cuando $#H_{2}= #C$:

#begin{definition}
El espacio $#L(#H,#C)$ es llamado el {#bf espacio dual} de $#H$ y es denotado por $#H^{*}$. Los elementos de $#H^{*}$ son llamados {#bf funcionales lineales continuos}.
#end{definition}

#n El siguiente teorema da una caracterizaci#'on de $#H^{*}$.

#begin{theorem} {#bf (Lema de Riesz)} 
Para cada $T #in #H^{*}$ existe un #'unico $y_{T} #in #H$, tal que, $ T(x)=(y_{T},x) $ para todo $x #in #H$. Adem#'as $#|y_{T}#|_{#H}= #| T #|_{#H^{*}} $.
#end{theorem}
#n {#bf Demostraci#'on}##
Sea $#N$ el conjunto de $x #in #H$ tales que, $T(x)=0$. Por la continuidad de $T$, $#N$ es un subespacio cerrado. Si $#N = #H$, entonces $T(x)=0=(0,x)$ para todo $x$ y as#'i $T$ es el operador constante cero. ##
Supongamos ahora que $#N$ es un subespacio propio de $#H$. Entonces por el teorema de proyecci#'on hay un vector $x_{0}$ diferente de cero en $#N^{#perp}$. Definamos $y_{T}= #ol{T(x_{0})} #|x_{0}#|^{-2} x_{0} $. Verifiquemos que el vector $y_{T}$ tiene propiedades correctas. Primero, si $x #in #N$, entonces:
$$ T(x)=0=(y_{T},x) $$
Mas aun, si $x = #alpha x_{0}$, entonces:
$$ T(x)= T(#alpha x_{0})= #alpha T(x_{0})= #left(#ol{T(x_{0})} #|x_{0}#|^{-2} x_{0}, #alpha x_{0}  #right)= (y_{T}, #alpha x_{0}) $$
Ya que las funciones $T(#cdot)$ y $(y_{T},#cdot)$ son lineales y concuerdan en $#N$ y $x_{0}$, estas deben de concordar tambi#'en en el espacio generado por $#N$ y $x_{0}$. Pero $#N$ y $x_{0}$ generan a todo $#H$ ya que todo elemento $y #in #H$ puede ser escrito como:
$$ y= #left( y - #frac{T(y)}{T(x_{0})}#, x_{0} #right) + #frac{T(y)}{T(x_{0})}#, x_{0}  $$
Entonces, $ T(x)= (y_{T},x) $ para todo $x #in #H$. Si tambi#'en $ T(x)= (y',x) $ entonces:
$$ #| y'-y_{T} #|^{2}= T(y'-y_{T}) - T(y'-y_{T}) = 0$$
as#'i, $y'=y_{T}$. ##
Luego, observamos que:
$$ #|T#|= #sup_{#|x#|=1} |T(x)| = #sup_{#|x#|=1} |(y_{T},x)| #leq #sup_{#|x#|=1} #|y_{T}#| #cdot #|x#|= #|y_{T}#| $$
$$ #|T#| = #sup_{#|x#|=1} |T(x)| #geq #left| T #left( #frac{y_{T}}{#|y_{T}#|} #right) #right|= #left( y_{T},#frac{y_{T}}{#|y_{T}#|}  #right) = #|y_{T}#| $$
por tanto $#|T#|= #| y_{T} #|$. ## #qed

#vspace{5 mm}

Debe observarse que la desigualdad de Schwarz provee el regreso del teorema de Riesz, es decir, cada $y #in #H$ define un funcional lineal continuo $T_{y}$ en $#H$ mediante $T_{y}(x)= (y,x)$.

#newpage

#section{Bases ortonormales}
Ya hemos definido lo que es un conjunto de vectores ortonormales en un espacio de Hilbert. En esta secci#'on desarrollaremos esta idea mas all#'a; en particular queremos extender la idea de ``Base'' de los espacios vectoriales de dimensi#'on finita, a los espacios normados completos con producto interno. Si $#S$ es un conjunto ortonormal en un espacio de Hilbert $#H$ y no existe otro conjunto ortonormal que contenga a $#S$ como sub conjunto propio, entonces $#S$ es conocido como una {#bf base ortonormal} (o {#bf sistema ortonormal completo}) para $#H$. Para desarrollar estas ideas de forma clara, primero estableceremos algunas definiciones y teoremas sobre teor#'ia de conjuntos que necesitaremos mas adelante. 

#vspace{5 mm}

#begin{definition}
Sea $X$ un conjunto. Una relaci#'on $R$ en $X$ es llamada una {#bf relaci#'on de equivalencia} si satisface:
#begin{enumerate}
#item Para todo $x#in X$, $xRx$ (propiedad reflexiva).
#item Para todo $x,y #in X$, $xRy$ implica $yRx$ (propiedad sim#'etrica).
#item Para todo $x,y,z #in X$, $xRy$ y $yRz$ implican $xRz$ (propiedad transitiva).
#end{enumerate}
Al conjunto de elementos de $X$ que est#'an relacionados a un $x #in X$, se le conoce como la {#bf clase de equivalencia} de $x$.
#end{definition}

Por medio de las tres propiedades anteriores la demostraci#'on del siguiente teorema es inmediata.

#begin{theorem}
Sea $R$ una relaci#'on de equivalencia en un conjunto $X$. Entonces cada $x #in X$ pertenece a una #'unica clase de equivalencia.  
#end{theorem}

#begin{definition}
Una relaci#'on en un conjunto $X$, que sea reflexiva, transitiva y anti sim#'etrica (esto es $xRy$ y $yRx$, implican $x=y$) es llamada un {#bf orden parcial}.
Si $R$ es un orden parcial escribiremos $x < y$ en lugar de $xRy$.
#end{definition}

Un ejemplo de un orden parcial es el siguiente. Sea $X$ la colecci#'on de todos los sub conjuntos de un conjunto $Y$. Definamos $A < B$ si $ A #subset B $. Entonces $<$ es un orden parcial.##

En la definici#'on anterior usamos el termino #emph{parcial}, por que dos elementos arbitrarios de $X$ no necesariamente deben de cumplir $x<y#,$ #'o $#,y<x$. Sin embargo, si para toda pareja $x,y #in X$ se satisface $ x<y $ o $y<x$, se dice que $X$ esta {#bf linealmente ordenado}. Por ejemplo $#R$ con el orden usual `` $#leq$ '' esta linealmente ordenado.##

Supongamos que $X$ esta parcialmente ordenado por `` $<$ '' y que $Y #subset X$. Un elemento $p #in X$ es llamado una {#bf cota superior} para $Y$, si $y<p$, para toda $y #in Y$. Si $ m #in X $ y $ m<x $ implica $x=m$, decimos que $m$ es un {#bf elemento maximal} de $X$. ##
Establecemos el siguiente teorema sin demostraci#'on ya que esta requiere de una teor#'ia de conjuntos mas elaborada. Solo hemos de mencionar que el siguiente resultado es equivalente al axioma de elecci#'on.

#begin{theorem}{#bf Lema de Zorn }
Sea $X$ un conjunto no vac#'io parcialmente ordenado, con la propiedad que cada sub conjunto linealmente ordenado tiene una cota superior en $X$. Entonces cada sub conjunto linealmente ordenado tiene una cota superior que es tambi#'en un elemento maximal de $X$.
#end{theorem}

#n Con lo anterior establecido, estamos listos para el primer teorema acerca de bases en espacios de Hilbert.

#begin{theorem}
Todo espacio de Hilbert $#H$ tiene una base ortonormal.
#end{theorem}
#n {#bf Demostraci#'on}##
Consideremos la colecci#'on $#c$ de conjuntos ortonormales en $#H$. Ordenamos a $#c$ por inclusi#'on, es decir, decimos que $S_{1} < S_{2}$ si, $S_{1} #subset S_{2}$. Con esta definici#'on de $"<"$, $#c$ esta parcialmente ordenado; tambi#'en es no vac#'io ya que si $v$ es un elemento de $#H$, el conjunto que consiste solo de $v/#|v#|$ es un conjunto ortonormal. Sea $#{S_{#alpha}#}_{#alpha #in A}$ un sub conjunto linealmente ordenado de $#c$. Luego, $ #bigcup_{#alpha #in A} S_{#alpha}$ es un conjunto ortonormal que contiene a cada $S_{#alpha}$ y es entonces una cota superior para $#{S_{#alpha}#}_{#alpha #in A}$. Ya que todo sub conjunto linealmente ordenado de $#c$ tiene una cota superior, podemos aplicar el lema de Zorn y concluir que $#c$ tiene un elemento maximal; esto es, un sistema ortonormal que no esta contenido propiamente en otro sistema ortonormal. ## #qed      

#vspace{5 mm}

Es en este punto en el que debemos hacer la observaci#'on de que los sistemas ortonormales no tienen que ser necesariamente conjunto finitos, ya que podemos tener el caso en el que estos son de cardinalidad infinita, incluso no numerable. El siguiente teorema muestra que todo elemento de un espacio de Hilbert puede ser expresado como una combinaci#'on lineal (posiblemente infinita) de elementos de la base, tal como sucede en los espacios vectoriales de dimensi#'on finita.

#vspace{5 mm}

#begin{theorem}#label{bas-exp}
Sea $#H$ un espacio de Hilbert y $S=#{x_{#alpha}#}_{#alpha #in A}$ una base ortonormal. Entonces para cada $y #in #H$,
$$ y = #sum_{#alpha #in A} (x_{#alpha},y)#, x_{#alpha} $$
y 
$$ #|y#|^{2}= #sum_{#alpha #in A} |(x_{#alpha},y)|^{2} $$
La primera igualdad significa que la suma del lado derecho converge (independientemente del orden) a $y#in#H$. as#'i tambi#'en, si $#sum_{#alpha #in A} |c_{#alpha}|^{2} < #infty$, $c_{#alpha} #in #C$, entonces $ #sum_{#alpha #in A} c_{#alpha}#, x_{#alpha} $ converge a un elemento de $#H$.

#end{theorem}  
#n {#bf Demostraci#'on}##
La desigualdad de Bessel establece que para todo conjunto finito $A' #subset A$ tenemos que, $ #sum_{#alpha #in A'} |(x_{#alpha},y)|^{2} #leq #|y#|^{2} $. Entonces $(x_{#alpha},y) #neq 0$, a lo mas para una cantidad numerable de $#alpha #in A$ las cuales ordenamos de alguna manera $ #alpha_{1},#alpha_{2},... $, mas aun, ya que $#sum_{j=1}^{N} |(x_{#alpha#,j},y)|^{2} $ es mon#'otona creciente y acotada, esta converge a un limite finito cuando $ N #rar #infty $. Sea $y_{n} = #sum_{j=1}^{N} (x_{#alpha#,j},y)#,x_{#alpha#,j} $. Entonces para $n>m$ tenemos que:
$$ #| y_{n}-y_{m} #|^{2} = #left#| #sum_{j=m+1}^{n} (x_{#alpha#,j},y)#,x_{#alpha#,j} #right#|^{2} = #sum_{j=m+1}^{n} |(x_{#alpha#,j},y)|^{2} $$
por tanto $#{ y_{n} #}$ es una sucesi#'on de Cauchy que converge a alg#'un $y' #in #H$.##
Observemos que:
$$ ( y-y' , x_{#alpha#,l})= #lim_{n #rar #infty} #left(y - #sum_{j=1}^{n} (x_{#alpha#,j},y) #,,#, x_{#alpha#,l}  #right)= (y,x_{#alpha#,l}) - (y,x_{#alpha#,l})=0 $$
Y si $#alpha #neq #alpha_{l}$ $#forall l$, tenemos que:
$$ ( y-y' , x_{#alpha})= #lim_{n #rar #infty} #left(y - #sum_{j=1}^{n} (x_{#alpha#,j},y) #,,#, x_{#alpha}  #right)= 0$$

Por tanto $y-y'$ es ortogonal a todas las $x_{#alpha}$ en $#S$. Ya que $#S$ es un sistema ortonormal completo tenemos que $y-y'=0$. Entonces:
$$ y= #lim_{n #rar #infty} #sum_{j=1}^{n} ( x_{#alpha#, j},y )#, x_{#alpha#, j} $$
Mas aun,
$$ 0 = #lim_{n #rar #infty} #left#| y - #sum_{j=1}^{n} ( x_{#alpha#,j},y )#, x_{#alpha#, j} #right#|^{2} = #lim_{n #rar #infty} #left( #|y#|^{2} - #sum_{j=1}^{n} |( x_{#alpha#,j},y )|^{2}  #right)  $$
$$ = #|y#|^{2} - #sum_{#alpha #in A} |(x_{#alpha},y)|^{2} $$
 
#n Ahora, sea $#{C_{#alpha}#}_{#alpha #in A} #subset #C $, tal que, $ #sum_{#alpha #in A} |C_{#alpha}|^{2} < #infty$. Entonces $C_{#alpha} #neq 0$, a lo mas para una cantidad numerable de $#alpha #in A$, las cuales ordenamos de alguna manera $ #alpha_{1},#alpha_{2},... $; sea $y_{n} = #sum_{i=1}^{n} C_{#alpha#,i}#, x_{#alpha#, i} $, luego:
$$ #| y_{n}-y_{m} #|= #left#| #sum_{i=m+1}^{n} C_{#alpha#,i}#, x_{#alpha#, i} #right#| = #sum_{i=m+1}^{n} |C_{#alpha#,i}|^{2} $$
Por tanto, $#{y_{n}#}$ es una sucesi#'on de Cauchy en $#H$ y ya que este es completo existe $ y #in #H $, tal que:
$$ y = #lim_{n #rar #infty} y_{n} = #lim_{n #rar #infty} #sum_{i=1}^{n} C_{#alpha#,i}#, x_{#alpha#, i} =#sum_{#alpha #in A} C_{#alpha}#, x_{#alpha} $$## #qed

#vspace{5 mm}

Ahora describiremos un procedimiento muy #'util para construir un conjunto ortonormal a partir de una sucesi#'on arbitraria de vectores linealmente independientes, conocido como el proceso de ortogonalizaci#'on de Gram-Schmidt.##
Sea $ u_{1},u_{2},... $ una sucesi#'on de vectores linealmente independientes y definamos:
$$ w_{1} = u_{1}  $$
$$v_{1} = w_{1}/#|w_{1}#|  $$
$$ w_{2}=u_{2} - ( v_{1},u_{2} )#, v_{1} ## $$
$$ #cdot #cdot #cdot $$
$$ w_{n}=u_{n} - #sum_{k=1}^{n-1} (v_{k},u_{n})#, v_{k}  $$
$$ v_{n} = w_{n}/#|w_{n}#| $$
$$ #cdot #cdot #cdot $$

#n La familia $#{v_{j}#}$ es un conjunto ortonormal y tiene la propiedad de que para todo $m$, $#{ u_{j} #}_{j=1}^{m}$ y $#{ v_{j} #}_{j=1}^{m}$, generan el mismo espacio vectorial. ##

#begin{definition}
Diremos que un espacio m#'etrico es {#bf separable} si contiene un conjunto denso numerable.
#end{definition}

Muchos de los espacios de Hilbert que surgen en la practica resultan ser separables (tal es el caso de $L^{2}(#R^{n})$ como veremos mas adelante). El siguiente teorema caracteriza a este tipo de espacios.

#begin{theorem}#label{sep-bas}
Un espacio de Hilbert $#H$ es separable, si y solo si, tiene una base ortonormal numerable. 
#end{theorem}
#n {#bf Demostraci#'on}##
Supongamos que $#H$ es separable. Sea $#{ x_{n} #}$ un conjunto denso numerable. Desechando algunos de los $x_{n}$ podemos obtener una sub colecci#'on de vectores linealmente independientes $#{ x_{n#,l} #}$, cuyo conjunto de combinaciones lineales finitas, $#hbox{span}(#{ x_{n#,l} #})$ es el mismo el del conjunto completo $#{ x_{n} #}$; por tanto $#hbox{span}(#{ x_{n#,l} #})$ es denso en $#H$. Aplicando el proceso de ortogonalizaci#'on de Gram-Schmidt a la sub colecci#'on $#{ x_{n#,l} #}$ obtenemos un sistema ortonormal completo numerable.##

Por otro lado, sea $#{y_{n}#}$ una base ortonormal numerable del espacio de Hilbert $#H$. Entonces por el teorema #ref{bas-exp}, el conjunto de las combinaciones lineales finitas con coeficientes racionales de las $y_{n}$ es denso en $#H$ y ya que este conjunto es numerable, $#H$ es separable. ## #qed 

#vspace{5 mm}

Hay que notar que en el caso separable, el proceso de ortogonalizaci#'on de Gram-Schmidt nos permite construir una base ortonormal sin tener que usar el lema de Zorn.##
En lo siguiente supondremos que todos los espacio de Hilbert $#H$ con los que estemos trabajando son separables, es decir, supondremos que tienen una base ortonormal numerable. 

#newpage %************************************************************

#section{Operadores lineales no acotados en espacios de Hilbert}
Al conjunto de todos los operadores lineales acotados (en el sentido del capitulo 1) de la forma $T: #H #rar #H$ la denotamos como $B(#H)$.##
Si $T #in B(#H)$, entonces este operador esta definido en todo el espacio de Hilbert $#H$ y existe un n#'umero real $C>0$ tal que:
#begin{equation}#label{Tnorm}
#|Tx#| #leq C#,#|x#| #,,#,#,#forall x #in #H
#end{equation}

Definimos la norma de $T$ como:
$$ #|T#|= #sup_{#|#hat{x}#|=1} #|T#hat{x} #| $$
la norma del operador $T$, $#|T#|$ resulta ser la constante $C$ mas peque#~na que cumple con #eqref{Tnorm}.

Lo anterior es similar a la discusi#'on sobre operadores lineales acotados del capitulo 1, ya que como hemos visto todo espacio de Hilbert es un espacio de Banach, sin embargo es importante tener en mente los conceptos anteriores al momento de hablar de los operadores no acotados para poder distinguir las diferencias y limitaciones que estos #'ultimos tienen.##

Sea $D#subset #H$ un sub conjunto lineal de $#H$ y sea $T: D #rar #H $ una funci#'on lineal, entonces $T$ #emph{es un operador lineal de} $#H$. Si sucede que:
$$ #sup_{#hat{x}#in D#,,#, #|#hat{x}#|=1} #|T#hat{x} #| < #infty $$
entonces $T$ es un #emph{operador acotado en su dominio} $D$. De lo contrario nos referiremos a $T$ simplemente como un operador no acotado en $#H$.##

Al dominio $D$ del operador $T$, tambi#'en lo denotamos como $D(T)$.##
En $D(T)$ definimos el producto interno $( #cdot , #cdot )_{T}$ como:
$$ (x,y)_{T}=(x,y)+(Tx,Ty)  #,#,,#,#, #forall x,y #in D(T)$$
bajo este producto interno $D(T)$ es un espacio pre Hilbert, esto es, un espacio con producto interno, no necesariamente completo bajo la norma inducida:
$$ #|x#|_{T}^{2}= #|x#|^{2} + #|Tx#|^{2} #,#,,#,#, #forall x #in D(T) $$

Definimos al conjunto:
$$ N(T)=#{ x #in D(T) #,|#, Tx=0 #} $$
y nos referimos a este como el #emph{n#'ucleo de} $T$. Tal y como sucede en el algebra lineal, $T$ es invertible, si y solo si, $N(T)=#{ 0 #}$. Mas aun, la inversa de $T$ es un operador acotado en su dominio (el rango de $T$), si y solo si, existe $C>0$ tal que:
$$ #|Tx#| #geq C#, #| x #| #,#,,#,#, #forall x #in D(T) $$

Ahora, sean $T_{1}$ y $T_{2}$ operadores lineales de $H$. Diremos que $T_{1}=T_{2}$, si y solo si:
$$ D(T_{1}) = D(T_{2}) $$
y 
$$ T_{1}x=T_{2}x #,,#,#, #forall x #in D(T_{1}) $$
Si sucede que:
$$ D(T_{1}) #subset D(T_{2}) $$
y 
$$ T_{1}x=T_{2}x #,,#,#, #forall x #in D(T_{1}) $$
entonces diremos que $T_{2}$ es una extensi#'on de $T_{1}$. Esto lo denotamos como $T_{1} #subset T_{2}$.

#newpage

#section{Productos tensoriales de espacios de Hilbert}
En esta secci#'on estudiaremos el producto tensorial $#H_{1} #pr #H_{2}$ de dos espacios de Hilbert $#H_{1}$ y $#H_{2}$. Sean $ #vp_{1} #in #H_{1} $ y $ #vp_{2} #in #H_{2} $ y denotemos por $ #vp_{1} #pr #vp_{2} $ a la forma bilinear conjugada, que act#'ua sobre $ #H_{1} #times #H_{2} $ mediante:
$$ (#vp_{1} #pr #vp_{2}) #la #psi_{1},#psi_{2} #ra= ( #psi_{1},#vp_{1} )#,( #psi_{2},#vp_{2} ) $$ 
en donde $#psi_{1} #in #H_{1}$ y $#psi_{2} #in #H_{2}$.##

Sea $#E$ el conjunto de combinaciones lineales finitas de tales formas bilineales conjugadas; definimos un producto interno $(#cdot,#cdot)$ en $#E$ como:
$$ (#vp #pr #psi , #eta #pr #mu )= (#vp,#eta) #, ( #psi , #mu ) $$
y lo extendemos a todo $#E$ por linealidad.

#begin{proposition}
La funci#'on $(#cdot , #cdot)$ esta bien definida y es definida positiva.
#end{proposition}
#n {#bf Demostraci#'on}##
Para demostrar que $(#cdot , #cdot)$ esta bien definida, debemos probar que $( #lambda , #lambda' )$ no depende de la combinaci#'on lineal finita que se use para expresar a $#lambda$ y a $#lambda'$. Para hacer esto es suficiente mostrar que si $#mu$ es una suma finita la cual da como resultado a la cero forma, entonces $(#eta , #mu)=0$, para toda $ #eta #in #E $. Para ver que esto se cumple sea, $ #eta = #sum_{i=1}^{N} c_{i} #, ( #vp_{i} , #psi_{i} ) $, entonces, por la linealidad del $( #cdot , #cdot )$:

$$ ( #eta,#mu  )= #left( #sum_{i=1}^{N} c_{i}#,( #vp_{i} , #psi_{i} ) , #mu #right)  $$
$$ = #sum_{i=1}^{N} c_{i} #, #mu #, #la #vp_{i},#psi_{i} #ra =0$$
Ya que $#mu$ es la cero forma. Por tanto $ (#cdot , #cdot ) $ esta bien definida.##
Ahora supongamos que $ #lambda = #sum_{k=1}^{M} d_{k}#, ( #eta_{k} #pr #mu_{k} ) $. Luego $#{ #eta_{k} #}_{k=1}^{M}$ y $#{ #mu_{k} #}_{k=1}^{M}$ generan a los subespacios $M_{1} #subset #H_{1} $ y $ M_{2} #subset #H_{2} $ respectivamente. Si $#{ #vp_{j} #}_{j=1}^{N_{1}}$ y $#{ #psi_{l} #}_{l=1}^{N_{2}}$ son bases ortonormales para $M_{1}$ y $M_{2}$, podemos expresar a cada $#eta_{k}$ en t#'erminos de la base $#{ #vp_{j} #}_{j=1}^{N_{1}}$ y cada $#mu_{k}$ en t#'erminos de la base $#{ #psi_{l} #}_{l=1}^{N_{2}}$, obteniendo:
$$ #lambda = #sum_{j=1 #, l=1}^{M_{1}#, M_{2}} c_{j#,l}#, ( #vp_{j} #pr #psi_{l} ) $$
pero,
$$ ( #lambda , #lambda )= #left( #sum c_{j#,l}#,(#vp_{j} #pr #psi_{l}) #,,#, #sum c_{i#,m}#,(#vp_{i} #pr #psi_{m})  #right) $$
$$ = #sum #ol{c_{j#,l}} #, c_{i#,m} (#vp_{j},#vp_{i})(#psi_{l} , #psi_{m})= #sum_{j#,l} |c_{j#,l}|^{2} $$
entonces si $( #lambda , #lambda )=0$, todas las $c_{j#,l}=0$ y $#lambda$ es la cero forma. Por tanto $ ( #cdot , #cdot ) $ es positiva definida.

#vspace{5 mm}

#begin{definition}
Definimos a $ #H_{1} #pr #H_{2} $ como el espacio que resulta de completar a $#E$ bajo el producto interno $( #cdot , #cdot )$ definido anteriormente. $#H_{1} #pr #H_{2}$ es llamado el {#bf producto tensorial} de $#H_{1}$ y $#H_{2}$.
#end{definition}


#begin{proposition}#label{pt-b}
Si $#{ #vp_{k} #}$ y $#{ #psi_{l} #}$ son bases ortonormales de $#H_{1}$ y $#H_{2}$ respectivamente, entonces $#{ #vp_{k} #pr #psi_{l} #}$ es una base ortonormal para $ #H_{1} #pr #H_{2} $.
#end{proposition}
#n {#bf Demostraci#'on}##
Para simplificar la notaci#'on consideraremos el caso en el que $#H_{1}$ y $#H_{2}$ son de dimensi#'on infinita y separables. Los dem#'as casos son similares. Por la definici#'on del producto interno en $#H_{1} #pr #H_{2}$, es claro que el conjunto $#{ #vp_{k} #pr #psi_{l} #}$ es ortonormal y por tanto solo debemos mostrar que $#E$ esta contenido en el espacio cerrado $S$, el cual es generado por $#{ #vp_{k} #pr #psi_{l} #}$. ##
Sea $ #vp #pr #psi #in #E $. ya que $#{ #vp_{k} #}$ y $#{ #psi_{l} #}$ son bases, $ #vp= #sum_{k} c_{k}#, #vp_{k}$ y $ #psi= #sum_{l} d_{l}#, #psi_{l} $, donde $#sum_{k} |c_{k}|^{2} < #infty$ y $ #sum_{l} |d_{l}|^{2} < #infty $, entonces, $#sum_{k#,l} | c_{k}#,d_{l} |^{2} < #infty $. Luego por el teorema #ref{bas-exp}, existe un vector $ #eta = #sum_{k#,l} c_{k}#,d_{l}#, ( #vp_{k} #pr #psi_{l} ) $ en $S$. Entonces, es claro que:
$$ #left#| #vp #pr #psi - #sum_{k<M #,,#, l<N} c_{k}#,d_{l}#, #vp_{k} #pr #psi_{l} #right#| #longrightarrow 0 $$
cuando $M,N #rar #infty$. ## #qed

#vspace{5 mm}

En este punto ya estamos listos para la introducci#'on de un espacio de Hilbert concreto, a saber el espacio de funciones de cuadrado integrable $L^{2}(#R^{n})$ y algunas de sus variantes que utilizaremos mas adelante en el desarrollo del texto.

#newpage

#chapter{El espacio $L^{2}(#R^{n})$}

Denotamos por $#L^{2}(#R^{n})$ al espacio de funciones medibles, en el sentido de Lebesgue, que van de $#R^{n}$ a $#C$, para las cuales:
$$  #int_{#R^{n}} |f(#x)|^{2} #, d#x  < #infty  $$

en donde, $#int_{#R^{n}} #, d#x $ denota a la integral de Lebesgue sobre $#R^{n}$ (para una introduccion a la integral de Lebesgue sobre espacios metricos nos referimos a [9] y [19]). Debemos notar que $#L^{2}(#R^{n})$ es un espacio lineal sobre los n#'umeros complejos, ya que:
$$ |f+g|^2 #leq 2  (|f|^{2}+|g|^{2}) $$
y que,
$$ 
#int_{#R^{n}} |#alpha #, f(#x)|^{2} #, d#x= |#alpha|^{2} #int_{#R^{n}} |f(#x)|^{2} #, d#x 
$$
Esperar#'{#i}amos poder hacer de $#L^{2}(#R^{n})$ un espacio de Hilbert bajo un producto escalar escogido adecuadamente, sin embargo la integral de Lebesgue presenta el siguiente detalle t#'ecnico referente a cuando la integral del modulo cuadrado de una funci#'on es cero. 

#vspace{5 mm}

#begin{lemma}#label{0ctp}
Sea $f$ una funci#'on medible de $#R^{n}$ a $#C$. Entonces:
$$ #int_{#R^{n}} |f(#x)|^{2}#, d#x=0 $$
si y solo si, $f(#x)=0$ en casi todo punto.
#end{lemma}
#n {#bf Demostraci#'on}##
Recordemos que una propiedad es valida $en#, casi #, todo #, punto$ si esta es valida solamente en un conjunto con medida de Lebesgue cero. ##
Sea $A= #left#{ #x #, #Big| #, |f(#x)|^{2}>0 #right#}$ y definamos a $A_{n}= #left#{ #x #, #Big| #, |f(#x)|^{2}>#frac{1}{n} #right#}$, $#forall n #in #mathbb{N}$. Mostremos que $A = #bigcup_{n} A_{n} $. Sea $#x #in A_{n} $ entonces $ |f(#x)|^{2} > #frac{1}{n} >0 $, implicando que $x#in A$ y que $ A_{n} #subset A $, $#forall n #in #mathbb{N}$; por tanto $ #bigcup_{n} A_{n} #subset A $. ##
Sea $#x #in A$ entonces existe $#epsilon >0$, tal que $ |f(#x)|^{2}>#epsilon >0 $ y ya que $#frac{1}{n} #rar 0 $, existe $ N #in #mathbb{N} $ tal que, $ |f(#x)|^{2}>#epsilon>#frac{1}{n} >0 $, $ #forall n>N $; as#'i $#x #in A_{n}$ $#forall n >N$ implicando que $ #x #in #bigcup_{n} A_{n} $. Por tanto $A #subset #bigcup_{n} A_{n}$.
Luego,
$$0 #leq #int_{A_{n}} |f(#x)|^{2}#, d#x #leq #int_{A} |f(#x)|^{2}#, d#x = #int_{#R^{n}} |f(#x)|^{2}#, d#x =0$$
luego tenemos que,
$$ 0 #leq #frac{1}{n}#, m(A_{n})#leq #int_{A_{n}} #frac{1}{n}#, d#x #leq #int_{A_{n}} |f(#x)|^{2}#, d#x =0 $$
lo cual implica que $m(A_{n})=0$ para toda $n #in #mathbb{N}$. Luego para la medida de $A$ tenemos que:
$$ 0#leq  m(A)=m( #cup_{n} A_{n} ) #leq #sum_{n} m(A_{n})=0 $$
por tanto $m(A)=0$.##
Si $|f(#x)|=0$ en casi todo punto, es claro que:
$$ #int_{#R^{n}} |f(#x)|^{2}#, d#x=0 $$  #qed

#vspace{5 mm}

#n Definimos la funci#'on $( #cdot , #cdot ) : #L^{2}(#R^{n}) #times #L^{2}(#R^{n}) #rar #C $, dada por: 
#begin{equation}#label{prod}
 ( f,g )= #int_{#R^{n}} f(#x)#, #ol{g(#x)} #, d#x
#end{equation}
Observemos que:
$$ (f,f)= #int_{#R^{n}} |f(#x)|^{2} #, d#x $$
La siguiente proposici#'on muestra que el dominio de $(#cdot , #cdot)$ es en efecto todo $#L^{2}(#R^{n}) #times #L^{2}(#R^{n})$.

#begin{proposition}#label{ppl2}
La funci#'on $( #cdot , #cdot )$ esta bien definida en $#L^{2}(#R^{n}) #times #L^{2}(#R^{n})$.
#end{proposition}
#n {#bf Demostraci#'on}##
Sean $f #in #L^{2}(#R^{n})$ y $g #in #L^{2}(#R^{n})$. Consideremos el modulo cuadrado de $(f-g)$,
$$ |f-g|^{2} = #ol{(f-g)}#, (f-g)= |f|^{2} - 2#, #hbox{Re}(f#, #ol{g}) +|g|^{2} #geq 0  $$
esto implica que,
$$ 2#, #hbox{Re}(f#, #ol{g}) #leq |f|^{2} + |g|^{2} $$
sea, $#x_{1}= #{ #x #in #R^{n} #,|#, 0 #leq #hbox{Re}(f#, #ol{g})  #} $, luego tenemos que,
$$ 0 #leq #int_{#x_{1}} #hbox{Re}(f#, #ol{g}) #, d#x #leq #int_{#R^{n}} |f(#x)|^{2}#, d#x  + #int_{#R^{n}} |g(#x)|^{2}#, d#x $$

#n Consideremos el modulo cuadrado de $(f+g)$,
$$ |f+g|^{2} = #ol{(f+g)}#, (f+g)= |f|^{2} + 2#, #hbox{Re}(f#, #ol{g}) +|g|^{2} #geq 0  $$
esto implica que,
$$ -2#, #hbox{Re}(f#, #ol{g}) #leq |f|^{2} + |g|^{2} $$
sea, $#x_{2}= #{ #x #in #R^{n} #,|#,  #hbox{Re}(f#, #ol{g}) < 0  #} $, luego tenemos que,
$$ 0 #leq -#int_{#x_{2}} #hbox{Re}(f#, #ol{g}) #, d#x #leq #int_{#R^{n}} |f(#x)|^{2}#, d#x  + #int_{#R^{n}} |g(#x)|^{2}#, d#x $$
Ahora consideremos el modulo cuadrado de $(f-ig)$,
$$ |f-ig|^{2} = #ol{(f-ig)}#, (f-ig)= |f|^{2} - 2#, #hbox{Re}(f#, #ol{(ig)}) +|g|^{2} $$
$$ = |f|^{2} - 2 #hbox{Im}(f#,#ol{g}) + |g|^{2} #geq 0 $$
esto implica que,
$$ 2#, #hbox{Im}(f#, #ol{g}) #leq |f|^{2} + |g|^{2} $$
sea, $#x_{3}= #{ #x #in #R^{n} #,|#, 0 #leq #hbox{Im}(f#, #ol{g})  #} $, luego tenemos que,
$$ 0 #leq #int_{#x_{3}} #hbox{Im}(f#, #ol{g}) #, d#x #leq #int_{#R^{n}} |f(#x)|^{2}#, d#x  + #int_{#R^{n}} |g(#x)|^{2}#, d#x $$

consideremos el modulo cuadrado de $(f + ig)$,
$$ |f + ig|^{2} = #ol{(f+ig)}#, (f+ig)= |f|^{2} + 2#, #hbox{Re}(f#, #ol{(ig)}) +|g|^{2} $$
$$ = |f|^{2} + 2 #hbox{Im}(f#,#ol{g}) + |g|^{2} #geq 0 $$
esto implica que,
$$ -2#, #hbox{Im}(f#, #ol{g}) #leq |f|^{2} + |g|^{2} $$
sea, $#x_{4}= #{ #x #in #R^{n} #,|#,  #hbox{Im}(f#, #ol{g}) <0 #} $, luego tenemos que,
$$ 0 #leq - #int_{#x_{4}} #hbox{Im}(f#, #ol{g}) #, d#x #leq #int_{#R^{n}} |f(#x)|^{2}#, d#x  + #int_{#R^{n}} |g(#x)|^{2}#, d#x $$
Luego,
$$ 0 #leq #left| #int_{#R^{n}} f(#x) #, #ol{g(#x)} #, d#x #right| = #left| #int_{#R^{n}} #hbox{Re}(f#, #ol{g})#, d#x + i #int_{#R^{n}} #hbox{Im}(f#, #ol{g})#, d#x #right| $$
$$#leq #left| #int_{#R^{n}} #hbox{Re}(f#, #ol{g})#, d#x #right| + #left| #int_{#R^{n}} #hbox{Im}(f#, #ol{g})#, d#x #right| $$
$$= #int_{#x_{1}} #hbox{Re}(f#, #ol{g}) #, d#x - #int_{#x_{2}} #hbox{Re}(f#, #ol{g}) #, d#x + #int_{#x_{3}} #hbox{Im}(f#, #ol{g}) #, d#x - #int_{#x_{4}} #hbox{Im}(f#, #ol{g}) #, d#x  $$
$$ #leq 4#, #left[ #int_{#R^{n}} |f(#x)|^{2} #, d#x + #int_{#R^{n}} |g(#x)|^{2} #, d#x #right] < #infty $$
Por tanto, $ |(f,g)|< #infty $, para toda $f,g #in #L^{2}(#R^{n})$. as#'i la funci#'on $(#cdot , #cdot)$ esta bien definida en todo $#L^{2}(#R^{n}) #times #L^{2}(#R^{n})$. ## #qed

#vspace{5 mm}

En este punto debemos hacer notar que por las propiedades de linealidad de la integral de Lebesgue, la funci#'on $(#cdot , #cdot)$ cumple con las propiedades 2 a 4 de la definici#'on de producto interno, sin embargo por el lema #ref{0ctp} es inmediato observar que esta falla en ser positiva definida, ya que si $f #in #L^{2}(#R^{n})$ y $(f,f)=0$, solo podemos asegurar que $f(#x) =0 $ $en #, casi #, todo #,punto$ y no en todo $#R^{n}$. Es aqu#'i cuando podemos descartar a la funci#'on $(#cdot , #cdot)$ como un producto interno de $#L^{2}(#R^{n})$ y empezar a definir otra que si cumpla con las propiedades requeridas o podr#'iamos quedarnos con esta y cambiar a un espacio en donde cumpla todas las propiedades del producto interno. Para empezar a hacer esto ultimo establecemos la siguiente proposici#'on.

#vspace{5 mm}

#begin{proposition}#label{eqv-l2}
Digamos que dos funciones $f$ y $g$ de $#L^{2}(#R^{n})$ est#'an relacionadas $f #sim g$, si y solo si, $ f(#x)=g(#x) $ en casi todo punto (C.T.P.). Entonces la relaci#'on $f #sim g$ es de equivalencia.
#end{proposition}
#n {#bf Demostraci#'on}##
Recordemos que para probar que la relaci#'on $#sim$ es de equivalencia, hay que verificar que esta posea las propiedades de Reflexividad, Simetr#'ia y Transitividad. ##
Sean $f,g,h #in #L^{2}(#R^{n})$ tales que, $f #sim g$ y $g #sim h$. Entonces:
#begin{enumerate}
#item Ya que $f(#x)= f(#x)$ en todo punto, $f(#x)= f(#x)$ en C.T.P; por tanto $f #sim f$. as#'i $#sim$ es reflexiva.

#item $f #sim g$ $#sss$ $f(#x) = g(#x)$ en C.T.P. $#sss$ $g(#x) = f(#x)$ en C.T.P. $#sss$ $g #sim f$. as#'i $#sim$ es sim#'etrica.

#item $f=g$ en C.T.P. y $g=h$ en C.T.P. $#sss$ $f-g=0$ en C.T.P. y $h-g=0$ en C.T.P. implica que $ f-g=g-h$ en C.T.P. entonces $f-g=g-h$ en C.T.P. y por tanto $f=h$ C.T.P. as#'i $f#sim h$.
#end{enumerate}
 #qed

#vspace{5 mm}

Es f#'acil ver que la relaci#'on de equivalencia $#sim$ tambi#'en se puede caracterizar como: $f#sim g$ $#sss$ $ f-g =0$ en C.T.P. ya que,
$$ f #sim g #sss f=g #, #hbox{ C.T.P. } #sss f-g =0  #, #hbox{ C.T.P. } $$
Sea,
$$ #N(#R^{n})= #{ f #in #L^{2}(#R^{n}) #,|#, f(#x)=0 #, #hbox{ en C.T.P. } #} $$
Entonces $#N(#R^{n})$ es un subespacio lineal de $#L^{2} (#R^{n})$, tal como lo establece la siguiente proposici#'on.

#begin{proposition}
$#N(#R^{n})= #{ f #in #L^{2}(#R^{n}) #,|#, f(#x)=0 #, #hbox{ en C.T.P. } #}$ es un subespacio lineal de $#L^{2}(#R^{n})$.
#end{proposition}
#n {#bf Demostraci#'on}##
Es claro que la funci#'on $#mathcal{O}(#x)=0$ $#forall #x #in #R^{n}$, es un elemento de $#N(#R^{n})$.##
Sean $f,g #in L^{2}(#R^{n})$ y sean,
$$ #x_{f}=#{ #x #in #R^{n} #,|#, f(#x) #neq 0  #} $$
$$ #x_{g}=#{ #x #in #R^{n} #,|#,g(#x) #neq 0  #} $$
$$ #x_{+}=#{ #x #in #R^{n} #,|#, f(#x)+g(#x) #neq 0  #} $$
Es claro que $m(#x_{f})= m(#x_{g}) = 0$. Sea $#x #in #x_{+}$ entonces $f(#x)+g(#x) #neq 0$; esto implica que $ #x #in #x_{f} $ y/o $  #x #in #x_{g}$ as#'i $#x #in #x_{f} #cup #x_{g} $ y por tanto $ #x_{+} #subset #x_{f} #cup #x_{g} $. Luego,
$$ 0 #leq m(#x_{+}) #leq m(#x_{f} #cup #x_{g}) #leq m(#x_{f})+ m(#x_{g})=0 $$
y por tanto $f+g #in #N(#R^{n})$.##
Sea $#alpha #in #C$; es claro que $ #x_{f}= #x_{#alpha f} $ y por tanto $#alpha f #in #N(#R^{n})$. ## #qed 

#vspace{5 mm}
Ahora ya que hemos visto que la relaci#'on $#sim$ es de equivalencia y que $#N(#R^{n})$ es un subespacio de $#L^{2}(#R^{n})$ podemos denotar sin ambiguiedad al espacio cociente:
$$ L^{2}(#R^{n})= #L^{2}(#R^{n}) / #N(#R^{n}) $$
cuyos elementos son las clases de equivalencia $[f]$ de la relaci#'on de equivalencia $#sim$. De la teor#'ia del algebra lineal sabemos que $L^{2}(#R^{n})$ es de hecho un espacio vectorial (en este caso sobre el campo de los complejos) bajo las operaciones de suma y multiplicaci#'on por escalar definidas como:
$$ [f]+[g] := [f+g] $$
$$ #alpha [f] := [#alpha f] $$
Por las propiedades sim#'etrica y transitiva de la relaci#'on de equivalencia $#sim$, las clases de equivalencia $[f+g]$ y $[#alpha f]$ son no vac#'ias y no dependen de los representantes de clase $f$ y $g$. Notemos que $#N(#R^{n}) = [#mathcal{O}]$ y que $[#mathcal{O}]$ es el neutro aditivo del espacio $L^{2}(#R^{n})$ en donde $#mathcal{O}(#x)=0$, $#forall #x #in #R^{n}$.##
tambi#'en debemos observar que si $f_{1},f_{2} #in [f]$, $f_{1}(#x)=f_{2}(#x)$ en C.T.P. y entonces:
$$ #int_{#R^{n}} f_{1}(#x) #, d#x = #int_{#R^{n}} f_{2}(#x) #, d#x$$

#n Definimos la funci#'on, 
$$(#cdot , #cdot)_{L^{2}} : L^{2}(#R^{n}) #times L^{2}(#R^{n}) #rar #C $$ 
dada por, 
$$ ([f],[g])_{L^{2}} = (f,g) $$
en donde $f,g #in #L^{2}(#R^{n})$ son representantes de las clases de equivalencia $[f]$ y $[g]$. El siguiente lema establece que $L^{2}(#R^{n})$ es un espacio con producto interno.

#vspace{5 mm}

#begin{lemma}
La funci#'on $(#cdot , #cdot)_{L^{2}} $ es un producto interno en $L^{2}(#R^{n})$.
#end{lemma}
#n {#bf Demostraci#'on}##
Primero debemos de mostrar que la funci#'on $(#cdot , #cdot)_{L^{2}}$ esta bien definida en $L^{2}(#R^{n}) #times L^{2}(#R^{n})$ en el sentido de que el valor $([f],[g])_{L^{2}}=(f,g) $ no depende de los representantes $f$ y $g$ de las clases de equivalencia, si no de las clases mismas.##
Sean $f_{1},f_{2} #in [f]$ y $g_{1},g_{2} #in [g]$. Ya que $g_{1}(#x)= g_{2}(#x)$ en C.T.P. entonces, $ #ol{g}_{1}(#x)= #ol{g}_{2}(#x) $ C.T.P. luego,
$$ f_{1}(#x)#, #ol{g}_{1}(#x) = f_{1}(#x)#, #ol{g}_{2}(#x) #, #hbox{ en C.T.P. } $$ 
Por la proposici#'on #ref{ppl2} estos productos de funciones est#'an en $#L^{2}(#R^{n})$, esto implica que $ f_{1}(#x)#, #ol{g}_{1}(#x) #sim f_{1}(#x)#, #ol{g}_{2}(#x)$. Por otro lado ya que $f_{1}(#x)= f_{2}(#x)$ en C.T.P. entonces,
$$ f_{1}(#x)#, #ol{g}_{2}(#x) = f_{2}(#x)#, #ol{g}_{2}(#x) #, #hbox{ en C.T.P. } $$
implicando que $f_{1}(#x)#, #ol{g}_{2}(#x) #sim f_{2}(#x)#, #ol{g}_{2}(#x)$.## 
Luego por la transitividad de la relaci#'on de equivalencia $#sim$ tenemos que $ f_{1}(#x)#, #ol{g}_{1}(#x) #sim f_{2}(#x)#, #ol{g}_{2}(#x) $ implicando que $f_{1}(#x)#, #ol{g}_{1}(#x) = f_{2}(#x)#, #ol{g}_{2}(#x)$ en C.T.P. y por tanto,
$$ #int_{#R^{n}} f_{1}(#x)#, #ol{g}_{1}(#x) #, d#x = #int_{#R^{n}} f_{2}(#x)#, #ol{g}_{2}(#x) #, d#x$$
as#'i, $ ([f],[g])_{L^{2}}$ no depende de los representantes $f_{1},f_{2}$ y $g_{1},g_{2}$.##
Por el argumento anterior y por la linealidad de la integral de Lebesgue, solo debemos verificar que la funci#'on $( #cdot , #cdot )_{L^{2}}$ es positiva definida. Sea $ [f] #in L^{2}(#R^{n}) $ entonces $|f(#x)|^{2} #geq 0$ $#forall #x #in #R^{n}$ implicando que,
$$ ([f],[f])_{L^{2}}=(f,f)= #int_{#R^{n}} |f(#x)|^{2} #, d#x #geq 0 $$

Es claro que $([#mathcal{O}],[#mathcal{O}])_{L^{2}} = #int_{#R^{n}} #mathcal{O}(#x) #, d#x=0  $. Por otro lado supongamos que $([h],[h])_{L^{2}}=0$, entonces:
$$ ([h],[h])_{L^{2}}= (h,h)= #int_{#R^{n}} |h(#x)|^{2} #, d#x =0$$ 
esto implica que, $h(#x)=0$ en C.T.P. (ya que el modulo complejo es una funci#'on positiva definida). Luego $h #sim #mathcal{O}$ haciendo que $#mathcal{O} #in [h]$ pero ya que las clases de equivalencia son disjuntas entre ellas, tenemos que $[h]=[#mathcal{O}]$.##
Por tanto la funci#'on $(#cdot , #cdot)_{L^{2}}$ es un producto interno de $L^{2}(#R^{n})$. ## #qed

#vspace{5 mm}

Aunque estrictamente hablando $L^{2}(#R^{n})$ esta formado por clases de equivalencia de funciones, en la practica y por conveniencia de notaci#'on, a los elementos de este espacio se les sigue llamando funciones. Sin embargo, se debe de notar que para $ f #in L^{2}(#R^{n}) $ el valor $f(#x)$ no esta bien definido, al menos que exista un representante continuo de la clase de equivalencia y que funciones continuas diferentes est#'en en clases de equivalencia distintas, pero esto sucede de esta forma con la medida de Lebesgue. Mas adelante en esta secci#'on estableceremos un teorema que aclara este hecho.

#vspace{5 mm}

Por el teorema #ref{pi-nor} tenemos que $L^{2} (#R^{n})$ es un espacio normado bajo la norma:
$$ #| f #|_{L^{2}}= #sqrt{(f,f)_{L^{2}}} $$

Para una funci#'on medible definimos al {#bf supremo esencial} como:
$$ #hbox{ess-sup}(f) ) = #inf #{ M #geq 0 #,|#, |f(#x)| < M #hbox{ en C.T.P.}  #} $$

En particular, Se dice que la funci#'on medible $f$ es {#bf esencialmente acotada}, si $ #hbox{ess-sup}(f) ) < #infty$.##
Es f#'acil ver que si para un elemento de una clase de equivalencia $[f] #in L^{2} (#R^{n}) $, el supremo esencial existe, entonces existe para todos los demas elementos y tiene el mismo valor.##

Ahora mostraremos que en efecto $L^{2}(#R^{n})$ es un espacio de Hilbert.

#vspace{5 mm}

#begin{theorem}
El espacio $L^{2}(#R^{n})$ es completo bajo la norma $#| #cdot #|_{L^{2}}= #sqrt{( #cdot , #cdot )_{L^{2}}}$
#end{theorem}
#n {#bf Demostraci#'on}##
Sea $#{ f_{n} #} #subset L^{2}(#R^{n})$ una sucesi#'on de Cauchy, entonces es suficiente mostrar que una subsucesi#'on de esta es convergente; as#'i podemos eliminar algunos t#'erminos de la sucesi#'on original para obtener otra tal que:
$$ #| f_{n+1} - f_{n} #|_{L^{2}} #leq #frac{1}{2^{n}} $$   
Sea $ g_{n}= f_{n} - f_{n-1} $ (en donde $f_{0}=0$). Definimos a la funci#'on:
$$ G(x)=#sum_{k=1}^{#infty} |g_{k}(x)| $$
luego observamos que:
$$ #left#| #sum_{k=1}^{n} |g_{k}(x)| #right#|_{L^{2}} #leq #sum_{k=1}^{n} #| g_{k}(x) #| #leq #|f_{1}#|_{L^{2}} + #frac{1}{2} #,#,#,#,#,#, (#forall n #in #mathbb{N}) $$
as#'i por el teorema de convergencia mon#'otona [19] tenemos que $G(x) #in L^{2}(#R^{n})$. En particular esto muestra que $G(x) < #infty $ en C.T.P. y tambi#'en que la suma,
$$ #sum_{n=1}^{#infty} g_{n}(x)= #lim_{n #rar #infty} f_{n}(x) $$
es absolutamente convergente en C.T.P.##
Sea $f(x)= #lim_{n #rar #infty} f_{n}(x) $; ya que $ | f(#x) - f_{n}(#x) |^{2} $ converge a cero en C.T.P. y $| f(#x) - f_{n}(#x) |^{2} #leq 4#, G^{2}(x) $ tenemos por el teorema de convergencia dominada [19] que $ #| f-f_{n} #|_{L^{2}} #rar 0 $. ## #qed

#vspace{5 mm}

Es importante observar que $L^{2}(#R^{n})$ es un espacio de Hilbert separable. Para mostrar esto primero debemos de recordar algunos conceptos b#'asicos de la topolog#'ia de $#R^{n}$ y la medida de Lebesgue.##

Sea $#mathbb{O}$ la familia de los conjuntos abiertos de $#R^{n}$. Una familia $#B #subseteq #mathbb{O} $ es una {#bf base} de la topolog#'ia de $#R^{n}$ si para cada $#x #in #R^{n}$ y cada vecindad $V(#x)$, hay alg#'un conjunto $O #in #B$ tal que, $ x #in O #subseteq V(x) $. ##
Ya que cualquier conjunto abierto $#tilde{O}$ es vecindad de cada uno de sus puntos, tenemos que:
$$ #tilde{O}= #bigcup_{#tilde{O} #supseteq O #in #B }#, O $$

Es f#'acil ver que el conjunto de las bolas abiertas con centro racional (vectores de $#R^{n}$ con coordenadas racionales) y con radio $#frac{1}{n}$ ($n #in #mathbb{N}$) forman una base numerable de la topolog#'ia de $#R^{n}$. Recordemos que un espacio topol#'ogico con una base numerable es conocido como un {#bf espacio segundo numerable}. ##
tambi#'en recordemos que si $A #in #Sigma$, en donde $#Sigma$ denota a la sigma algebra de los conjuntos medibles en $#R^{n}$, entonces:
$$ m(A)= #inf_{A #subseteq O} m(O) $$
donde $O #in #mathbb{O} $. La propiedad anterior hace de la medida de Lebesgue una {#bf medida de Borel exterior regular}.##
Sea $W #subset V$ un sub conjunto de un espacio vectorial $V$. Recordemos que el conjunto $W$ es {#bf total} si el conjunto de sus combinaciones lineales finitas, $#hbox{Span}(W)$ es denso en $V$. 

#vspace{5 mm}

#begin{lemma}#label{l2-sep}
El espacio $L^{2}(#R^{n})$ es separable.
#end{lemma}
#n {#bf Demostraci#'on}##
Por construcci#'on de la integral de Lebesgue, el conjunto de todas las funciones caracter#'isticas 
$$ #X_{#Sigma}=#{ #X_{A}(#x) | A #in #Sigma #,,#, m(A) < #infty #} $$ 
es un conjunto total en $L^{2}(#R^{n})$.##
Sea $A #in #Sigma$ fijo. Ya que la medida de Lebesgue es exterior regular, existe una sucesi#'on decreciente de conjuntos abiertos $#{ O_{n} #}$ tal que, $ m(O_{n}) #rar m(A) $ cuando $ n #rar #infty $. Ya que $m(A) < #infty$ podemos suponer sin restricci#'on que $m(O_{n}) < #infty $ $#forall n$. De esta forma, 
$$ m(O_{n} #setminus A )= m(O_{n}) - m(A) #rar 0 $$ 
cuando $ n #rar #infty $. Luego, el teorema de convergencia dominada de Lebesgue implica que:
$$ #| #X_{A} - #X_{O_{n}} #|_{L^{2}} #rar 0 $$
cuando $ n #rar #infty $. as#'i, hemos encontrada una sucesi#'on de funciones caracter#'isticas de conjuntos abiertos que converge en norma $L^{2}$ a la funci#'on caracter#'istica de un conjunto medible. Luego, el conjunto de las funciones caracter#'isticas $ #X_{op}=#{ #X_{O} #,|#, O #in #mathbb{O} #,,#, m(O)< #infty #} $ es un conjunto denso en $#X_{#Sigma}$ y por tanto es total en $L^{2}(#R^{n})$.##
Ya que $#R^{n}$ es un espacio segundo numerable, podemos encontrar una base numerable $#B$ para su topolog#'ia. Entonces cada conjunto abierto $O$, se puede escribir como:
$$ O= #bigcup_{j=1}^{#infty} #tilde{O}_{j} $$
con $#tilde{O}_{j} #in #B$. Mas aun, considerando el conjunto de todas las uniones finitas de elementos de $#B$, podemos asumir que $#bigcup_{j=1}^{n} #tilde{O}_{j} #in #B $. as#'i existe una sucesi#'on creciente  $ #tilde{O}_{n} #subseteq #tilde{O}_{n+1} #rar O $ con $#tilde{O}_{n} #in #B$. Por el teorema de convergencia mon#'otona $ #| #X_{O} - #X_{#tilde{O}_{n}} #|_{L^{2}} #rar 0 $ cuando $n #rar #infty$. as#'i el conjunto de las funciones caracter#'isticas $#X_{#B}= #{ #X_{#tilde{O}} | #tilde{O}#in #B  #}$ es numerable y denso en $#X_{op}$ y por tanto es total en $L^{2}(#R^{n})$. Luego el conjunto de todas las combinaciones lineales finitas racionales (escalares complejos con parte real e imaginaria racional) de $#X_{#B}$ es numerable y denso en $#hbox{Span}(#X_{#B}) $ y por lo tanto tambi#'en es denso en $ L^{2}(#R^{n}) $. ## #qed

#vspace{5 mm}

Por el teorema #ref{sep-bas} el resultado anterior implica que $L^{2}(#R^{n})$ posee una base ortonormal numerable.##
Ahora mostraremos que el conjunto de las funciones continuas con soporte compacto es denso en $L^{2}(#R^{n})$. Pero para esto primero debemos recordar algunos detalles acerca de la topolog#'ia de $#R^{n}$ y la medida de Lebesgue.##

Recordemos que $#R^{n}$ es un espacio {#bf localmente compacto}, es decir, cada punto $#x #in #R^{n}$ tiene una vecindad compacta (por ejemplo una bola cerrada con centro en $#x$). ##

tambi#'en debemos de recordar que para $#R^{n}$ podemos encontrar una cubierta numerable $#{ X_{j} #}_{j=1}^{#infty}$ con $ m(X_{j})< #infty $, $#forall j$ (por ejemplo el conjunto de bolas con centro en el orinen y radio $n #in #mathbb{N}$). Esto hace que la medida de Lebesgue en $#R^{n}$ sea {#bf sigma finita}.##

as#'i tambi#'en, es preciso recordar que si $A #subset #Sigma$ entonces:
$$ m(A)= #sup_{K #subseteq A} m(K) $$
en donde $k$ denota un sub conjunto compacto de $#R^{n}$. Esto hace que la medida de Lebesgue sea una {#bf medida de Borel interior regular}. ##

As#'i tambi#'en necesitamos establecer el siguiente resultado acerca de como construir una funci#'on continua con soporte compacto en $#R^{n}$.

#begin{lemma}
{#bf (Urysohn)} Sean $C_{1}$ y $C_{2}$ dos conjuntos de $#R^{n}$ cerrados y disjuntos. Entonces existe una funci#'on continua $f: #R^{n} #rar [0,1]$ tal que, $f$  es cero en $C_{1}$ y uno en $C_{2}$.##
Si $C_{1}$ es compacto entonces uno puede tomar a $f$ con soporte compacto.  
#end{lemma}
#n {#bf Demostraci#'on}##
Definimos a la funci#'on $dist(#x,C): #R^{n} #rar [0,#infty)$ donde $C #subset #R^{n}$ es cerrado, como:
$$ dist(#x,C):= #inf_{#x_{0} #in C} #|#x- #x_{0} #|= #| #x - #x^{*} #| $$  
para alg#'un $#x^{*} #in C$ cuya existencia queda siempre asegurada ya que $C$ es cerrado. Sea $#{#x_{n}#}_{n=1}^{#infty} #subset #R^{n} $ tal que $#x_{n} #rar #x_{0}$ cuando $n #rar #infty$. Supongamos que:
$$ dist(#x_{0},C)= #| #x_{0} - #x_{0}^{*} #| #,#,#,#, #x_{o}^{*} #in C $$
$$ dist(#x_{n},C)= #| #x_{n} - #x_{n}^{*} #| #,#,#,#, #x_{n}^{*} #in C #,,#, #forall n #in #mathbb{N} $$ 
Luego, por la continuidad de la norma en $#R^{n}$,
$$ 0 #leq #lim_{n #rar #infty} dist(#x_{n},C) = #lim_{n #rar #infty} #| #x_{n}-#x_{n}^{*} #| #leq #lim_{n #rar #infty} #| #x_{n}-#x_{0}^{*} #| = #|#x_{0}-#x_{0}^{*} #| $$
por otro lado,
$$ 0 #leq #| #x_{0}-#x_{0}^{*} #| #leq #| #x_{0}-#x_{n}^{*} #| #leq #|#x_{0}-#x_{n}#| + #| #x_{n}-#x_{n}^{*} #| = #|#x_{0}-#x_{n}#| + dist(#x_{n},C) $$
Para toda $n #in #mathbb{N}$. Tomando el limite cuando $n #rar #infty$ en la desigualdad anterior y utilizando el hecho de que $#lim_{n #rar #infty} #|#x_{0}-#x_{n}#| =0 $, obtenemos:
$$ 0 #leq #| #x_{0}-#x_{0}^{*} #| #leq #lim_{n #rar #infty} dist(#x_{n},C) $$
por tanto, $ #lim_{n #rar #infty} dist(#x_{n},C)=#| #x_{0}-#x_{0}^{*} #|= dist(#x_{0},C) $. Luego la funci#'on:
$$ f(#x)= #frac{ dist(#x,C_{1}) }{ dist(#x,C_{1})+dist(#x,C_{2}) } $$
esta definida en $#R^{n}$, es continua y tiene valor cero en $C_{1}$ y uno en $C_{2}$. Esto muestra la primera pate del teorema.##

Ahora supongamos que $C_{1}$ es compacto. Observemos que existe un conjunto abierto $O$ tal que $#ol{O}$ es compacto y $C_{1} #subset O #subset #ol{O} #subset #R^{n} #setminus C_{2}$. De hecho para cada $#x$, hay una bola $B_{#epsilon}(#x)$ tal que $#ol{B_{#epsilon}(#x)}$ es compacto y $ #ol{B_{#epsilon}(#x)} #subset #R^{n} #setminus C_{2} $. Ya que $C_{1}$ es compacto un n#'umero finito de estas bolas cubren a $C_{1}$ y podemos hacer que la uni#'on de estas bolas sea $O$. Ahora remplazamos a $C_{2}$ por $#R^{n} #setminus #ol{O}$ en la funci#'on $f$.  ## #qed

#vspace{5 mm}

#begin{theorem}
El conjunto $C_{c}(#R^{n})$ de funciones continuas con soporte compacto, es denso en $L^{2}(#R^{n})$. 
#end{theorem}
#n {#bf Demostraci#'on}##
Sea $#X_{#Sigma}$ como en la demostraci#'on del teorema anterior; sabemos que por construcci#'on de la integral de Lebesgue que este conjunto es total en $L^{2}(#R^{n})$. Luego, ya que la medida de Lebesgue es interior regular el conjunto de funciones caracter#'isticas $#X_{c}= #{ #X_{k} |#, k #hbox{ es compacto } #}$ es denso en $#X_{#Sigma}$ y por tanto es total en $L^{2}(#R^{n})$. Por tanto bastara con probar que $#X_{k}$ se puede aproximar por funciones continuas. Ya que la medida de Lebesgue es exterior regular, para todo $#epsilon >0$, existe un conjunto abierto $O #supset k$ tal que $ m(O #setminus k) #leq #epsilon $. Por el lema de Urysohn existe una funci#'on $f_{#epsilon}$ con valor uno en $k$ y valor cero fuera de $O$. Luego,
$$ #int_{#R^{n}}  |#X_{k}(#x) - f_{#epsilon}(#x)|^{2} #, d#x  = #int_{O#setminus k} |f_{#epsilon}(#x)|^{2}#, d#x #leq m(O #setminus k) #leq #epsilon $$
por tanto $#| f_{#epsilon} - #X_{k} #| #rar 0$. ## #qed 

#newpage

#section{El Producto tensorial $L^{2}(#R^{n}) #otimes L^{2}(#R^{m}) $}

En esta secci#'on discutiremos como la multiplicaci#'on de n#'umeros complejo hace que el producto tensorial $ L^{2}(#R^{n}) #otimes L^{2}(#R^{m}) $ surja de manera natural. ##

Consideremos a los espacios de Hilbert $L^{2}(#R^{n})$ y $L^{2}(#R^{m})$ con medidas de Lebesgue $m_{n}$ y $m_{m}$ respectivamente. Por el lema #ref{l2-sep} existen bases numerables $#{#vp_{k}(#x) #}$ y $#{ #psi_{l}(#y) #}$ para $L^{2}(#R^{n})$ y $L^{2}(#R^{m})$ respectivamente. Luego, claramente el conjunto $ #{ #vp_{k}(#x)#, #psi_{l}(#y) #} $ es ortonormal en el espacio $ L^{2}( #R^{n} #times #R^{m} ) $, en donde $m_{n} #cdot m_{m}$ es la medida en el espacio $#R^{n} #times #R^{m}$. El hecho de que $ #{ #vp_{k}(#x)#, #psi_{l}(#x) #} $ es una base de este ultimo espacio, se puede ver mediante el siguiente argumento:##
Supongamos que $f(x,y) #in L^{2}( #R^{n}#times #R^{m} )$ y que,
$$  #iint_{#R^{n} #times #R^{m}} #ol{f(#x,#y)}#, #vp_{k}(#x) #, #psi_{l}(#y)#, d#x #, d#y =0 $$
para toda $k$ y $l$. Por de teorema de Fubini podemos escribir,
$$ #int_{#R^{m}} #left( #int_{#R^{n}} #ol{f(#x,#y)}#, #vp_{k}(#x)#, d#x #right)#, #psi_{l}(#y)#, d#y =0$$
Ya que $ #{ #psi_{l} #} $ es una base de $L^{2}(#R^{m})$ lo anterior implica que,
$$ #int_{#R^{n}} #ol{f(#x,#y)}#, #vp_{k}(#x)#, d#x =0$$
excepto en un conjunto $ S_{k} #subset #R^{m} $ con $ m_{m}(S_{k})=0 $. Entonces para $ #y #notin #bigcup_{k} S_{k} $, $ #int_{#R^{n}} #ol{f(x,y)}#, #vp_{k}(#x)#, d#x=0 $, para toda $k$; esto implica que $ f(#x,#y)=0$ para casi toda $#x #in #R^{n}$ en la medida $m_{n}$. Entonces $ f(#x,#y)=0 $ para casi toda $(#x,#y)$ en la medida $ m_{n}#cdot m_{m} $ de $#R^{n} #times #R^{m} $. as#'i, $#{ #vp_{k}(#x)#, #psi_{l}(#y) #}$ es un conjunto ortonormal completo y por lo tanto una base para $ L^{2}( #R^{n}#times #R^{m} ) $.##

Por la proposici#'on #ref{pt-b} tenemos que el conjunto $#{ #vp_{k} #otimes #psi_{l} #}$ es una base ortonormal del producto tensorial $L^{2}(#R^{n}) #otimes L^{2}(#R^{m})$. Ahora, sea $U$ un operador lineal tal que,
$$ U: #vp_{k} #otimes #psi_{l} #rar #vp_{k}(#x)#, #psi_{l}(#y) $$
Es claro que $U$ es una biyecci#'on entre la base de $ L^{2}(#R^{n}) #otimes L^{2}(#R^{m}) $ y la de $L^{2}(#R^{n} #times #R^{m})$, luego $U$ se puede extender de manera #'unica a un operador lineal unitario entre estos dos espacios.##

Notemos que si $ f #in L^{2}(#R^{n}) $ y $ g #in L^{2}(#R^{m}) $, entonces ya que $U$ es lineal: 
#begin{align*}
U(f #otimes g) &= U#left( #sum_{k} c_{k}#, #vp_{k} #otimes #sum_{l} d_{l}#, #psi_{l} #right)##
&= U #left( #sum_{k#,l} c_{k}#,d_{l}#, #vp_{k}#otimes #psi_{l} #right)##
&=  #sum_{k#,l} c_{k}#,d_{l}#, #vp_{k}(#x) #, #psi_{l}(#y) ##
&= f(#x)#, g(#y)
#end{align*}

Debido a esta propiedad se dice que $ L^{2}(#R^{n} #times #R^{m}) $ y $L^{2}(#R^{n}) #otimes L^{2}(#R^{m})$ son $naturalmente$ $#,isomorfos$. as#'i, hemos demostrado el siguiente teorema:

#begin{theorem}#label{l2-ten}
Existe un #'unico isomorfismo entre los espacios $ L^{2}(#R^{n}) #otimes L^{2}(#R^{m}) $ y $ L^{2}(#R^{n} #times #R^{m}) $ tal que, $ f #otimes g #rar fg $.
#end{theorem}
En la practica por conveniencia, se suele omitir al isomorfismo anterior y simplemente se define el producto tensorial de dos elementos de $L^{2}(#R^{n})$ y $L^{2}(#R^{m})$ como $f#otimes g := fg$, en donde $f$ y $g$ son representantes de sus respectivas clases de equivalencia.


#chapter{An#'alisis de Fourier}

El an#'alisis de Fourier a demostrado ser de crucial importancia en muchas #'areas de las matem#'aticas, la f#'isica y la ingenier#'ia. En mec#'anica cu#'antica, la transformada de Fourier es una herramienta esencial para la soluci#'on y la interpretaci#'on de la ecuaci#'on de Schr#"odinger, ya que esta ayuda a entender como la funci#'on de onda puede describir de manera simultanea las propiedades de localizaci#'on y la distribuci#'on de momento de una part#'icula.##

En este capitulo recopilamos algunos resultados del an#'alisis de Fourier que usaremos mas adelante. Las demostraciones formales de estos resultados son largas y necesitan de una teor#'ia mas elaborada, por lo cual solo expondremos sus enunciados y sus aplicaciones.

#section{Series de Fourier de funciones complejas}
El an#'alisis de Fourier es el $arte$ de escribir funciones de onda (funciones peri#'odicas) arbitrarias, como la superposici#'on de funciones trigonom#'etricas. Como un primer paso en el desarrollo de esta teor#'ia consideramos funciones complejas peri#'odicas y sus respectivas series de Fourier.##

#paragraph{Definiciones b#'asicas:} Dado un n#'umero real $L >0$, definimos a los n#'umeros:
#begin{equation}
k_{n}^{(L)} = n#, #frac{#pi}{L} #,,#,#,#,#,#, n =0,#pm1,#pm2,#pm3,...
#end{equation}
y a las funciones:
#begin{equation}#label{o-plan}
u_{n}^{(L)}(x)= #frac{1}{#sqrt{2L}}#, #exp ( i k_{n}^{(L)} x )
#end{equation}
para toda $x$ y $n$. Con la vista puesta en las aplicaciones para la mec#'anica cu#'antica, llamaremos a las funciones $u_{n}^{(L)}(x)$, {#bf ondas planas estacionarias} con n#'umero de onda $k_{n}$. Observemos que cada $u_{n}^{(L)}(x)$ es una funci#'on trigonom#'etrica compleja,
$$ u_{n}^{(L)}(x)= #frac{1}{#sqrt{2L}}#, #left( #cos(k_{n}^{(L)} x) + i#, #sen (k_{n}^{(L)} x)  #right) $$
as#'i tambi#'en, hay que observar que cada una de estas funciones es peri#'odica con periodo $2L$. Debido a esta periodicidad es suficiente el restringir todas nuestras consideraciones a un intervalo de longitud $2L$, digamos al intervalo $[-L,L]$.##

Claramente toda suma finita (superposici#'on o combinaci#'on lineal) de la forma,
#begin{equation}#label{F-sum}
#psi(x)= #sum_{n=-N}^{N} c_{n}#, u_{n}^{(L)}(x) 
#end{equation}
con $x #in [-L,L]$ y con coeficientes complejos arbitrarios $c_{n}$, representa a una funci#'on $suave$ (continuamente diferenciable) en el intervalo $[-L,L]$, con la propiedad, $#psi(-L)= #psi(L)$. La expresi#'on #eqref{F-sum} es conocida como {#bf suma trigonom#'etrica} o {#bf suma de Fourier}. Para $n #geq 0$ la suma,
#begin{equation}
c_{n}#, u_{n}^{(L)}(x) + c_{-n}#, u_{-n}^{(L)}(x)
#end{equation}
se conoce como un {#bf sumando de Fourier} de orden $n$.

#vspace{15 mm}

#section{Series de Fourier de funciones de cuadrado integrable en $[-L,L]$}
Decimos que una funci#'on compleja $#psi$, es de cuadrado integrable en el intervalo $[-L,L]$, si y solo si:
$$ #int_{-L}^{L} |#psi(x)|^{2}#, dx < #infty $$
En el capitulo anterior estudiamos de forma muy general al espacio de Hilbert $L^{2}(#R^{n})$, al cual nos referimos como el espacio de las funciones de cuadrado integrable en todo $#R^{n}$; de manera completamente an#'aloga a como se obtuvo este espacio, se construye al espacio de Hilbert $L^{2}([-L,L])$, de las funciones de cuadrado integrable en el intervalo $[-L,L]$.##

Es un resultado fundamental del an#'alisis de Fourier el que toda funci#'on $#psi #in L^{2}([-L,L])$, puede ser aproximada, en la norma de $L^{2}([-L,L])$, por ondas planas estacionarias (ec. #ref{o-plan}), tal y como lo establecemos a continuaci#'on:

#paragraph{Series de Fourier de funciones en $L^{2}([-L,L])$ :} Sea $#psi #in L^{2}([-L,L])$. Entonces, en la norma de $L^{2}([-L,L])$:
#begin{equation}#label{sn-con}
#lim_{N #rar #infty} #left#| #psi - #sum_{n=-N}^{N} c_{n}#, u_{n}^{(L)} #right#|=0
#end{equation} 
con,
#begin{equation}
c_{n}= #int_{-L}^{L} #ol{u_{n}^{(L)}(x)}#, #psi(x)#, dx= (u_{n}^{(L)}(x),#psi)
#end{equation}
en donde $(#cdot , #cdot)$ denota al producto interno de $L^{2}([-L,L])$. A la suma trigonom#'etrica infinita,
#begin{equation}
#sum_{n=-#infty}^{#infty} c_{n}#, u_{n}^{(L)} =#lim_{N#rar #infty} #sum_{n=-N}^{N} c_{n}#, u_{n}^{(L)}
#end{equation}
la llamaremos la {#bf serie de Fourier} de la funci#'on $#psi$. Los coeficientes complejos $c_{n}$, son cuadrado sumables; mas aun, su suma tiene el valor:
#begin{equation}
#sum_{n=-#infty}^{#infty} |c_{n}|^{2}= #int_{-L}^{L} |#psi(x)|^{2}#, dx < #infty
#end{equation} #rightline{$#dag$}

#vspace{5 mm}

La convergencia de la serie de Fourier de una funci#'on, en la norma de $L^{2}([-L,L])$ debe de ser entendida como {#bf convergencia en la media}, esto es convergencia en el sentido de que:
$$ #lim_{N #rar #infty} #left#| #psi - #sum_{n=-N}^{N} c_{n}#, u_{n}^{(L)} #right#|=  #left#| #psi - #sum_{n=-#infty}^{#infty} c_{n}#, u_{n}^{(L)} #right#|=0 $$ 
como vimos en el capitulo anterior, esta expresi#'on solo implica que $#psi(x)=#sum_{n=-#infty}^{#infty} c_{n}#, u_{n}^{(L)}(x) $, en C.T.P. as#'i, la convergencia en la media no implica que la serie de Fourier de una funci#'on converja a esta para todo valor fijo de $x #in [-L,L]$ es decir, la serie no necesariamente converge puntualmente. Esta observaci#'on es importante si nuestra funci#'on $#psi$ tiene discontinuidades de $salto$. Recordemos que una funci#'on de cuadrado integrable no necesita ser continua, por ejemplo, la funci#'on caracter#'istica del intervalo $[-1,1]$, $#chi_{[-1,1]}(x)$ tiene discontinuidades de tipo $salto$ en $x= #pm 1$, pero claramente es de cuadrado integrable en todo intervalo $[-L,L]$.##

Si tomamos solamente una cantidad finita de sumandos de la serie de Fourier de una funci#'on de cuadrado integrable, obtenemos una aproximaci#'on de esta a trav#'es de una suma de Fourier finita $#psi_{N}$. Toda suma de Fourier finita es una funci#'on $suave$ con la propiedad $ #psi_{N}(-L)= #psi(L) $ (ec. #eqref{o-plan}). Si intentamos aproximar de esta manera a una funci#'on con discontinuidades de salto, observaremos que cerca de los puntos de discontinuidad, las sumas de Fourier finitas empiezan a oscilar muy r#'apidamente, sin que las oscilaciones bajen su amplitud mientras la sumas crecen en t#'erminos, haciendo que la serie de Fourier no converja puntualmente en donde la funci#'on es discontinua, pero si en el sentido de la ecuaci#'on #eqref{sn-con}.##

Si $#psi #in L^{2}([-L,L])$ es una funci#'on $suave$ (continuamente diferenciable en $[-L,L]$), con la propiedad $#psi(-L) = #psi(L) $, podemos asegurar la convergencia puntual y uniforme de la serie de Fourier a la funci#'on, es decir:
#begin{equation}
#max_{-L#leq x #leq L} #left[ #sum_{n=-N}^{N} c_{n}#, u_{n}^{(L)}(x) - #psi(x) #right] #rar 0
#end{equation}
cuando $N #rar #infty$.

#newpage

#section{La transformada de Fourier}
Es interesante estudiar el comportamiento de las series de Fourier en el limite cuando la longitud del intervalo de periodicidad de una funci#'on tiende a infinito ya que esto nos conduce naturalmente al estudio de funciones no peri#'odicas.##

Consideremos una funci#'on suave $#psi$, que se anula fuera del intervalo $[-L_{0},L_{0}]$. Para $L> L_{0}$, definimos a la funci#'on:
#begin{equation}#label{ftran}
#hat{#psi}(k)= #frac{1}{#sqrt{2 #pi}} #int_{-L}^{L} e^{-ikx}#, #psi(x)#, dx
#end{equation}
Para todo $k #in #R$.##
Luego, la serie de Fourier de la funci#'on $#psi$ converge puntualmente y esta dada por:
$$#psi(x) = #frac{1}{#sqrt{2#pi}} #sum_{n=-#infty}^{#infty} #hat{#psi}(k_{n}^{(L)})#, e^{ik_{n}^{(L)}x} #, #frac{#pi}{L} $$
#begin{equation}#label{rsum}
= #frac{1}{#sqrt{2#pi}} #sum_{n=-#infty}^{#infty} #hat{#psi}(k_{n}^{(L)})#, e^{ik_{n}^{(L)}x} #, ( k_{n+1}^{(L)} - k_{n}^{(L)})
#end{equation}
en donde,
$$ k_{n}^{(L)}= #frac{n#pi}{L} #,#,#,#,;#,#,#, k_{n+1}^{(L)}-k_{n}^{(L)}= #frac{#pi}{L}= #Delta^{(L)}#, k $$

La ecuaci#'on #eqref{rsum} se puede interpretar como la aproximaci#'on de una integral a trav#'es de una suma de Rieman:
#begin{equation}#label{intapp}
#sum_{n=-#infty}^{#infty} #hat{#psi}(k_{n}^{(L)})#, e^{ik_{n}^{(L)}x} #, #Delta^{(L)} k #approx #int_{-#infty}^{#infty} #hat{#psi}(x) #, e^{ikx}#, dk
#end{equation}

as#'i, en el limite cuando $L #rar #infty$ por las ecuaciones #eqref{ftran} y #eqref{intapp}, obtenemos las expresiones:
#begin{equation}#label{finv}
#psi(x)= #frac{1}{#sqrt{2 #pi}} #int_{-#infty}^{#infty} #hat{#psi}(x)#, e^{ikx}#, dk
#end{equation}
#begin{equation}#label{ff}
#hat{#psi}(x) = #int_{- #infty}^{#infty} #psi(x)#, e^{-ikx}#, dx
#end{equation}
La funci#'on $#hat{#psi}$ es conocida como la {#bf transformada de Fourier} de la funci#'on $#psi$. Conoceremos como {#bf transformaci#'on de Fourier} al mapeo $#F: #psi #rar #hat{#psi}$, dado por la ecuaci#'on #eqref{ff} y como {#bf transformaci#'on inversa de Fourier} al mapeo $#F^{-1}: #hat{#psi} #rar #psi $, dado por la ecuaci#'on #eqref{finv}.##

La transformada de Fourier puede ser definida para todas las funciones integrables en el sentido de Lebesgue, ya que la existencia de la integral $#int #psi(x)$, es equivalente a la existencia de la integral $#int |#psi(x)|$ y por tanto tambi#'en a la existencia de la integral $ #int e^{ikx}#, #psi(x) $. ##

#newpage

#subsubsection{La transformada de Fourier en $n$ dimensiones}
La transformada de Fourier puede ser generalizada f#'acilmente a funciones de varias variables. Tomemos por ejemplo a una funci#'on integrable en $#R^{2}$, $#psi (x_{1},x_{2})$. Hagamos primero una transformaci#'on de Fourier respecto a la variable $x_{1}$ y despu#'es respecto a la variable $x_{2}$. as#'i obtenemos:
$$ #hat{#psi}(k_{1},k_{2})= #frac{1}{#sqrt{2 #pi}} #int_{-#infty}^{#infty} #left[ #frac{1}{#sqrt{2 #pi}} #int_{-#infty}^{#infty} #psi(x_{1},x_{2})#, e^{-ik_{1}x}#, dx_{1}  #right]#, e^{-ik_{2}x}#, dx_{2} $$
#begin{equation}
= #frac{1}{2 #pi} #int_{#R^{2}} #psi(x_{1},x_{2})#, e^{-i(k_{1}x_{1}+k_{2}x_{2})}#, dx_{1}#, dx_{2}
#end{equation}
La integral de la ecuaci#'on anterior esta bien definida y es independiente del orden de integraci#'on, siempre y cuando $#psi$ sea una funci#'on integrable en $#R^{2}$. Lo anterior sugiere la siguiente generalizaci#'on de la transformada de Fourier para funciones de varias variables:

#paragraph{Transformada de Fourier para funciones de varias variables: } Sea $#psi (#x)$, $ #x #in #R^{n}$, una funci#'on integrable. La transformaci#'on de Fourier $#F$, mapea a la funci#'on $#psi(#x)$, en la funci#'on $#hat{#psi}(#k)$, definida por,
#begin{equation}#label{TF-nD}
#hat{#psi}(#k)= ( #F #psi )(#k)= #frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{-i#k #cdot #x}#, #psi(#x)#, d#x
#end{equation}
#rightline{$#dag$}
#vspace{3 mm}

El espacio $#R^{n}$ formado por las variables independientes $#k= (k_{1},...,k_{n}) #in #R^{n}$ es conocido usualmente como el {#bf espacio de Fourier}, o en vista de las aplicaciones que esta teor#'ia tiene en la mec#'anica cu#'antica, tambi#'en se le conoce como el {#bf espacio de momentos}; esto lo distingue del {#bf espacio de posiciones} formado por las variables $ #x #in #R^{n} $. ##
En vista de la ecuaci#'on #eqref{finv} y siguiendo un procedimiento similar al de las funciones de una variable, definimos a la transformada inversa de Fourier para funciones integrables de varias variables $#hat{#psi}(#k)$, como:
#begin{equation}
#psi(#x)= #frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{i#k #cdot #x} #, #hat{#psi}(#k)#, d#k
#end{equation}
Lo anterior es conocido como el {#bf teorema de inversi#'on de Fourier} el cual enunciamos a continuaci#'on.
#begin{theorem}
Si $#psi$ es una funci#'on integrable en $#R^{n}$ y si su transformada de Fourier,
$$ #hat{#psi}(#k) = #F#psi(#k)=#frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{-i #k #cdot #x} #, #psi(#x)#, d#x $$
es tambi#'en una funci#'on integrable (como funci#'on de $#k #in #R^{n}$), entonces $#psi$ tiene la representaci#'on:
$$ #psi(#x) =  #F^{-1}#hat{#psi}(#x) =#frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{i #k #cdot #x} #, #hat{#psi}(#k)#, d#k $$ 
#end{theorem}
#rightline{$#dag$}
#vspace{5 mm}

Como hemos observado, una funci#'on integrable y su transformada de Fourier dependen una de la otra de una manera sim#'etrica. Aparte del cambio de signo en la expresi#'on de la transformada inversa, debemos de entender que esta no es mas que una transformaci#'on de Fourier del espacio de momentos al espacio de posiciones, mas aun tenemos la relaci#'on:
#begin{equation}
(#F^{-1}#psi)(#x)= (#F #psi)(-#x)
#end{equation}

#newpage

El siguiente lema describe el rango de la transformaci#'on de Fourier.
#begin{lemma} 
(Riemann-Lebesgue) La transformada de Fourier $#hat{#psi}= #F #psi$ de una funci#'on integrable $#psi$ es una funci#'on continua con las siguientes propiedades:
#begin{enumerate}
#item $#hat{#psi}$ es acotada, $ #sup_{#k #in #R^{n}} |#hat{#psi}(#k)| #leq #int_{#R^{n}} |#psi(#x)|#, d#x $. 
#item $ #hat{#psi}(#k) #rar 0 $, cuando $|#k|#rar #infty$.
#end{enumerate}
El mismo resultado es valido para la transformada inversa de una funci#'on integrable en el espacio de momentos.
#end{lemma}

as#'i tambi#'en, enunciamos el siguiente teorema valido para las funciones en $L^{2}(#R^{n})$ con transformada de Fourier.

#begin{theorem}#label{FPt}
(Fourier-Plancherel) Si $#psi$ es una funci#'on integrable y de cuadrado integrable, su transformada de Fourier $#hat{#psi}= #F#psi $, tambi#'en es de cuadrado integrable, adem#'as:
$$ #int_{#R^{n}} |#psi(#x)|^{2}#, d#x = #int_{#R^{n}} |#hat{#psi}(#k)|^{2}#, d#k  $$
#end{theorem} 

El teorema anterior establece que la transformaci#'on de Fourier $#F$, es continua respecto a la norma del espacio de Hilbert $L^{2})(#R^{n})$. Si $#psi$ y $#phi$ son cercanas entre si respecto a la norma de $L^{2}(#R^{n})$ entonces, $#hat{#psi}$ y $#hat{#phi}$ tambi#'en son cercanas entre si, ya que $#| #psi - #phi #| = #| #hat{#psi} - #hat{#phi} #|$.  

#vspace{3 mm}

#subsubsection{Extensi#'on de la transformada de Fourier a $L^{2}(#R^{n})$}
Podemos extender la definici#'on de la transformaci#'on de Fourier $#F$ a todas las funciones de cuadrado integrable del espacio de Hilbert $L^{2}(#R^{n})$. Supongamos que la funci#'on $#psi$ es de cuadrado integrable; es un hecho que la funci#'on $#psi$, no es necesariamente integrable (tomemos por ejemplo a la funci#'on $|#x|^{-1}$) y por tanto su transformada de Fourier puede no estar definida. Sin embargo, podemos aproximar a $#psi$ a trav#'es de funciones integrables de la siguiente forma: Para todo $n #in #mathbb{N}$, definimos a las funciones,
$$ #psi_{n}(#x)=#begin{cases} #begin{matrix} #psi(#x) & si#; |#x| #leq n ## 0 & si#; |#x|>n #end{matrix} #end{cases} $$
Entonces, cada $#psi_{n}$ es integrable y observamos que,
$$ #| #psi - #psi_{n} #|^{2}= #int_{|#x|>n} |#psi(#x)|^{2}#, d#x #rar 0 $$
cuando $n #rar #infty$. as#'i, una funci#'on arbitraria de cuadrado integrable es el limite de una sucesi#'on de funciones integrables. Esto demuestra que el conjunto de funciones de $L^{2}(#R^{n})$ que son integrables, es denso en $L^{2}(#R^{n})$. Al ser $L^{2}(#R^{n})$ un espacio de Hilbert, la sucesi#'on $#{ #psi_{n} #}$ es de Cauchy. Luego por el teorema #ref{FPt} y al estar cada una de las $#psi_{n}$ en el dominio de $#F$, la sucesi#'on $ #{  #F #psi_{n} #} $ tambi#'en esta en $L^{2}(#R^{n})$ y es de Cauchy; por lo tanto tiene limite en $L^{2}(#R^{n})$. Ya que la continuidad de la transformaci#'on de Fourier es una propiedad que deseamos conservar, definimos su extensi#'on a $L^{2}(#R^{n})$ de la siguiente forma: 

#paragraph{Transformaci#'on de Fourier en $L^{2}(#R^{n})$:} Sea $#psi #in L^{2}(#R^{n}) $ y sea $#{ #psi_{n} #}$ una sucesi#'on de funciones integrables tal que, $ #lim_{n #rar #infty} #psi_{n} = #psi $. Definimos a la transformaci#'on de Fourier de $#psi$ como:
#begin{equation}
#F #psi = #lim_{n #rar #infty} #F #psi_{n}
#end{equation}
as#'i, la transformada de Fourier de $#psi$ esta dada por,
#begin{equation}
#hat{#psi} = #lim_{n #rar #infty} #hat{#psi}_{n}
#end{equation}
#rightline{$#dag$}
 
#vspace{5 mm}

#n Debemos observar que la transformaci#'on de Fourier mapea a $L^{2}(#R^{n})$ en si mismo. Definimos a la transformaci#'on inversa de Fourier en $L^{2}(#R^{n})$ de manera similar. 

#paragraph{Transformaci#'on inversa de Fourier en $L^{2}(#R^{n})$:} Sea $#hat{#psi} #in L^{2}(#R^{n}) $ y sea $#{ #hat{#psi}_{n} #}$ una sucesi#'on de funciones integrables tal que, $ #lim_{n #rar #infty} #hat{#psi}_{n} =#hat{#psi} $. Definimos a la transformaci#'on inversa de Fourier de $#hat{#psi}$ como:
#begin{equation}
#F^{-1} #hat{#psi} = #lim_{n #rar #infty} #F^{-1} #hat{#psi}_{n}
#end{equation}
as#'i, la transformada inversa de Fourier de $#hat{#psi}$ esta dada por,
#begin{equation}
#psi = #lim_{n #rar #infty} #psi_{n}
#end{equation}
#rightline{$#dag$}

#subsubsection{Transformada de Fourier de la derivada}
Sea $#psi$ una funci#'on integrable en $#R$. Supongamos que $#psi$ es diferenciable y que tiende a cero cuando $|x| #rar #infty$, de tal forma que su derivada $#frac{d}{d x}#, #psi$, tambi#'en es integrable. Entonces integrando por partes y usando el hecho de que la funci#'on $#psi$ se anula en el infinito, obtenemos la siguiente expresi#'on para la transformada de Fourier de la derivada de $#psi$,
#begin{equation}#label{f-der}
#left(#F #, #frac{d #psi}{d x} #right)(k)= #int_{#R} #left( #frac{d}{dx}#,e^{-ikx} #right)#, #psi(x)#, dx = ik #, #F(#psi)(k) 
#end{equation}
Observamos que la derivada en el espacio de posiciones es transformada en una simple multiplicaci#'on por la variable $ik$ en el espacio de Fourier. Es f#'acil ver que la ecuaci#'on anterior se puede generalizar para derivadas de orden superior:
#begin{equation}#label{f-dern}
#left(#F #, #frac{d^{n} #psi}{d x^{n}} #right)(k)= #int_{#R} #left( #frac{d^{n}}{dx^{n}}#,e^{-ikx} #right)#, #psi(x)#, dx = (ik)^{n} #, #F(#psi)(k) 
#end{equation}

Si la funci#'on $x #psi (x)$ es integrable, entonces:
#begin{equation}
( #F x#psi )(k)= i #, #frac{d}{dk}(#F #psi)(k)
#end{equation}

#newpage

#chapter{part#'iculas libres}

Comenzamos nuestra exposici#'on de la teor#'ia cu#'antica con la derivaci#'on de la ecuaci#'on de Sch#"odinger para part#'iculas libres. Este es solo el primer paso en el desarrollo de esta teor#'ia ya que en situaciones mas realistas las part#'iculas pueden interact#'uan con campos de fuerza o con otras part#'iculas y pueden ser detectadas solamente a trav#'es de su interacci#'on con alg#'un dispositivo de medici#'on, aunque un buen entendimiento del comportamiento libre es importante para la descripci#'on asint#'otica de part#'iculas interactuantes en experimentos de dispersi#'on, tal y como veremos en las pr#'oximas secciones. Empezamos con una breve discusi#'on del origen de la teor#'ia cu#'antica.

#section{Part#'iculas y ondas}
Seg#'un la f#'isica cl#'asica las entidades fundamentales del mundo f#'isico son las part#'iculas y los campos. Las part#'iculas son capaces de moverse en todo el espacio como trozos bien localizados de materia, mientras que los campos se propagan por todo el espacio como ondas. Sin embargo, este modelo tuvo que ser reconsiderado cuando la disponibilidad de nuevas t#'ecnicas experimentales permiti#'o investigar fen#'omenos f#'isicos a escalas muy peque#~nas, las cuales est#'an normalmente mas all#'a del alcance de nuestros sentidos. En el comienzo del desarrollo de la teor#'ia cu#'antica moderna en la d#'ecada de 1920 era claro que muchos fen#'omenos f#'isicos pose#'ian una inherente dualidad onda - part#'icula. Por ejemplo, la explicaci#'on del efecto foto el#'ectrico parec#'ia indicar que la propagaci#'on de la luz, la cual hab#'ia sido considerada como un fen#'omeno ondulatorio cl#'asico, tambi#'en pose#'ia las caracter#'isticas de un haz de part#'iculas (fotones). Por otro lado, bajo ciertas circunstancias part#'iculas materiales tales como los electrones mostraban propiedades ondulatorias. ##
Es conveniente hacer un repaso acerca de las definiciones b#'asicas de los fen#'omenos ondulatorios.##

#paragraph{Ondas y fen#'omenos ondulatorios}
En el caso mas simple, un fen#'omeno ondulatorio es descrito por una onda plana. La onda describe alguna cantidad de varia peri#'odicamente en el espacio y en el tiempo, caracterizada por una longitud de onda $#lambda$ y un periodo de oscilaci#'on $T$. Definimos al n#'umero de onda $k$ y la frecuencia $w$ respectivamente como:
#begin{equation}
k= #frac{2 #pi}{#lambda} #,#,#,#,#,#,#,;#,#,#,#,#, w= #frac{2 #pi}{T}
#end{equation}
Una oda plana con n#'umero de onda $k$, es descrita por la funci#'on compleja,
#begin{equation}
u(x,t)= #exp ( ikx - iwt )
#end{equation}
La frecuencia $w$ puede depender del n#'umero de onda $w=w(k)$. A esta relaci#'on entre $k$ y $w$ se le conoce como la {#bf relaci#'on de dispersi#'on}. La funci#'on,
#begin{equation}
#phi (x,t)= kx - wt
#end{equation}
se conoce como la {#bf fase} de la onda plana. Un punto donde la fase tiene un valor fijo se mueve con {#bf velocidad de fase},
#begin{equation}
v= #frac{#lambda}{T}= #frac{w}{k}
#end{equation}
Para fen#'omenos ondulatorios en mas de una dimensi#'on definimos al vector de onda $#k$, el cual tiene magnitud $#| #k #|= 2#pi / #lambda$ y apunta en la direcci#'on de propagaci#'on de la onda. En este caso la onda plana es descrita por la funci#'on compleja,
#begin{equation}
u(#x,t)= #exp ( i #k #cdot #x - iwt )
#end{equation}
para un tiempo fijo $t$, los puntos $#x$ con una fase fija, est#'an caracterizados por, $ #k #cdot #x =#, constante $. Estos puntos forman un plano ortogonal al vector de onda $#k$. A estos planos se les conoce como los {#bf planos fase} y se mueven con velocidad de fase, $v= w/#| #k #|$ en la direcci#'on de $#k$.##

Con el objetivo de explicar el efecto foto el#'ectrico (A. Einstein 1905 [22]) se introdujeron las relaciones, 
#begin{equation}#label{E-rel}
E= #hbar #, w #,#,#,#,#,#,#,;#,#,#,#,#, #P= #hbar #, #k
#end{equation}
entre la energ#'ia $E$ de los fotones en un haz y la frecuencia $w$ de la onda de luz, as#'i como entre el momento de los fotones $#P$ y el vector de onda $#k$. En estas relaciones $#hbar$ denota a la {#bf constante de Planck}, la cual tiene el valor:
#begin{equation}
#hbar= 1.0546 #cdot 10^{-34} #, J #cdot s
#end{equation}
La dimensi#'on f#'isica energ#'ia $#times$ tiempo, es llamada {#bf acci#'on}.  

#paragraph{Efecto foto el#'ectrico:}
Este fen#'omeno consiste en la liberaci#'on de electrones de un metal en el cual incide luz con cierta longitud de onda. En este fen#'omeno se observa que la energ#'ia de los electrones liberados depende linealmente de la frecuencia de la onda de luz incidente (y no de la intensidad como podr#'ia esperarse). En 1905 este efecto fue descrito asumiendo que los electrones eran dispersados por part#'iculas de luz (fotones). Durante este proceso de dispersi#'on, la energ#'ia de los fotones es transferida a los electrones. Si la energ#'ia de los fotones es lo suficientemente grande, el metal emite electrones con energ#'ia cin#'etica $E_{k}= E - W$, en donde $W$ es el trabajo necesario para liberar un electr#'on del metal y $E$ es la energ#'ia recibida a trav#'es del fot#'on. Debido a la relaci#'on lineal observada entre la energ#'ia cin#'etica de los electrones liberados y la frecuencia $w$ del haz de luz, se tiene que la energ#'ia de un fot#'on en el haz de luz debe de ser proporcional a la frecuencia, esto es, $ E= #hbar #, w $. Ya que los fotones se mueven a la velocidad de la luz, deben de tener masa en reposo $m=0$. Para part#'iculas con masa en reposo igual a cero la relaci#'on relativista entre energ#'ia y momento se convierte en:
#begin{equation}
E= #sqrt{ c^{2}#, p^{2} + m^{2}#, c^{4} } = c#,p 
#end{equation}  
en donde $c$ es la velocidad de la luz. Por tanto los fotones deben de tener momento:
$$ p= #frac{E}{c}= #frac{#hbar #, w}{c} = #hbar #, k $$

#paragraph{Ondas de Materia:}
La descripci#'on del efecto foto el#'ectrico postula que a un fen#'omeno ondulatorio como la propagaci#'on de la luz, se le puede asociar un fen#'omeno corpuscular, como lo es la propagaci#'on de un haz de part#'iculas.##
En 1924 {#bf Louis de Broglie} [24] postulo lo contrario, que a un fen#'omeno corpuscular, como la propagaci#'on de un haz de electrones, se le puede asociar un fen#'omeno ondulatorio. ##
El asumi#'o que un haz de electrones puede ser descrito por una funci#'on de onda compleja de una onda plana:
#begin{equation}#label{utx}
u(x,t)= #exp ( i #k #cdot #x - iwt)
#end{equation}
en donde el vector $#k$ y la frecuencia $w$ est#'an determinadas por el momento y la energ#'ia de las part#'iculas como en las ecuaciones #eqref{E-rel}. Estas relaciones crean una conexi#'on entre las propiedades de ondas y part#'iculas haciendo que por tanto no tengan una interpretaci#'on obvia. Como hemos visto, un fot#'on posee un momento:
$$ p= #hbar #, k = #frac{2#pi#,#hbar}{#lambda} $$
de aqu#'i observamos que la longitud de onda del fot#'on esta dada por:
$$ #lambda = #frac{2 #pi #, #hbar}{p} $$
$de$ $Broglie$ propuso que la expresi#'on anterior era completamente general aplic#'andose tanto a fotones como a part#'iculas. Para estas ultimas $p$ denota el momento relativista $p=#gamma #, m v$ .## 

En 1927 Davison y Germer [23] verificaron experimentalmente las suposiciones de $de$ $Broglie$, realizando un experimento en el cual un haz de electrones es dispersado por un cristal, generando un patr#'on de interferencia propio de los fen#'omenos ondulatorios.

#paragraph{Dispersi#'on de electrones por un cristal (Experimento de Davison y Germer)}
Si hacemos incidir un haz de luz sobre un cristal los #'atomos de este dispersaran a las ondas de luz en todas direcciones. Cada #'atomo se convierte entonces en el origen de una onda dispersada, pero ya que los #'atomos de un cristal se encuentran ordenados en un arreglo peri#'odico y regular, hay algunas direcciones en las que todas las ondas dispersadas se encuentran en fase y son amplificadas por interferencia constructiva. as#'i, la intensidad de la onda dispersada muestra m#'aximos de intensidad muy marcados en algunas direcciones as#'i como m#'inimos de intensidad en otras, formando un patr#'on de interferencia. La condici#'on para tener una m#'aximo de intensidad a un #'angulo $#theta$ es, $2d#, #sen #theta = n #lambda$ (condici#'on de Bragg), en donde $n$ es un entero arbitrario, $d$ es la constante cristalina (par#'ametro de red) y $#lambda$ es la longitud de onda de la luz incidente. De esta forma el f#'isico alem#'an Max von Laue [26] fue capaz de mostrar la naturaleza ondulatoria de los rayos x. Si la longitud de onda es conocida, el patr#'on de interferencia de la onda dispersada puede ser utilizado para estudiar la estructura interna de un cristal. Esencialmente, el mismo fen#'omeno de interferencia se observa al dispersar un haz de electrones con una lamina delgada de metal. Si la constante cristalina de la lamina es conocida, la condici#'on de Bragg nos permite determinar la longitud de onda asociada al haz de electrones; Davison y Germer [23] compararon esta longitud de onda, obtenida de forma meramente experimental, con la hip#'otesis de $de$ $Broglie$, es decir, asumieron que la longitud de onda asociada al haz de electrones con energ#'ia $E= p^{2}/2m$, deber#'ia de ser, $#lambda = 2 #pi #hbar / p$ y encontraron que ten#'ian el mismo orden de magnitud, con lo cual probaron que un haz de part#'iculas tambi#'en posee propiedades ondulatorias.
 
#paragraph{La ecuaci#'on libre de Sch#"odinger}
Las ecuaciones #eqref{E-rel} y #eqref{utx} son un punto de partida muy com#'un para motivar la ecuaci#'on libre de Sch#"odinger. Recordemos que la relaci#'on no relativista entre la energ#'ia y el momento de una part#'icula libre de masa $m$ esta dada por, 
#begin{equation}
E= #frac{#P^{2}}{2m}
#end{equation}
a trav#'es de las ecuaciones #eqref{E-rel} vemos que esta corresponde a la relaci#'on de dispersi#'on:
#begin{equation}
w= #frac{#hbar #k^{2}}{2m}
#end{equation}
Con esta relaci#'on es f#'acil ver que la funci#'on #eqref{utx} debe de ser una soluci#'on de la ecuaci#'on diferencial parcial:
#begin{equation}#label{ecsl}
i #hbar #frac{#partial }{#partial t}#, #psi (#x , t)= - #frac{#hbar^{2}}{2m}#, #Delta #psi (#x , t)
#end{equation}
En donde, 
$$ #Delta = #frac{#partial^{2}}{#partial x_{1}^{2}} + #cdot #cdot #cdot + #frac{#partial^{2}}{#partial x_{n}^{2}}$$
con $n$ la dimensi#'on del {#bf espacio de configuraci#'on}. Usualmente $n= 1,2,3$. La ecuaci#'on #eqref{ecsl} es la ecuaci#'on de Sch#"odinger para part#'iculas libres, la cual fue introducida por el f#'isico Austriaco Erwin Sch#"odinger en 1926 [28]. Si una funci#'on $#psi$ satisface de la ecuaci#'on #eqref{ecsl} representando una situaci#'on f#'isica factible, nos referiremos a ella como una {#bf funci#'on de onda}. 

#paragraph{Cambio de escala:} 
Para facilitar la notaci#'on en las secciones siguientes y por motivos te#'oricos, definimos el siguiente cambio de variable para eliminar a las constantes f#'isicas $#hbar$ y $m$ de la ecuaci#'on libre de Sch#"odinger,
#begin{equation}
#x = #tilde{#x} #, #sqrt{#frac{#hbar}{m}}
#end{equation}
de esta forma definimos una nueva funci#'on $#tilde{#psi}$ que depende de $#tilde{#x}$, mediante:
#begin{equation}
#tilde{#psi}( #tilde{#x},t)= #psi ( #tilde{#x}#, #sqrt{#hbar / m},t )
#end{equation}
esta nueva funci#'on satisface la ecuaci#'on diferencial parcial:
#begin{equation}
i #, #frac{#partial}{#partial t} #, #tilde{#psi} (#tilde{#x},t)= -#frac{1}{2}#, #Delta_{#tilde{#x}} #, #tilde{#psi} (#tilde{#x},t)
#end{equation}
En donde $#Delta_{#tilde{#x}}$ es el laplaciano respecto a las nuevas coordenadas $#tilde{#x}$. Ya que pudimos haber utilizado las nuevas coordenadas desde un principio omitiremos la $#sim$ de ahora en adelante y consideraremos a la ecuaci#'on libre de Sch#"odinger como:
#begin{equation}#label{s2}
i#, #frac{#partial}{#partial t}#, #psi(#x,t)= -#frac{1}{2} #Delta #, #psi(#x,t)
#end{equation}
Por las ecuaciones #eqref{E-rel} es claro que un conjunto de soluciones de la ecuaci#'on  #eqref{ecsl} esta dado por las ondas planas:
#begin{equation}#label{oplan}
#exp #left( #frac{i}{#hbar}#, #P #cdot #x - #frac{i}{#hbar} #frac{#P^{2}}{2m}#, t #right) #,#,#,#,#,#, #forall #P #in #R^{n}
#end{equation}
Estas soluciones describen a un haz de part#'iculas con momento $#P$. Es f#'acil ver que sustituyendo a $#x$ por $#x #sqrt{#hbar / m}$ en #eqref{oplan}, obtenemos las ondas planas que son soluci#'on de la ecuaci#'on #eqref{s2}:
#begin{equation}
u_{#k}(#x , t)= #exp #left( i#k #cdot #x - i#,#frac{#k^{2}}{2}#, t #right)
#end{equation}
en donde $#k = #P / #sqrt{#hbar #, m}$, es el vector de onda que describe al momento de la onda plana en las nuevas coordenadas. El espacio vectorial $#R^{n}$ de todos los posibles vectores $#k$ es conocido como el {#bf espacio de momentos}.##

Com#'unmente las ondas planas son usadas para modelar haces de part#'iculas, ya que claramente una onda plana tiene el mismo modulo $|u(x,t)|$ en todo el espacio a todo tiempo, haciendo que una sola part#'icula bien localizada no pueda ser descrita de esta forma. Tambi#'en debemos de observar que algo tan complicado como un haz de part#'iculas interactuantes tampoco puede ser descrito por una sola onda plana (mas aun, ni siquiera por un n#'umero finito de estas). Por tanto, la descripci#'on de haces de part#'iculas en t#'erminos de ondas planas solo es aceptable para haces de baja intensidad, en los cuales las part#'iculas est#'an tan separadas unas de otras que se les puede considerar como objetos f#'isicos independientes sin interacci#'on con su alrededor.  

#section{Paquetes de ondas}
El comportamiento ondulatorio de las part#'iculas se manifiesta a trav#'es de los patrones de interferencia observados experimentalmente, como en el caso del experimento de Davison y Germer [23]. Tal como sucede en el caso de la interferencia #'optica, los patrones de interferencia producidos por part#'iculas, pueden ser descritos por la superposici#'on lineal de dos o mas ondas planas. Esto nos lleva a plantear el {#bf principio de superposici#'on}: Si las funciones de onda $#psi_{1}(#x,t)$ y $#psi_{2}(#x,t)$ son soluci#'on de la ecuaci#'on libre de Schr#"odinger, entonces la suma $#psi_{1}(#x,t) + #psi_{2}(#x,t)$ tambi#'en es una soluci#'on, es decir, describe una posible situaci#'on f#'isica. Esto surge del hecho de que la ecuaci#'on libre de Schr#"odinger,
$$ i #, #frac{#partial}{#partial t} #psi (#x , t)= -#frac{1}{2}#, #Delta #psi(#x,t) $$
es lineal en $#psi$, es decir, no contiene potencias de $#psi$ o productos de $#psi$ con sus derivadas, solo contiene derivadas simples $#frac{#partial}{#partial t}$ y $#Delta$. Si explotamos aun mas la linealidad de las derivadas de la ecuaci#'on libre de Schr#"odinger encontramos que una combinaci#'on lineal de la forma, $#psi = a#,#psi_{1} + b#, #psi_{2} $ ($a,b #in #C$) tambi#'en es una soluci#'on de la ecuaci#'on.##
Esta observaci#'on puede ser generalizada f#'acilmente para la superposici#'on de un n#'umero arbitrario de soluciones:

#paragraph{Linealidad de la ecuaci#'on de Schr#"odinger} Cualquier combinaci#'on lineal finita de soluciones $#psi_{i}$ de la ecuaci#'on libre de Schr#"odinger,
#begin{equation}
#psi=#sum_{i=1}^{n} a_{i}#, #psi_{i}
#end{equation}
con $a_{i} #in #C$, es tambi#'en una soluci#'on de la ecuaci#'on.

#rightline{$#dag$}
#vspace{5 mm}

En particular podemos obtener nuevas funciones de onda combinando ondas planas $u_{#k}$ con diferentes momentos. Por ejemplo, cualquier serie de Fourier dependiente del tiempo de la forma:
#begin{equation}#label{psi-sf}
#psi (#x,t)= #sum_{i} a_{i}#, u_{#k}(#x, t)
#end{equation}
es una soluci#'on de la ecuaci#'on libre de Schr#"odinger. El coeficiente complejo $a_{i}$ es la amplitud de la onda plana con vector de onda $#k_{i}$.

#subsubsection{Superposici#'on continua de ondas planas}
En muchas ocasiones una superposici#'on finita no es suficiente para representar a una funci#'on bien localizada en t#'erminos de ondas planas. Para resolver esto usamos una superposici#'on continua (no numerable) de ondas planas con diferente momento $#k$. Tal superposici#'on continua esta dada por la integral:
#begin{equation}#label{sup-cont}
#psi(#x , t)= #int_{#R^{n}} #phi(#k)#, u_{#k}(#x,t)#, d#k
#end{equation}
Aqu#'i, cada onda plana esta multiplicada por el factor escalar $#phi(#k)$. Como en el caso de una combinaci#'on lineal finita, los coeficientes $#phi (#k)$ deben de ser n#'umeros complejos. Por tanto la {#bf funci#'on de amplitud} $#phi$ es una funci#'on compleja de la variable real $#k$. ##

Ahora surge la siguiente pregunta: Supongamos que tenemos una funci#'on arbitraria $#psi$. ?`Como podemos obtener la funci#'on de amplitud $#phi$ tal que $#psi$ pueda ser escrita como una superposici#'on continua de ondas planas como en la ecuaci#'on #eqref{sup-cont}? Mas aun, ?`Como podemos conocer la distribuci#'on de momento de $#psi$? ##

Por simplicidad consideraremos primero la situaci#'on al tiempo $t=0$. Una onda plana con momento $#k$ al tiempo $t=0$ esta dada por la funci#'on, $u_{#k}(#x,0)= #exp (i #k #cdot #x)$. La superposici#'on continua de ondas planas que representa a $#psi$ al tiempo $t=0$, esta dada por la integral:
#begin{equation}#label{sup1}
#psi(#x ,t) #Big|_{t=0}= #psi_{0}(#x)= #int_{#R^{n}} e^{i #k #cdot #x}#, #phi(#k)#, d#k
#end{equation}
Por lo visto en el capitulo anterior, sabemos que cualquier funci#'on integrable $#psi_{0}$ con transformada de Fourier integrable $#hat{#psi}_{0}$, puede ser escrita como,
#begin{equation}#label{sup2}
#psi_{0}(#x)= #frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{i#k #cdot #x}#, #hat{#psi}_{0}(#k) #, d#k
#end{equation}
en donde,
#begin{equation}
#hat{#psi}_{0}(#x)= #frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{-i#k #cdot #x}#, #psi_{0}(#x) #, d#x
#end{equation}
Comparando las ecuaciones #eqref{sup1} y #eqref{sup2} podemos observar que la superposici#'on continua de ondas planas puede ser interpretada como la transformada de Fourier inversa de la funci#'on $#phi$. As#'i la funci#'on de amplitud $#phi$, es solo la transformada de Fourier de la funci#'on de onda. ##
Com#'unmente a las funciones de onda que pueden ser expresadas como series de Fourier #eqref{psi-sf} o como transformadas de Fourier inversas #eqref{sup2} se les conoce como {#bf paquetes de ondas} por el hecho de que son el resultado de la superposici#'on ondas planas y por razones que veremos mas adelante que tienen que ver con las mediciones f#'isicas que se pueden hacer sobre los sistemas cu#'anticos.

#paragraph{Distribuci#'on de momento de una funci#'on de onda:} Cualquier funci#'on de onda integrable $#psi_{0}$, puede ser vista al tiempo $t=0$, como una superposici#'on continua de ondas planas $#exp ( i #k #cdot #x )$, en donde cada onda plana con momento $#k$ tiene una amplitud:
#begin{equation}
#phi (#k)= #frac{1}{(2#pi)^{n/2}}#, #hat{#psi}_{0}(#k)
#end{equation}
con $#hat{#psi}_{0}$ la transformada de Fourier de $#psi$. as#'i, la distribuci#'on de momento de una funci#'on de onda esta dada por su transformada de Fourier.
#rightline{$#dag$}

#subsubsection{Evoluci#'on de las funciones de onda}
Habiendo descrito a la funci#'on $#psi_{0}$ como la transformada de Fourier inversa de una funci#'on $#hat{#psi}_{0}$, podemos ahora determinar f#'acilmente la soluci#'on general $#psi(#x , t)$ de la ecuaci#'on libre de Schr#"odinger, la cual es igual a $#psi_{0}$ cuando $t=0$. Esto lo podemos hacer ya que sabemos como es la evoluci#'on en el tiempo de las ondas planas $#exp( i #k #cdot #x )$, cuya superposici#'on forma a la funci#'on inicial $#psi_{0}$.##
Una onda plana esta dada para todo $#x #in #R^{n}$ y para todo tiempo $t$ por la funci#'on:
$$ u_{#k}(#x,t)= #exp ( i #k #cdot #x -i k^{2}t/2  )=#exp(i #k #cdot #x)#cdot #exp(-i k^{2}t/2)= #exp(-i k^{2}t/2)#cdot u_{#k}(#x, 0) $$
De aqu#'i observamos que la evoluci#'on de la onda plana $u_{#k}(#x, 0)$ esta dada por el factor $#exp(-i k^{2}t/2)$. Insertando este factor de evoluci#'on en la ecuaci#'on #eqref{sup1}, obtenemos:
#begin{equation}#label{gen-s}
#psi (#x , t)= #frac{1}{(2 #pi)^{n/2}} #int_{#R^{n}} e^{i#k #cdot #x - ik^{2}t/2}#, #hat{#psi}_{0}(#k)#, d#k
#end{equation}

Podemos verificar de manera formal que la ecuaci#'on #eqref{gen-s} es en efecto una soluci#'on de la ecuaci#'on libre de Schr#"odinger, insert#'andola en esta e intercambiando el orden de integraci#'on y derivaci#'on. Para hacer esto de forma rigurosa, la funci#'on $#hat{#psi}_{0}$ debe de ser suficientemente bien comportada. La condici#'on matem#'atica para permitir el intercambio de derivada e integral, es que las funciones $#hat{#psi}_{0}$ y $ k^{2}#, #hat{#psi}_{0} $, sean de cuadrado integrable integrable.##

Como podemos observar, la ecuaci#'on #eqref{gen-s} establece que la soluci#'on $#psi(#x,t)$ de la ecuaci#'on libre de Schr#"odinger al tiempo $t$, es la transformada de Fourier inversa de la funci#'on,
#begin{equation}
#hat{#psi}(#k , t)= #exp ( -ik^{2}t/2 )#, #hat{#psi}_{0}(#k)
#end{equation}

as#'i, hemos resuelto el siguiente problema con valor inicial: Dada una funci#'on apropiada $#psi_{0}$, determinar una soluci#'on $#psi (#x ,t)$ de la ecuaci#'on libre de Schr#"odinger, tal que, $ #psi(#x ,0)= #psi_{0}(#x) $.##

El procedimiento para resolver este problema se puede resumir de la siguiente manera:

#paragraph{Soluci#'on del problema con valor inicial para part#'iculas libres:} 
#begin{enumerate}
#item Determinar la transformada de Fourier $#hat{#psi}_{0}$ de la funci#'on inicial $#psi_{0}$.
#item Determinar la transformada de Fourier inversa de la funci#'on $ #exp(-ik^{2}t/2)#, #hat{#psi}_{0}(#k) $. 
#end{enumerate}
#rightline{$#dag$}
#vspace{5 mm}

El procedimiento anterior es valido para toda funci#'on inicial $#psi_{0}$ que este en el espacio de Hilbert $L^{2}(#R^{n})$, ya que como vimos en el capitulo anterior, la transformada de Fourier esta bien definida en este conjunto. As#'i, podemos definir una funci#'on que evolucione en el tiempo $#psi(#x,t)$, incluso si la funci#'on inicial $#psi_{0}$, no es diferenciable en el sentido cl#'asico (aunque si en el sentido de las distribuciones).##

Estudiemos ahora un ejemplo concreto para ilustrar la teor#'ia desarrollada hasta este punto.##

#subsubsection{Ejemplo: Funci#'on Gaussiana}
Calculemos la evoluci#'on temporal de la funci#'on inicial:
#begin{equation}
#psi_{0}(x)= e^{-#alpha x^{2}/2}#, e^{ipx}
#end{equation}
con $x#in #R$, $#alpha >0$ y $p #in #R$. 

#paragraph{Paso 1:} La transformada de Fourier de esta funci#'on esta dada por:
#begin{equation}
#hat{#psi}_{0}(k)= #frac{1}{#sqrt{#alpha}}#, #exp #left( - #frac{(k-p)^{2}}{2#alpha}  #right)
#end{equation}
Notemos que el momento $k$ de $#psi_{0}$ esta distribuido alrededor del momento promedio $p$ y $#hat{#psi}_{0}$ es una funci#'on real. 

#paragraph{Paso 2:} La soluci#'on al tiempo $t$ del problema con valor inicial esta dada por:
$$ #psi(x,t)= #frac{1}{#sqrt{#alpha}} #, #frac{1}{#sqrt{2 #pi}} #int_{#R} e^{ikx}#, e^{-ik^{2}t/2} #, e^{-(k-p)^{2}/2#alpha} #, dk $$
#begin{equation}
= #frac{1}{#sqrt{#alpha}}#, #exp(-p^{2}/2#alpha) #, #frac{1}{#sqrt{2#pi}} #int_{#R} #exp #left( - #frac{it + 1/#alpha}{2}#, q(k)^{2} + #frac{ (ix+p/#alpha)^{2} }{ 2(it+1/#alpha) }   #right) #, dk
#end{equation}
en donde:
#begin{equation}
q(k)= k - #frac{ix + p/#alpha}{it+1/#alpha}
#end{equation}
de donde obtenemos la siguiente soluci#'on:
#begin{equation}
#psi (x,t)= #frac{1}{#sqrt{#alpha}} #, #exp #left( -#frac{p^{2}}{2#alpha} + #frac{(ix+p/#alpha)^{2}}{2(it + 1/#alpha)} #right) #, #frac{1}{#sqrt{2 #alpha}} #int_{#R} #exp (-(it+1/#alpha)q(k)^{2} /2 )#, dk
#end{equation}
La integral de la ecuaci#'on anterior tiene la siguiente soluci#'on: 
#begin{equation}
#int_{#R} #exp (-(it+1/#alpha)q(k)^{2} /2 )#, dk = #int_{#R} e^{ -(it+1/#alpha)k^{2}/2} #, dk= #frac{ #sqrt{2 #pi} }{ #sqrt{it + 1/#alpha} }
#end{equation}
por tanto la soluci#'on la funci#'on de onda para todo $#x$ y $t$ esta dada por:
#begin{equation}#label{sol-g}
#psi (x,t)= #frac{1}{#sqrt{1+i #alpha t}}#, #exp #left( - #frac{ #alpha x^{2}-2ixp + ip^{2}t }{2(1+i#alpha t)} #right)
#end{equation}

Un calculo elemental muestra que la parte real de la funci#'on exponencial en la ecuaci#'on #eqref{sol-g} es:
#begin{equation}
- #frac{#alpha}{ 2 (1 + #alpha^{2} t^{2}) } #, (x-pt)^{2}
#end{equation}
as#'i, el modulo de la funci#'on de onda Gaussiana al tiempo $t$ es de nuevo una funci#'on Gaussiana,
#begin{equation}
|#psi (x,t)|= (1+#alpha^{2} t^{2})^{-1/4} #, #exp #left( -#frac{#alpha(t)}{2}#, (x-pt)^{2} #right) 
#end{equation}
en donde,
#begin{equation}
#alpha (t)= #frac{#alpha}{ 1 + #alpha^{2}#, t^{2} }
#end{equation}
La funci#'on Gaussiana $|#psi (x,t)|$ esta centrada alrededor de $x(t)= pt$, que no es mas que la posici#'on al tiempo $t$ de una part#'icula libre cl#'asica de masa $m=1$ (recordemos que estamos en un sistema de unidades en el cual $#hbar / m=1$) con momento $p$ y condici#'on inicial $x(0)=0$. El ancho de la funci#'on Gaussiana esta dado por $1 / #sqrt{#alpha (t)}$ y aumenta mientras $t$ crece; llamaremos a esto {#bf propagaci#'on del paquete de ondas}. Al mismo tiempo el valor m#'aximo de la funci#'on Gaussiana disminuye como $1 / #sqrt{t}$.      

#subsubsection{Conservaci#'on de la norma}

La propagaci#'on del paquete de ondas $#psi$ y la disminuci#'on simultanea de su modulo, nos lleva a la conservaci#'on de la norma:
#begin{equation}#label{nor}
#| #psi(t) #|^{2}= #int_{#R^{n}} |#psi(#x,t)|^{2} #, d#x
#end{equation}

#paragraph{Invariancia temporal de la norma:} Para cualquier soluci#'on $#psi (#x,t)$ de cuadrado integrable (respecto a $#x$) de la ecuaci#'on libre de Schr#"odinger, la norma #eqref{nor} es independiente del tiempo, es decir:
#begin{equation}
#| #psi(t) #|^{2}= #| #psi(0) #|^{2}
#end{equation}
para todo tiempo $t$.
#rightline{$#dag$}
#vspace{5 mm}

Para probar esto, tomemos la funci#'on $#hat{#psi}(#k,t)$ en el espacio de momentos, la cual se puede re-escribir como:
$$ #hat{#psi}(#k,t) = #exp ( -ik^{2}t/2 )#, #hat{#psi}_{0}(#k) $$
pero ya que el modulo del factor de evoluci#'on $#exp ( -ik^{2}t/2 )$ es uno y por el teorema de Fourier-Plancherel #ref{FPt}, tenemos que: 

$$ #int_{#R^{n}} |#psi(#x,t)|^{2} #, d#x = #int_{#R^{n}} |#hat{#psi}(#k,t)|^{2} #, d#k = #int_{#R^{n}} |#hat{#psi}_{0}(#k)|^{2} #, d#k= #int_{#R^{n}} |#psi_{0}(#x)|^{2} #, d#x$$
#qed

#section{Interpretaci#'on f#'isica de la funci#'on de onda}
?` Que es lo que la funci#'on de onda describe? ?`Cual es su significado f#'isico? Hasta ahora nuestra discusi#'on solo a relacionado a la funci#'on de onda $#psi$, con la distribuci#'on de la posici#'on en una regi#'on y a su transformada de Fourier $#hat{#psi}$, con la contribuci#'on de momentos.##

Como hemos visto, los paquetes de ondas tienden a propagarse en regiones cada vez mas grandes del espacio conforme evolucionan en el tiempo. Pero no importa que tan grande sea la regi#'on del espacio ocupada por la funci#'on de onda, nunca se ha observado experimentalmente que la masa o la carga de una sola part#'icula se disperse sobre dicha regi#'on, al contrario, estas propiedades siempre se encuentran en part#'iculas puntuales bien localizadas.## 
Consideremos el siguiente ejemplo. Supongamos que al tiempo $t=0$ sabemos que hay una part#'icula (libre) de masa $m$ en el origen de un sistema de coordenadas, con una funci#'on de onda asociada $#psi$, la cual es un paquete de ondas Gaussiano, como el del ejemplo del capitulo anterior. Supongamos tambi#'en que hay sensores detectores de part#'iculas en todo el espacio. Al transcurrir el tiempo, como hemos visto anteriormente, el paquete de ondas se va a propagar por todo el espacio, sin embargo, la experiencia nos dice que la part#'icula asociada a este, solo va a ser detectada en un y solo un detector a la vez, mas aun, la part#'icula va a llegar a cada uno de los detectores como un $todo$, es decir, con toda su masa y carga inicial. Entonces, la funci#'on de onda no esta relacionada con la propagaci#'on de las propiedades f#'isicas de la part#'icula asociada.##
Si repetimos un experimento como el anterior muchas veces, vamos a observar que en cada nuevo experimento la part#'icula es encontrada en diferentes regiones del espacio: con mayor frecuencia en donde la funci#'on de onda tiene un modulo mayor y con menor frecuencia en donde tiene un modulo menor. La conexi#'on entre la part#'icula y su funci#'on de onda es meramente estad#'istica, ya que esta ultima muestra la probabilidad de encontrar a la part#'icula en cierta regi#'on del espacio al realizar un experimento. Lo anterior fue sugerido por Max Born en 1926 [27] y se conoce como la {#bf interpretaci#'on estad#'istica de la funci#'on de onda}, la cual establecemos a continuaci#'on:

#paragraph{Interpretaci#'on estad#'istica de la funci#'on de onda:} 
Supongamos que una part#'icula puede ser descrita por la funci#'on de onda $#psi(#x)$ que satisface la condici#'on de normalizaci#'on:
#begin{equation}#label{norm-c}
#int_{#R^{n}} |#psi(#x,t)|^{2}#, d#x=1
#end{equation}
Entonces la expresi#'on:
#begin{equation}#label{probb}
p(B)= #int_{B} |#psi (#x,t)|^{2}#, d#x 
#end{equation}
se refiere a la probabilidad de encontrar a la part#'icula en la regi#'on $B$ del espacio de configuraci#'on $#R^{n}$.## 
Similarmente:
#begin{equation}#label{probm}
#int_{G}  |#hat{#psi}(#x,t)|^{2}#, d#k 
#end{equation}
se refiere a la probabilidad de que el momento de la part#'icula se encuentre en el sub conjunto $G$ del espacio de momentos $#R^{n}$.

#rightline{$#dag$}
#vspace{5 mm}

Debemos notar que la interpretaci#'on estad#'istica nos da probabilidades sobre $regiones$ de $#R^{n}$ y no sobre puntos individuales. Seg#'un esta interpretaci#'on no tiene sentido el preguntar:## 

?`Cual es la probabilidad de encontrar a la part#'icula asociada a $#psi$, al tiempo $t$ en el $punto$ $#x_{0}$ ?##

Sin embargo si podemos preguntar: ##

?`Cual es la probabilidad de encontrar a la part#'icula asociada a $#psi$, al tiempo $t$ en una $vecindad$ $V$ de $#x_{0}$ ?##

Para cualquier  vecindad de $V$ de $#x_{0}$ (por peque#~na que sea) la respuesta esta dada por la ecuaci#'on #eqref{probb}. El integrando $|#psi(#x,t)|^{2}$ da la {#bf densidad de probabilidad} en el punto $#x$ al tiempo $t$. Para $#hat{#psi}$ sucede algo completamente an#'alogo con el momento $#k$.##

La funci#'on de onda contiene informaci#'on acerca de la posici#'on una part#'icula y a trav#'es de la transformada de Fourier, del momento que esta pueda tener al mismo tiempo. ##
Para aplicar la interpretaci#'on estad#'istica a una funci#'on de onda de cuadrado integrable, esta necesita ser normalizada primero. El procedimiento de normalizaci#'on es completamente inmediato para funciones $#psi #in L^{2}(#R^{2})$ tales que $#|#psi(#x)#| > 0$ ya que solo hay que remplazar a $#psi$ por $#psi /#|#psi #| $. ##

La condici#'on de normalizaci#'on #eqref{norm-c} nos dice que la probabilidad que tenemos de encontrar a la part#'icula #emph{ en alg#'un lugar } del espacio, es uno (es decir la part#'icula no desaparece nunca, pero puede estar en el lugar que menos esperamos). Esta probabilidad es independiente del tiempo si la funci#'on de onda depende temporalmente de acuerdo con la ecuaci#'on libre de Schr#"odinger. El teorema de Fourier-Plancherel implica que la misma condici#'on de normalizaci#'on se cumple para la distribuci#'on de momento. ##

?`Como se mueve una part#'icula de una regi#'on del espacio a otra? Cu#'anticamente esto es descrito como el cambio en las probabilidades de posici#'on asociadas con estas regiones. El mecanismo detr#'as de este cambio es el $movimiento$ del paquete de ondas de acuerdo con la ecuaci#'on de Schr#"odinger. Sin embargo la interpretaci#'on estad#'istica no es capaz de describir como se mueve en realidad una part#'icula a trav#'es del espacio, ya que solo nos da la probabilidad de encontrarla en una regi#'on a un cierto tiempo. Hemos encontrado entonces que en la mec#'anica cu#'antica no existe una noci#'on de trayectoria, tal como en la f#'isica cl#'asica.  

#section{Valores esperados}
Supongamos que $#psi(#x)$ es una funci#'on de onda normalizada. Ya que la funci#'on $|#hat{#psi}(#k , t)|^{2}$ describe la densidad de probabilidad de la posici#'on, podemos calcular el {#bf valor esperado} (o valor promedio) del resultado de muchas mediciones de la posici#'on. Este valor esta dado por:
#begin{equation}#label{pos-ve}
#la #x #ra_{#psi}= #int_{#R^{n}} #x #, |#psi(#x , t)|^{2} #, d#x
#end{equation}

De forma similar, el valor esperado del momento $#k$ de un estado $#psi$ se obtiene a trav#'es de la interpretaci#'on de la transformada de Fourier $#hat{#psi}$ como la densidad de probabilidad del momento:
#begin{equation}#label{mon-ve}
#la #k #ra_{#hat{#psi}}= #int_{#R^{n}} #k #, | #hat{#psi}(#k,t) |^{2}#, d#k
#end{equation}

El resultado de muchas mediciones de la posici#'on y del momento va a estar distribuido alrededor de sus respectivos valores esperados. Como es bien sabido, el ancho de las distribuciones de los valores medidos, son las incertidumbres $#Delta x$ y $#Delta k$, las cuales satisfacen el {#bf principio de incertidumbre de Heisenberg} [25]:
#begin{equation}#label{piH}
#Delta x #, #Delta k #geq #frac{1}{2}
#end{equation}
El principio de incertidumbre de Heisenberg establece que una distribuci#'on de la posici#'on muy localizada (es decir una distribuci#'on no muy ancha) corresponde a una distribuci#'on de momento muy amplia y viceversa. Este principio tambi#'en implica que no existen estados para los cuales, tanto la posici#'on como el momento, tengan distribuciones bien localizadas simult#'aneamente. ##

En principio no hay un limite para la precisi#'on de la posici#'on o del momento que pueda tener una funci#'on de onda. Sin embargo, el principio de incertidumbre implica que no puede haber una funci#'on de onda cuya distribuci#'on de momento solo conste de un valor. El momento de las ondas planas solo tiene un valor, pero estas no pueden ser normalizadas, por lo que no tienen una interpretaci#'on probabil#'istica real y por si solas no son admisibles como funciones de onda en la mec#'anica cu#'antica. Por tanto, no es posible preparar a una part#'icula en un estado cu#'antico cuyo momento tenga un solo valor. Similarmente no hay una funci#'on de onda cuya posici#'on tenga un solo valor. Tal funci#'on de onda se comportar#'ia como una Delta de Dirac, pero esta funci#'on resulta no ser de cuadrado integrable [9] y por tanto no puede ser normalizada.


#chapter{Estados y observables}

En este capitulo describiremos la estructura matem#'atica b#'asica de cualquier teor#'ia cu#'antica. El primer paso en esta formulaci#'on matem#'atica es la asociaci#'on de un espacio de Hilbert adecuado a un sistema f#'isico, de tal forma que los vectores en este espacio describan todos los posibles estados del sistema. Veremos tambi#'en que ciertos operadores lineales corresponden a la medici#'on de cantidades f#'isicas.

#section{Vectores de estado}  
En la mec#'anica cl#'asica, el $estado$ de una part#'icula al tiempo $t$ es descrito por su posici#'on $x(t)$ y su momento $p(t)$, es decir, por un punto en el espacio fase. Una part#'icula cu#'antica ocupa una cierta regi#'on en el espacio fase cuyo tama#~no m#'inimo esta dado por el principio de incertidumbre de Heisenberg, lo cual hace imposible definir en que estado (en el sentido cl#'asico) se encuentra la part#'icula, ya que no se puede saber con exactitud cual es su posici#'on y su momento a un cierto tiempo. Entonces ?`Que es un $estado$ para la mec#'anica cu#'antica? Para ver esto primero debemos de examinar a que nos referimos por $estado$ en un sistema f#'isico. ##

De forma general, el estado de un sistema f#'isico debe de ser una colecci#'on de propiedades que sean capaces de dar una descripci#'on completa del sistema. El conjunto de informaci#'on que forma una descripci#'on completa depende de en que aspectos del sistema estamos interesados, aunque en cualquier caso, esta informaci#'on debe de ser completa en el sentido de que se pueda determinar el estado futuro del sistema a partir de la descripci#'on del estado al tiempo inicial. El desarrollo en el tiempo de un sistema en un cierto estado, usualmente esta dado a trav#'es de una ley din#'amica, tal como una ecuaci#'on de evoluci#'on o de movimiento. En la mec#'anica cl#'asica esta ley din#'amica esta dada por las ecuaciones de Hamilton.##

En la mec#'anica cu#'antica la funci#'on de onda contiene toda la informaci#'on acerca de la posici#'on y la distribuci#'on de momento de una part#'icula. La informaci#'on provista por la funci#'on de onda tambi#'en es completa, en el sentido de que si sabemos como es la funci#'on al tiempo $t=0$ podemos determinar, por la ecuaci#'on de Schr#"odinger, como va a ser esta a cualquier otro tiempo. Por tanto, el estado cu#'antico de una part#'icula puede ser asociado con su funci#'on de onda, con la ecuaci#'on de Schr#"odinger como su ecuaci#'on de evoluci#'on. ##

Hay cantidades f#'isicas tales como la masa y la carga el#'ectrica las cuales afectan el comportamiento de la part#'icula pero usualmente no son consideradas parte del estado del sistema ya que estas cantidades pueden permanecer constantes en una amplia gama de experimentos din#'amicos y se les considera mas como caracter#'isticas del sistema en si y no del estado de este. En el formalismo siguiente, estas cantidades aparecen como par#'ametros cuyo valor num#'erico se mantiene constante.##
Puede suceder que la ecuaci#'on de evoluci#'on tenga que ser modificada para describir el desarrollo temporal de estados en situaciones extremas, por ejemplo, para part#'iculas con velocidades muy cercanas a la de la luz, ya que en este limite la ecuaci#'on de Schr#"odinger se vuelve imprecisa y tiene que ser remplazada por alguna otra ecuaci#'on de evoluci#'on que sea invariante de forma relativista. La teor#'ia que desarrollaremos a continuaci#'on es valida en el limite no relativista, para part#'iculas con una velocidad mucho menor que la de la luz.##

La correspondencia entre estados cu#'anticos y funciones de onda no es uno a uno. Por ejemplo, si multiplicamos a una funci#'on de onda por un n#'umero real, esta seguir#'ia describiendo el mismo estado cu#'antico, ya que el n#'umero se anular#'ia cuando normaliz#'aramos a la funci#'on de onda para poder aplicar la interpretaci#'on probabil#'istica. Mas aun, las ecuaciones #eqref{probb} y #eqref{probm} que describen el contenido f#'isico de la funci#'on de onda, no son afectadas si multiplicamos a la funci#'on por un {#bf factor de fase} es decir, por un n#'umero complejo con modulo uno. Por tanto podemos concluir que despu#'es de la normalizaci#'on una funci#'on de onda $#psi$ y su m#'ultiplo escalar $c #psi$ (en donde $c #in #C$, $ c #neq 0 $) solo difieren por un factor de fase, llev#'andonos a las mismas predicciones f#'isicas, como por ejemplo, acerca de la probabilidad de encontrar a una part#'icula en alg#'un lugar del espacio de posiciones o momentos. En este sentido $#psi$ y $c #psi$ describen ambas el mismo estado.##

En principio, cualquier funci#'on no nula de cuadrado integrable, puede ser usada para definir un estado cu#'antico. Como hemos visto en las secciones anteriores, el conjunto de todas estas funciones forma al espacio de Hilbert $L^{2}(#R^{n})$, el cual esta asociado claramente con una part#'icula que se mueve en un espacio de configuraci#'on de $n$ dimensiones. ##
Esta es una de las suposiciones b#'asicas del formalismo de la mec#'anica cu#'antica, que los estados de todo sistema f#'isico est#'an dados por vectores de un espacio de Hilbert apropiado y que dos vectores describen el mismo estado, si uno es m#'ultiplo escalar del otro. Establecemos estas suposiciones a continuaci#'on.##

#paragraph{Estados de un sistema f#'isico} Los estados de un sistema cu#'antico pueden ser descritos por vectores de un espacio de Hilbert apropiado. Dos vectores $#psi$ y $#phi$ describen el mismo estado, si y solo si, $#phi = c#, #psi$. Por tanto, los estados f#'isicos corresponden a los subespacios de una dimensi#'on:
#begin{equation}#label{est-1d}
[#psi]= #{ c #, #psi #,|#, c #in #C #} #,#,#,#,#,#,#, #psi #neq 0
#end{equation} 
del espacio de Hilbert elegido. Cualquier elemento no nulo $#psi$ de este subespacio, puede ser usado para representar al estado $[#psi]$ ya que las predicciones f#'isicas no dependen de la elecci#'on de la funci#'on.  
#rightline{$#dag$}
#vspace{3 mm} 

La mejor manera de representar al estado de un sistema, es mediante una funci#'on de onda normalizada $#psi$ ( $#| #psi #|=1$ ) ya que esto permite una aplicaci#'on directa de las reglas de interpretaci#'on estad#'istica. Como hemos visto, la evoluci#'on temporal que da la ecuaci#'on de Schr#"odinger implica la invariancia temporal de la norma, $#|#psi(t)#|= #|#psi_{0}#|$, lo que muestra que si la funci#'on que describe al estado inicial del sistema esta normalizada, entonces las funciones que lo describen a tiempos distintos tambi#'en lo est#'an.  

#subsubsection{Principio de superposici#'on}
Dadas dos funciones de onda de un sistema $#psi_{1}$ y $#psi_{2}$, cualquier combinaci#'on lineal $#phi = #psi_{1} + #psi_{2}$ define un nuevo posible estado del sistema. Este principio de superposici#'on solo expresa la linealidad del espacio de Hilbert en el cual estemos trabajando. Sin embargo agregamos las siguientes observaciones:
#begin{enumerate}
#item Aunque las funciones de onda $#psi$ y $c #, #psi$ describen el mismo estado, esto no es cierto para las funciones $ #psi_{1}+#psi_{2} $ y $c_{1} #, #psi_{1}+c_{2}#, #psi_{2}$, al menos que $c_{1}= c_{2}$,
$$ [#psi_{1}+#psi_{2}] #neq [c_{1} #, #psi_{1}+c_{2}#, #psi_{2}] $$

#item Para dos funciones de onda $#psi_{1}$ y $#psi_{2}$ la densidad de probabilidad de la suma $#psi_{1}+#psi_{2}$ es en general diferente de la suma de las densidades individuales,
$$ |#psi_{1}+#psi_{2}|^{2} #neq |#psi_{1}|^{2} + |#psi_{2}|^{2} $$
#end{enumerate}

#section{Observables y operadores lineales}
Los observables son cantidades f#'isicas como la posici#'on, el momento y la energ#'ia, las cuales se pueden medir experimentalmente para saber en que estado se encuentra un sistema. En la mec#'anica cu#'antica los observables son descritos por operadores lineales en el espacio de Hilbert del sistema f#'isico en el que estamos interesados. En esta parte especificaremos como extraer informaci#'on experimentalmente verificable de estos operadores lineales. Comenzaremos estudiando algunos ejemplos concretos de observables, como la posici#'on y el momento.

#subsubsection{El operador de posici#'on}
Una cantidad que esta relacionada con la posici#'on y que (al menos en principio) puede ser determinada experimentalmente es el valor esperado de la posici#'on. Para una part#'icula en un espacio de una dimensi#'on esta cantidad puede ser escrita como:
#begin{equation}#label{pos-exp1}
#la x #ra_{#psi}= #int_{-#infty}^{#infty} x#, |#psi(x)|^{2} #, dx = (#psi , x#, #psi)
#end{equation}
en donde $(#cdot , #cdot)$ denota al producto interno del espacio de Hilbert $L^{2}(#R)$. Aqu#'i y en lo siguiente, supondremos que la funci#'on $#psi$ esta normalizada ($#| #psi #|=1$). Desde el punto de vista matem#'atico, la expresi#'on del valor esperado de la posici#'on no es mas que el producto escalar entre $#psi$ y la funci#'on $#xi$, dada por:

#begin{equation}
#xi (x)= x #, #psi (x)
#end{equation}

El mapeo entre $#psi$ y $#xi$ es un operador lineal del espacio de Hilbert $L^{2}(#R)$. Claramente, solo necesitamos a este operador para obtener toda la informaci#'on necesaria acerca de la posici#'on de una part#'icula en un cierto estado cu#'antico, por tanto escogemos a este operador lineal como el representante del observable de la posici#'on en el formalismo de la mec#'anica cu#'antica.

#paragraph{Operador de posici#'on:} 
El observable de la posici#'on para una part#'icula en una dimensi#'on, es representado por el {#bf operador de posici#'on $x$}, el cual se define como el operador lineal que multiplica a la funci#'on de onda $#psi$ por la variable $x$.
 
#rightline{$#dag$}
#vspace{3 mm} 

Seguiremos la usual pero peligrosa convenci#'on de denotar tanto al operador como a la variable de posici#'on por la letra $x$.

#paragraph{Dominio de definici#'on:} La definici#'on de un operador lineal no puede estar completa sin que especifiquemos su dominio. El dominio de definici#'on del operador de multiplicaci#'on $x$, es el subespacio lineal de todas las funciones $#psi #in L^{2}(#R)$ para las cuales:
#begin{equation}
#int_{-#infty}^{#infty} x^{2}#, |#psi(x)|^{2}#, dx < #infty
#end{equation}
El operador $x$ solo puede ser aplicado a las funciones de este subespacio, ya que de otra manera la funci#'on $x #psi$, no estar#'ia en el espacio de Hilbert $L^{2}(#R)$ haciendo que la expresi#'on #eqref{pos-exp1} no tuviera sentido.##

?`Como es una funci#'on de onda que no esta en el dominio del operador de posici#'on $x$? Si la funci#'on $x #psi(x)$ no es de cuadrado integrable, podemos decir que $#psi$ tiene a cero muy $despacio$ cuando $|x| #rar #infty$. Puede suceder tambi#'en que la posici#'on no tenga un valor de expectaci#'on finito, por ejemplo cuando la funci#'on $ #sqrt{x} #psi (x) $, no es de cuadrado integrable. Sin embargo, mientras $#psi$ pertenezca al espacio de Hilbert de las funciones de cuadrado integrable, podemos mantener la interpretaci#'on de $|#psi (x)|^{2}$ como la densidad de probabilidad de la posici#'on. 

#paragraph{Operador de posici#'on en $n$ dimensiones:} Para part#'iculas en un espacio de configuraci#'on de $n$ dimensiones, los observables de la posici#'on son representados por una $n$-tupla de operadores lineales representados por el vector:
#begin{equation}
#x = ( x_{1},...,x_{n} )
#end{equation} 

as#'i cuando hablamos por ejemplo del operador de posici#'on para una part#'icula en $#R^{3}$, nos estamos refiriendo en realidad al vector formado por los tres operadores de multiplicaci#'on por $x_{1}$, $x_{2}$ y $x_{3}$. 

#subsubsection{El operador de momento}
Para una part#'icula en una dimensi#'on, el valor esperado del momento esta dado por:
#begin{equation}
#la k #ra_{#psi} = #int_{-#infty}^{#infty} k#, |#hat{#psi}(k)|^{2}#, dk 
#end{equation}
De nuevo, es posible representar a esta cantidad como un producto escalar de $L^{2}(#R)$:
#begin{equation}#label{mom-k}
#la k #ra_{#psi} = (#hat{#psi} , k #hat{#psi})
#end{equation}
con el operador lineal $k$. Recordamos la siguiente propiedad de la transformada de Fourier $#F$:
$$ #F (-i #psi' )= k #, #F (#psi) = k #hat{#psi} $$
en donde $#psi' = #frac{d}{dx} #psi$. Usando esta propiedad y el teorema de Fourier-Plancherel, obtenemos la siguiente expresi#'on para el valor esperado del momento:
#begin{equation}
#la k #ra_{#psi}= #int_{-#infty}^{#infty} #ol{#psi(x)} #, (-i)#psi'(x) #, dx = (#psi , -i #psi')
#end{equation}
Entonces tiene sentido definir al {#bf operador de momento}, en el espacio de configuraci#'on $#x$, como el operador lineal $p$ dado por: 
#begin{equation}
p #psi = -i #, #frac{d}{dx} #psi
#end{equation}

Mientras que en el espacio de momentos, este operador act#'ua como una multiplicaci#'on por la variable $k$.
#begin{equation}
 p#hat{#psi}= k #, #hat{#psi} 
#end{equation}

El dominio del operador de momento es el conjunto de las funciones de cuadrado integrable $#psi$, para las cuales $k #hat{#psi}$ tambi#'en es de cuadrado integrable.##

Para part#'iculas en un espacio de configuraci#'on de $n$ dimensiones, formamos al vector $#P$ con las componentes del momento en las direcciones coordenadas y escribimos al operador de momento como:
#begin{equation}
#P = -i #, ( #partial_{1},...,#partial_{n} )
#end{equation}
en donde, $#partial_{k} = #frac{#partial}{#partial_{k}} $.

#subsubsection{Energ#'ia cin#'etica}
El operador de Laplace $- #Delta$ que aparece en la ecuaci#'on de Schr#"odinger se puede escribir como:
#begin{equation}
- #Delta = #P #cdot #P = - #sum_{i=1}^{n} #partial_{i}^{2}
#end{equation}
Ya que,
$$ #frac{1}{2}#, #P #cdot #P = #frac{|#k|^{2}}{2} $$
es la expresi#'on cl#'asica para la energ#'ia cin#'etica en t#'erminos del momento de una part#'icula (de masa $m=1$), asociamos al operador lineal:
#begin{equation}#label{H0}
H_{0}= - #frac{1}{2} #Delta
#end{equation}
con el observable f#'isico de la energ#'ia cin#'etica.

#paragraph{El dominio de $H_{0}$:} El operador $H_{0}$ tiene que estar definido en el espacio de Hilbert $L^{2}(#R^{n})$. Podr#'iamos definir al dominio de $H_{0}$
como el conjunto de funciones doblemente diferenciables, para las cuales $#Delta #psi$ es de cuadrado integrable. Sin embargo, es mas apropiado entender a la diferenciabilidad en el sentido de la transformada de Fourier (ec. #eqref{f-der}). Diremos que una funci#'on $#psi$ esta en el dominio de $H_{0}$, $D(H_{0})$, si y solo si, la funci#'on $k^{2} #, #hat{#psi}(#k)$ es de cuadrado integrable.##
Para $ #psi #in D(H_{0}) $, la acci#'on de $H_{0}$ esta dada por:
#begin{equation}
H_{0} #psi = #F^{-1} #frac{k^{2}}{2} #F #psi
#end{equation}

#section{Valor esperado de un observable}
Como lo hemos mencionado anteriormente, en la mec#'anica cu#'antica, cualquier observable de un sistema f#'isico es representado por un operador apropiado en el espacio de Hilbert del sistema. El valor esperado de cualquier observable se define en completa analog#'ia a los valores esperados de la posici#'on y el momento, tal como lo establecemos a continuaci#'on.

#paragraph{Valor esperado:} 
Para cualquier operador lineal $A$, que representa a un observable f#'isico, el {#bf valor esperado} de $A$ en el estado normalizado $#psi$, esta dado por:
#begin{equation}
#la A #ra_{#psi}= ( #psi , A #psi )
#end{equation}

En donde $(#cdot , #cdot)$ es el producto interno del espacio de Hilbert del sistema. Esta cantidad es interpretada como el valor promedio de muchas mediciones realizadas sobre copias id#'enticas de un sistema f#'isico.

#rightline{$#dag$}
#vspace{3 mm} 

Debemos hacer notar que solo los operadores para los cuales $#la A #ra_{#psi}$ sea un n#'umero real, pueden representar a un observable, ya que las mediciones f#'isicas son siempre como resultado un n#'umero real. Tales operadores son llamados {#bf operadores sim#'etricos} los cuales definimos a continuaci#'on.

#begin{definition}#label{op-s}
Un operador con dominio denso es llamado {#bf sim#'etrico} o {#bf Hermitiano}, si todos sus valores esperados son n#'umeros reales:##

$A$ es sim#'etrico, si y solo si, $(#psi , A #psi) #in #R$, para todo $#psi #in D(A)$. 
#end{definition} 

Hay mas restricciones sobre los operadores que pueden ser asociados con un observable f#'isico. Para que un operador sea un buen candidato para representar a un observable, este debe de ser {#bf auto adjunto}. Como veremos en las pr#'oximas secciones, los operadores autoadjuntos son un sub conjunto  de los operadores sim#'etricos. Ahora daremos la noci#'on que existe en la mec#'anica cu#'antica de la incertidumbre en la medici#'on de un observable f#'isico.

#paragraph{Incertidumbre}
Para un observable $A$, la cantidad:
#begin{equation}
#Delta_{#psi} A = #| ( A-#la A #ra_{#psi} ) #psi #|= #sqrt{ #la  (A-#la A #ra_{#psi})^{2}  #ra_{#psi} }
#end{equation}
es llamada la {#bf incertidumbre} de $A$ en el estado normalizado $#psi$. La incertidumbre describe la dispersi#'on de los valores realmente medidos del observable $A$ alrededor del valor medio $#la A #ra_{#psi}$.

#rightline{$#dag$}
#vspace{5 mm}    

Debemos hacer notar que en general, el valor esperado y la incertidumbre de un observable $A$, no est#'an definidos para todo $#psi$ del espacio de Hilbert del sistema estudiado. Estas cantidades solo est#'an definidas para las funciones de onda $#psi$ que se encuentren dentro del dominio del operador $A$.

#section{Otros observables y la regla de substituci#'on}
Hasta ahora, solo hemos introducido a los operadores lineales para los observables de posici#'on, momento y energ#'ia cin#'etica. Sin embargo, desear#'iamos encontrar a los operadores que corresponden a otros observables f#'isicos de inter#'es como el momento angular, la energ#'ia potencial etc. En muchos casos, el siguiente procedimiento ha demostrado tener #'exito en encontrar a los operadores lineales apropiados para muchos observables de inter#'es.##

Cl#'asicamente un observable es una funci#'on de la posici#'on y del momento, es decir, una funci#'on del espacio fase. Frecuentemente (pero no siempre) es posible obtener un operador cu#'antico apropiado para un observable, simplemente haciendo la sustituci#'on en la expresi#'on cl#'asica de cada componente del momento $p$, por su correspondiente operador diferencial:
#begin{equation}#label{sus1}
p_{k} #longrightarrow -i#, #frac{#partial}{#partial x_{k}}, #,#,#,#,#,#,#,#, k=1,...,n
#end{equation}
y cada componente de $#x$ por su correspondiente operador de multiplicaci#'on:
#begin{equation}#label{sus2}
x_{k} #longrightarrow #hbox{ multiplicaci#'on por } x_{k}, #,#,#,#,#,#,#,#, k=1,...,n
#end{equation}
en donde $n$ es la dimensi#'on del espacio de configuraci#'on. A este procedimiento se le conoce como la {#bf regla de substituci#'on}.## 
Utilizando unidades en las que $#hbar$ no es igual a $1$, la regla de substituci#'on para el momento esta dada por:
#begin{equation}#label{sus3}
p_{k} #longrightarrow -i #hbar #, #frac{#partial}{#partial x_{k}} #,#,#,#,#, #hbox{o} #,#,#,#,#, #P #longrightarrow -i #hbar #nabla
#end{equation}

#subsubsection{Funciones de $#x$}
Si $V(#x)$ es una funci#'on real de la posici#'on $#x$, entonces su observable cu#'antico correspondiente es el operador de multiplicaci#'on por $V(#x)$:
#begin{equation}
V : #psi(#x) #longrightarrow V(#x)#, #psi(#x)
#end{equation}
Com#'unmente el operador $V$ representa a la energ#'ia potencial de una part#'icula. Recordando la definici#'on del Hamiltoniano de un sistema cl#'asico $H=K+V$, introducimos al {#bf operador cu#'antico Hamiltoniano}:
#begin{equation}
H=- #frac{1}{2} #Delta + V(#x)
#end{equation}
Mas adelante veremos la importancia que tiene este operador en el desarrollo de la teor#'ia cu#'antica. Notemos que el hamiltoniano de una part#'icula libre esta dado por la ecuaci#'on #eqref{H0} ($H_{0}$).

#paragraph{El dominio de $V$:} 
Si $V(#x)$ es una funci#'on no acotada (por ejemplo $V(#x)= |#x|^{2}$), el dominio del operador de multiplicaci#'on $V$, consiste solo de las funciones de cuadrado integrable para las cuales:
#begin{equation}
#int_{#R^{n}} |V(#x) #, #psi (#x)|^{2} #, d#x < #infty
#end{equation}
 
Si $V(#x)$ es una funci#'on acotada (es decir, existe una constante $M$ tal que $V(#x)<M$, $#forall #x$), entonces el operador $V$ esta definido para toda funci#'on $#psi$ de cuadrado integrable, ya que en este caso:
#begin{equation}
#int_{#R^{n}} |V(#x)#, #psi(#x)|^{2} #, d#x #leq M^{2}#, #| #psi #|^{2} < #infty 
#end{equation}
$ #psi #in L^{2}(#R^{n})$. as#'i tambi#'en, el valor esperado de un operador como el anterior siempre es un n#'umero real.

#subsubsection{Funciones de $#P$}
Si aplicamos directamente la regla de substituci#'on $ #P #rar -i #nabla $, a una funci#'on $f(#P)$, tenemos que darle entonces un significado a la expresi#'on $f(-i #nabla)$. Recordando como act#'ua el operador de momento en el espacio de momentos (como una multiplicaci#'on por la variable $k$, $k #hat{#psi}$), una funci#'on del operador de momento puede ser definida a trav#'es de la transformada de Fourier:
#begin{equation}
f( -i #nabla ) #psi(#x) = #frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{i #k #cdot #x} #, f(#k)#, #hat{#psi} (#k) #, d #k
#end{equation}
 
#subsubsection{Momento angular}
Para una part#'icula cl#'asica en tres dimensiones, el momento angular en la direcci#'on del eje $z$ es una funci#'on del momento y de las coordenadas:
#begin{equation}
L_{3}= x_{1}p_{2}-x_{2}p_{1}
#end{equation}
Las otras componentes del momento est#'an definidas de manera similar:
#begin{equation}
L_{1}= x_{2}p_{3} - x_{3}p_{2} #,#,#,#,#,#,#,#,#,#, L_{2}= x_{3}p_{1} - x_{1}p_{3}
#end{equation}
Esto es escrito en su forma vectorial como:
#begin{equation}
#bf{L}= #x #times #P
#end{equation}

Apliquemos ahora la regla de substituci#'on para el momento angular. De esta forma obtenemos al {#bf operador de momento angular} $#bf{L}= (L_{1},L_{2},L_{3})$ para una part#'icula en tres dimensiones:
#begin{equation}
L-_{j}= -i #, #left( x_{k}#, #frac{#partial}{#partial x_{l}} - x_{l}#, #frac{#partial}{#partial x_{k}} #right)
#end{equation}
en donde $(j,k,l)$ es una permutaci#'on c#'iclica de $(1,2,3)$. 

#newpage


#chapter{Funciones propias y valores propios}

Las siguientes definiciones pueden resultar familiares a algunas provenientes del algebra lineal, aunque existen algunas diferencias. En un espacio vectorial de dimensi#'on finita, los operadores lineales usualmente est#'an definidos en todo el espacio. En los espacios de funciones, la acci#'on de los operadores lineales no puede ser siempre definida para todos  los elementos del espacio y se tiene que restringir el dominio de estos a un subespacio adecuado, siempre y cuando esto sea posible.
#begin{definition}
Sea $A$ un operador lineal definido en un dominio $D(A)$, el cual es un subespacio lineal de un espacio vectorial $H$. Un vector $#phi #in D(A)$, $#phi #neq 0$, es llamado un {#bf vector propio} del operador $A$, si existe un n#'umero complejo $#lambda$, tal que:
#begin{equation}
A#phi = #lambda #phi
#end{equation}
Tal n#'umero $#lambda$, es conocido como {#bf valor propio} del operador $A$. Si el espacio vectorial en cuesti#'on es un espacio de funciones, a los vectores propios tambi#'en los conoceremos como {#bf funciones propias} del operador $A$.
#end{definition}

Es f#'acil ver que el conjunto de vectores propios correspondientes a un mismo valor propio $#lambda$, forman un subespacio lineal.

#begin{definition}
El subespacio lineal generado por todas las funciones propias que pertenecen al mismo valor propio $#lambda$, es conocido como el {#bf  espacio propio} de $#lambda$. La dimensi#'on del espacio propio es conocida como la {#bf multiplicidad} o {#bf grado de degeneraci#'on} de tal valor propio. Un valor propio es llamado {#bf degenerado} si su multiplicidad es mayor de uno, de otra forma es llamado {#bf no degenerado}.
#end{definition} 

Supongamos que el operador lineal $A$ representa un observable de un sistema f#'isico. Si el estado de dicho sistema esta dado por una funci#'on propia $#phi$ del operador $A$ con valor propio $#lambda #in #R$, entonces el valor esperado de $A$ es igual a $#lambda$ y su incertidumbre es cero. El reciproco de esta proposici#'on tambi#'en es cierto y su demostraci#'on es inmediata a trav#'es de las definiciones de valor esperado e incertidumbre.

$$ #la A #ra_{#phi} = #lambda #,#,#,,#,#,#, #Delta_{#phi} A =0 #, #sss #, A#phi = #lambda #phi #,;#,#,#, (#lambda #in #R)  $$

De acuerdo con la interpretaci#'on estad#'istica de la funci#'on de onda, esto significa que siempre que el estado de un sistema este dado por una funci#'on propia de $A$, una medici#'on de este observable siempre va a dar como resultado al valor $#lambda$.##
Cuando el estado de un sistema este dado por una funci#'on propia de un observable $A$, diremos que el sistema se encuentra en un {#bf estado propio} de $A$. Lo anterior lo resumimos a continuaci#'on.

#paragraph{Valor de un observable en un estado propio:} Si el estado $#phi$ de un sistema f#'isico es descrito por una funci#'on propia de un observable $A$ con valor propio $#lambda$, cualquier medici#'on de $A$ dar#'a como resultado al valor $#lambda$.

#rightline{$#dag$}
#vspace{3 mm}

Poe ejemplo, si $#psi$ es una funci#'on propia del Hamiltoniano, $H #psi= E #psi$ con $E #in #R$, cualquier medici#'on de la energ#'ia de sistema dar#'a como resultado el valor $E$.##

Hay operadores lineales para los cuales no existe ninguna funci#'on propia, la cual sea parte del espacio de Hilbert del sistema. Tal es el caso de los operadores de posici#'on y de momento (y por tanto, del operador de energ#'ia cin#'etica, en el caso de las part#'iculas libres). Una funci#'on propia de la posici#'on tendr#'ia incertidumbre cero, lo cual no es posible en el espacio de Hilbert de las funciones de onda de cuadrado integrable. La funci#'on $#delta(x)$ (delta de Dirac) es una especie de funci#'on propia generalizada para el operador $x$, pero $#delta(x)$ no esta en el dominio del operador $x$, ya que esta ni siquiera pertenece al espacio de Hilbert del sistema. De forma similar, las ondas planas $u_{k}(x)=#exp ( ikx )$, satisfacen formalmente la ecuaci#'on de valores propios $ p#, u_{k}(x)= k #, u_{k}(x) $, con el operador de momento $p= -i #, #frac{d}{dx} $. Pero como hemos observado anteriormente, las ondas planas no son funciones de cuadrado integrable y tampoco pertenecen al espacio de Hilbert del sistema. ##


#section{Dependencia temporal de las funciones propias del Hamiltoniano}
En la mec#'anica cu#'antica las funciones propias del Hamiltoniano son de particular importancia. Si $H$ es el Hamiltoniano de un sistema cu#'antico, entonces la ecuaci#'on de valores propios $ H #phi = E #phi $ ($E #in #R$), es conocida como la {#bf ecuaci#'on de Schr#"odinger estacionaria}. Si conocemos una soluci#'on de esta ecuaci#'on (es decir, una funci#'on propia de $H$), podemos f#'acilmente obtener una soluci#'on de la {#bf ecuaci#'on general de Schr#"odinger dependiente del tiempo}. Esta ecuaci#'on dependiente del tiempo no es mas que un problema de valor inicial de la forma:
#begin{equation}#label{sch-t}
i#, #frac{#partial}{#partial t} #psi (t)= H #psi(t),#,#,#,#,#,#,#,#, #psi(0)= #phi
#end{equation}
Si $H#phi = E #phi$, es f#'acil ver por el m#'etodo de separaci#'on de variables con constante de separaci#'on $E$, que:
#begin{equation}
#psi (t)=e^{-iEt} #, #phi
#end{equation}
es una soluci#'on de la ecuaci#'on #eqref{sch-t}. Es claro que si $#psi (0) = #phi$ es una funci#'on propia de $H$, entonces $#psi(t)$ es tambi#'en una funci#'on propia de $H$, con el mismo valor propio para todo tiempo $t$.##

La dependencia temporal de una funci#'on propia es bastante simple como podemos observar, ya que esta completamente descrita por el factor de fase $#exp(-iEt)$. Una funci#'on de onda al tiempo $t_{1}$ difiere solamente en un factor de fase de otra funci#'on de onda al tiempo $t_{2}$. Entonces, de acuerdo con nuestra interpretaci#'on cu#'antica en t#'erminos estad#'isticos, estas funciones de onda representan al mismo estado f#'isico. as#'i, cuando un sistema se encuentra en un estado propio del Hamiltoniano, este permanecer#'a en este mismo estado por siempre. Por tanto, las funciones propias del Hamiltoniano son tambi#'en llamadas {#bf estados estacionarios} del sistema f#'isico. Resumimos estos hechos a continuaci#'on.   
  
#paragraph{Estados estacionarios:} Si el estado de un sistema cu#'antico esta descrito inicialmente por una funci#'on propia del Hamiltoniano $H$, entonces el sistema permanecer#'a en este estado para siempre.##

#vspace{5 mm}    

Todas las cantidades f#'isicas medibles que pueden ser calculadas a trav#'es del estado estacionario del sistema, no dependen del tiempo. Por ejemplo, las distribuciones de posici#'on y momento permanecen iguales para todo tiempo:
#begin{equation}
|#psi (x,t)|^{2}= |#phi (x)|^{2}  #,#,#,#,#,#,,#,#,#,#,#, |#hat{#psi} (k,t)|^{2}= |#hat{#phi}(k)|^{2}
#end{equation}

Un estado propio de $H$ se coloca en una cierta regi#'on del espacio fase y se queda en esta para siempre. Este comportamiento es muy diferente al de la propagaci#'on de paquetes de ondas libres.##
A los estados propios del Hamiltoniano y a sus combinaciones lineales las conoceremos como {#bf estados ligados}.

#subsubsection{Expansi#'on en t#'erminos de funciones propias}
Debido a la linealidad de la ecuaci#'on general de Schr#"odinger dependiente del tiempo #eqref{sch-t} es f#'acil obtener la soluci#'on para cualquier estado inicial que sea una combinaci#'on lineal de funciones propias.

#paragraph{Soluci#'on de la ecuaci#'on de Schr#"odinger dependiente del tiempo:}  
Sea $#{ #psi_{n} #}$ un conjunto de funciones propias correspondientes a los valores propios $#{ E_{n} #}$ del Hamiltoniano $H$ de un sistema f#'isico. Sea $#phi$ un estado inicial del sistema que puede ser escrito como una combinaci#'on lineal de las funciones $#psi_{n}$:
#begin{equation}
#phi = #sum_{n} c_{n} #psi_{n}
#end{equation}
con las constantes complejas apropiadas $c_{n}$. Entonces la #'unica soluci#'on al problema con valor inicial:
#begin{equation}
i#, #frac{#partial}{#partial t} #psi (t)= H #psi (t) ,#,#,#,#,#,#,#,#, #psi (0)= #phi
#end{equation}
esta dada por:
#begin{equation}
#psi (t)= #sum_{n} c_{n}#, #psi_{n} #, e^{-iE_{n}t}
#end{equation}


#chapter{Operadores lineales en espacios de Hilbert}
En este capitulo nos dedicaremos a profundizar un poco mas en las propiedades matem#'aticas de la teor#'ia cu#'antica. Estudiaremos la evoluci#'on temporal de las soluciones estrictas de la ecuaci#'on de Schr#"odinger dependiente del tiempo, daremos otra definici#'on de los operadores sim#'etricos para poder introducir a los operadores autoadjuntos y dar finalmente las caracter#'isticas que debe de tener un operador lineal para poder representar a un observable f#'isico.

#section{El Hamiltoniano y la evoluci#'on temporal}

En el capitulo anterior estudiamos la soluci#'on al problema de valor inicial de la ecuaci#'on de Schr#"odinger dependiente del tiempo en t#'erminos de las funciones propias del Hamiltoniano y la expansi#'on de funciones en t#'erminos de estas mismas. Sin embargo, surge la siguiente pregunta ?`Que tan grande es el conjunto de funciones de cuadrado integrable que pueden ser representadas en t#'erminos de las funciones propias del Hamiltoniano? Para resolver esto, primero es conveniente examinar las propiedades que una funci#'on debe de poseer para poder ser una soluci#'on de la ecuaci#'on de Schr#"odinger dependiente del tiempo.##

La ecuaci#'on de Schr#"odinger tiene la forma general,
#begin{equation}#label{schh}
i #, #frac{#partial}{#partial t}#, #psi (t) = H #psi (t)
#end{equation}
en donde $H$ representa al Hamiltoniano de un sistema f#'isico, el cual es un operador lineal en un espacio de Hilbert $#H$. ##
Si $H$ y $#psi_{0}$ fueran solamente un n#'umeros complejos, sabemos que una soluci#'on de la ecuaci#'on anterior seria una funci#'on dada por $#psi(t) = #exp ( -iHt ) #psi_{0} $. M#'as aun, en este caso podemos encontar una soluci#'on de la forma $#psi(t,#x)$ con $#psi(0,#x) = #psi_{0}$ y $#psi(t,#x) #in #H$, $#forall t$.##

Sin embargo en nuestro caso $H$ es un operador lineal en el espacio de Hilbert $#H$ y no es posible efectuar el procedimiento anterior de forma arbitraria. En esta secci#'on comenzaremos a estudiar las propiedades que deben tener las soluciones de la ecuaci#'on de Schr#"odinger.##

#paragraph{Soluciones estrictas de la ecuaci#'on de Schr#"odinger: }
?`Que propiedades debe de tener una soluci#'on de la ecuaci#'on de Schr#"odinger? Con el objetivo de darle sentido a la derivada temporal de la ecuaci#'on #eqref{schh} tenemos que suponer que el limite,
#begin{equation}#label{dift}
#lim_{h #rar 0} #frac{ #psi(t+h) - #psi(t) }{h}
#end{equation}
existe para toda $t$ y en donde el limite se toma con respecto a la topolog#'ia del espacio de Hilbert del sistema f#'isico (es decir, respecto a la norma $#| #cdot #|$ de $#H$). Si este limite existe es denotado como $ #frac{#partial}{#partial t} #, #psi(t) $. Debemos hacer notar que $#frac{#partial}{#partial t}$ no es un operador que act#'ua sobre el espacio de Hilbert $#H$ del sistema, si no que mas bien este operador act#'ua sobre las funciones de la forma $t #rar #psi(t)$, las cuales tienen valores en $#H$ ($#psi(t) #in #H$).##

Por otro lado, si queremos aplicar el operador $H$ a la funci#'on $#psi (t)$, tal como sucede en el lado derecho de la ecuaci#'on #eqref{schh}, tenemos que estar seguros que la soluci#'on al tiempo $t$ se encuentra en el dominio de definici#'on de $H$. Por tanto, una soluci#'on de la ecuaci#'on de Schr#"odinger debe de tener la propiedad,  
#begin{equation}
#psi (t) #in D(H) #,,#,#,#,#,#,#,#, #forall t
#end{equation}

Aqu#'i hemos descrito las propiedades que debe de tener una soluci#'on estricta de la ecuaci#'on de Schr#"odinger, es decir, soluciones en sentido literal no importando que estas sean o no parte de un problema con valor inicial, simplemente hemos destacado las propiedades que debe de tener una funci#'on para que esta pueda satisfacer la ecuaci#'on #eqref{schh} sin ninguna ambiguedad. Antes de que consideremos las soluciones de problemas con valor inicial, mas generales a los vistos en el capitulo anterior, demos de nuevo un vistazo a la evoluci#'on temporal de las soluciones de la ecuaci#'on libre de Schr#"odinger.

#paragraph{Evoluci#'on temporal de las soluciones libres: } 
La ecuaci#'on #eqref{gen-s} nos muestra que la soluci#'on de la ecuaci#'on libre de Schr#"odinger con funci#'on inicial $#psi_{0}$ puede ser expresada como:
#begin{equation}
#psi (#cdot , t)= #F^{-1} #exp #left( -i#, #frac{k^{2}}{2}#, t #right) #F #psi_{0}
#end{equation}
Por tanto, la evoluci#'on temporal puede ser descrita por medio de la acci#'on de un operador lineal sobre la funci#'on inicial $#psi_{0}$. Este operador es la funci#'on exponencial del Hamiltoniano libre $H_{0}= p^{2}/2$, esto es:
#begin{equation}#label{evv}
e^{-iH_{0}t}=  #F^{-1} #exp #left( -i#, #frac{k^{2}}{2}#, t #right) #F
#end{equation}
Por tanto, la evoluci#'on temporal de las soluciones libres, es la transformada de Fourier inversa de un operador de multiplicaci#'on acotado en el espacio de momentos. Debemos hacer notar que no es del todo obvio el como definir al operador $#exp (-iH_{0}t)$ en t#'erminos de la conocida serie de potencias de la funci#'on exponencial compleja, ya que esta resulta ser una suma infinita de operadores lineales no acotados ($#frac{d}{dx}$ resulta ser un operador lineal no acotado [19] ). Mas aun, para todo entero positivo $n$, el dominio del operador $H_{0}^{n+1}$ resulta ser estrictamente menor al del operador $H_{0}^{n}$. El operador de evoluci#'on de las soluciones libres posee las siguientes propiedades:

#paragraph{Propiedades del operador $#exp (-iH_{0}t)$: }
#begin{enumerate}
#item El dominio del operador $#exp (-iH_{0}t)$ consiste de todo el espacio de Hilbert $#H$.

#item $#| #exp (-iH_{0}t) #psi #| = #| #psi #|$, para toda $#psi #in #H$.

#item Si $ #psi #in D(#H_{0}) $, entonces $#psi (t)= #exp (-iH_{0}t) #psi_{0} $ es una soluci#'on estricta de la ecuaci#'on libre de Schr#"odinger.
#end{enumerate}
#rightline{$#dag$}
#vspace{3 mm} 

La primera propiedad establece que podemos definir una evoluci#'on temporal para cualquier estado inicial del espacio de Hilbert del sistema. La segunda propiedad establece que el operador de evoluci#'on es acotado (por tanto continuo) y de norma uno, para todo $t$. Por lo tanto, la norma de $#psi (t)$ es independiente de $t$, lo cual es esencial para nuestra interpretaci#'on estad#'istica de la funci#'on de onda. Esta propiedad tambi#'en implica que la soluci#'on depende de la condici#'on inicial de forma continua, ya que si comparamos dos evoluciones temporales $#psi(t)$ y $#phi (t)$, correspondientes a dos estados iniciales diferentes $#psi_{0}$ y $#phi_{0}$, entonces:
$$ #| #psi(t) - #phi(t) #|= #| #exp(-iH_{0}t) (#psi_{0} -#phi_{0}) #| = #| #psi_{0} -#phi_{0} #| $$

Si los estados iniciales est#'an muy juntos (en el sentido de la norma $#| #cdot #|$) entonces, los estados al tiempo $t$ tambi#'en van a estar muy juntos. as#'i, la separaci#'on de los estados es constante en el tiempo.##

La tercera propiedad establece que el dominio de $H_{0}$ es invariante en el tiempo, es decir, si el estado inicial pertenece a $D(H_{0})$, entonces el estado al tiempo $t$ tambi#'en es parte de este dominio. Mas aun, el mapeo $t #rar #psi(t)$, es diferenciable con respecto a $t$ en el sentido de la ecuaci#'on #eqref{dift}.##


#section{Operadores unitarios}

En el capitulo dos de espacios de Hilbert, en la definici#'on #ref{unita} introdujimos a los operadores unitarios. Es claro que un espacio de Hilbert $#H$ es isomorfo a si mismo y por tanto existe un operador unitario $U: #H #rar #H$. Es facil ver que el operador identidad y cualquier operador que sea un reordenamiento de la base de $#H$ son algunos ejemplos de operadores unitarios.##

En el desarrollo siguiente supondremos que los operadores unitarios mapean a $#H$ en si mismo.##

Un ejemplo interesante de un operador unitario del espacio de Hilbert $L^{2}(#R^{n})$, es la transformada de Fourier $#F$. Este hecho es consecuencia inmediata de teorema de Fourier-Plancherel (#ref{FPt}) y del hecho de que $#F$ y la transformada inversa $#F^{-1}$ est#'an definidas en todo el espacio $L^{2}(#R^{n})$.##

Otro ejemplo de operador unitario es $#exp( -i H_{0}t )$, tal como se defini#'o en la ecuaci#'on #eqref{evv}. Que este operador sea unitario surge del hecho de que la transformada de Fourier es un operador unitario y que la multiplicaci#'on por el factor de fase, $#exp ( -i k^{2}t/2 )$ defina tambi#'en a un operador unitario en el espacio de momentos.##

Debemos hacer notar que todo operador unitario $U$, satisface las dos primeras propiedades que cumple el operador de evoluci#'on para el caso libre $#exp( -i H_{0}t )$. Por esta raz#'on nos enfocaremos en describir la evoluci#'on temporal de los sistemas cu#'anticos en t#'erminos de operadores unitarios.##

#section{Evoluci#'on temporal y grupos unitarios}
El estudio de la evoluci#'on temporal de los sistemas cu#'anticos nos lleva a la definici#'on del operador lineal de evoluci#'on $U(t)$, el cual mapea al estado inicial del sistema $#psi_{0}$, al estado del sistema al tiempo $t$, $#psi(t)$. Una propiedad que debe de tener este operador de evoluci#'on es la de mantener la norma de los estados a todo tiempo, es decir, $#| #psi_{0} #| = #| #psi(t) #|= #| U(t)#psi_{0} #|$, as#'i como la de mapear al espacio de Hilbert del sistema en si mismo, tal como lo hacen los operadores unitarios. Por tanto podemos decir que para cada $t$, el operador $U(t)$ es unitario en el espacio de Hilbert del sistema. Esto nos lleva a introducir a los {#bf grupos unitarios}. 

#begin{definition}
Un grupo unitario de un par#'ametro, es una funci#'on $t #rar U(t)$, de los n#'umeros reales $t #in #R$ al conjunto de los operadores lineales de un espacio de Hilbert $#H$, con las siguientes propiedades:
#begin{enumerate}
#item Para toda $t$, $U(t)$ es un operador unitario. (propiedad unitaria)
#item $U(0)= #1$, $U(t) #, U(s)= U(t+s)$, para toda $s,t$. (propiedad de grupo) 
#item $#lim_{t #rar 0} U(t) #psi = #psi$, para toda $#psi #in #H$. (continuidad fuerte)
#end{enumerate}
en donde $#1$ denota al {#bf operador identidad}.
#end{definition}

Un grupo unitario posee un {#bf generador} el cual aparece en la ecuaci#'on de evoluci#'on (ecuaci#'on de Schr#"odinger) del sistema cu#'antico que se esta estudiando. 

#begin{definition}
Sea $U(t)$ un grupo unitario de un par#'ametro en un espacio de Hilbert $#H$. El 
generador infinitesimal $H$ del grupo unitario, es el operador lineal dado por:
#begin{equation}#label{H-gen}
H #psi = i #, #lim_{t #rar 0} #frac{ U(t) - #1 }{t}#, #psi 
#end{equation}
cuyo dominio es el conjunto de todos los vectores $#psi$ para los cuales este limite existe.
#end{definition}

Podemos esbozar algunas conclusiones inmediatas de las definiciones anteriores. La mas importante establece que un grupo unitario da la soluci#'on de una ecuaci#'on de Schr#"odinger apropiada, a saber una en la cual el Hamiltoniano $H$ sea el generador de un grupo unitario.

#paragraph{Ecuaci#'on de evoluci#'on temporal: } Sea $U(t)$ un grupo unitario con generador $H$ y sea $#psi (t) = U(t) #psi $. Entonces, para toda $#psi$ en el dominio del generador, $#psi (t)$ es una soluci#'on del problema de valor inicial:
#begin{equation}#label{ec-sch}
i #, #frac{#partial}{#partial t} #psi (t)= H #psi (t)#,, #,#,#,#,#,#,#,#,#, #psi (0)= #psi
#end{equation}
#rightline{$#dag$}
#vspace{3 mm} 

El enunciado anterior incluye la invariancia del dominio $D(H)$, bajo la acci#'on del operador de evoluci#'on. Mas precisamente tenemos que:
$$ U(t) D(H) #subset D(H)#,,#,#,#,#,#,#,#,#,#,#,#,#, HU(t) - U(t)H=0 #hbox{ , en } D(H) $$
Demostramos este hecho a continuaci#'on:##

Si $#psi$ esta en $D(H)$, entonces el limite #eqref{H-gen} existe. Debido a que $U(t)$ es acotado y esta definido en todo $#H$ el teorema #ref{T-ac-con} nos asegura que es tambi#'en un operador continuo y por tanto conmuta con el limite:
#begin{equation}
U(t)#, #lim_{h #rar 0} #frac{U(h) - #1}{h}#, #psi = #lim_{h #rar 0} U(t)#, #frac{U(h) - #1}{h}#, #psi 
#end{equation}
#begin{equation}#label{g-prop}
= #lim_{h #rar 0} #frac{U(t+h) - U(t)}{h}#, #psi 
#end{equation}
#begin{equation}#label{g-prop2}
= #lim_{h #rar 0} #frac{U(h) - #1}{h}#, U(t)#psi
#end{equation}

Por tanto $U(t) #psi$ esta tambi#'en en el dominio de $H$ y $ U(t)H #psi = HU(t) #psi $. En la ecuaci#'on #eqref{g-prop} utilizamos la propiedad de grupo de los operadores unitarios y haciendo $U(t) #psi = #psi (t)$, notamos que esta expresi#'on no es mas que la derivada de $#psi(t)$:
#begin{equation}
#lim_{h #rar 0} #frac{ U(t+h) - U(t) }{h} #, #psi = #frac{#partial}{#partial t}#psi(t)
#end{equation}
Con esto, que $#psi(t)$ y $H$ cumplen con #eqref{ec-sch} es consecuencia inmediata de las ecuaciones #eqref{g-prop} y #eqref{g-prop2} ya que:
#begin{equation}
i#,#frac{#partial}{#partial t}#psi(t)=i#,#lim_{h #rar 0} #frac{ U(t+h) - U(t) }{h} #, #psi
=i#, #lim_{h #rar 0} #frac{U(h) - #1}{h}#, U(t)#psi= H U(t)#psi = H#psi (t)
#end{equation}
#qed
#vspace{5 mm}

Que el operador de evoluci#'on $U(t)$ sea unitario hace que el producto de dos estados $#psi$ y $#phi$ (y por tanto la transici#'on de probabilidad) sea independiente del tiempo:
#begin{equation}
( #psi (t) , #phi (t) ) = ( #psi , #phi )#,,#,#,#,#,#,#,#, #forall t
#end{equation}
Por lo tanto, tenemos que:
#begin{equation}
#frac{#partial}{#partial t} ( #psi (t) , #phi (t) ) =0 
#end{equation}
Desarrollando esta derivada y utilizando las propiedades del producto interno, obtenemos que:
$$ #frac{#partial}{#partial t} ( #psi (t) , #phi (t) ) = #lim_{h #rar 0} #frac{ ( #psi(t+h),#phi(t+h) ) - ( #psi (t) , #phi(t) ) }{h}  $$
$$ = #lim_{h #rar 0} #frac{ ( #psi(t+h),#phi(t+h) )+(-#psi(t),#phi(t+h))-(#psi(t),-#phi(t+h) ) - ( #psi (t) , #phi(t) ) }{h} $$
$$= #lim_{h #rar 0} #left(#frac{#psi(t+h)-#psi(t)}{h}, #phi(t+h) #right) - #lim_{h #rar 0} #left( #psi(t), #frac{ #phi(t) - #phi(t+h) }{h} #right) $$
luego,
$$ i #frac{#partial}{#partial t} ( #psi (t) , #phi (t) ) =  #left( i#, #lim_{h #rar 0} #frac{#psi(t+h)-#psi(t)}{h}, #phi(t+h) #right) - #left(#psi(t),i#, #lim_{h #rar 0} #frac{#phi(t+h) - #phi(t) }{h} #right) $$
De esta forma tenemos que,
$$ i #frac{#partial}{#partial t} ( #psi (t) , #phi (t) ) = (H #psi(t) ,#phi(t) ) - ( #psi(t),H #phi(t)  ) =0 $$
Por tanto, para que $U(t)$ sea unitario es necesario que:
#begin{equation}
( #psi , H#phi )= ( H #psi , #phi ) #,,#,#,#,#,#,#,#,#,#, #forall #psi , #phi #in D(H)
#end{equation} 
Esta propiedad de $H$ es equivalente a que este operador sea sim#'etrico.

#section{Operadores sim#'etricos}
En la definici#'on #ref{op-s} introdujimos a los operadores sim#'etricos, caracteriz#'andolos como los operadores cuyo valor esperado es un n#'umero real para todo elemento de su dominio, el cual es denso en el espacio de Hilbert del sistema cu#'antico que se esta estudiando. La siguiente definici#'on da una nueva caracterizaci#'on de este tipo de operadores la cual es equivalente a la anterior.

#begin{definition}
Diremos que un operador lineal $H$ en un espacio de Hilbert $#H$ es {#bf sim#'etrico} si su dominio es denso y si:
#begin{equation}
( #psi , H#phi )= (H#psi , #phi )
#end{equation}
para todos los vectores $#psi$ y $#phi$ en el dominio de $H$.
#end{definition}

El siguiente resultado es una de las razones principales por las cuales el estudio de los operadores sim#'etricos es tan importante en la teor#'ia cu#'antica.

#begin{theorem}
Un operador sim#'etrico solo tiene valores propios reales. Si $#psi_{1}$ y $#psi_{2}$ son dos vectores propios de un operador sim#'etrico $H$ pertenecientes a dos valores propios diferentes $E_{1}$ y $E_{2}$, entonces $#psi_{1}$ y $#psi_{2}$ son ortogonales,
#begin{equation}
( #psi_{1},#psi_{2} )=0
#end{equation}
#end{theorem}
#n {#bf Demostraci#'on}##
Utilizando la propiedad anti lineal del producto interno tenemos que:
#begin{equation}
#la H #ra_{#psi} = #la #psi , H#psi #ra = #ol{ #la H#psi,#psi #ra }
#end{equation}
Esto implica que el valor esperado $#la H #ra_{#psi}$ de un operador sim#'etrico es un n#'umero real. Si $#psi$ es un vector propio (normalizada) de $H$ con valor propio $E$, el valor esperado de $H$ en este estado esta dado por:
#begin{equation}
#la H #ra_{#psi} = #la #psi,H#psi #ra = E#, #la #psi,#psi #ra = E#, #| #psi #|^{2}= E  
#end{equation}
por tanto $E$ es un n#'umero real.##
Consideremos dos vectores propios de $H$, $#psi_{1}$ y $#psi_{2}$ con valores propios diferentes $E_{1}$ y $E_{2}$, entonces por la simetr#'ia de $H$, tenemos que:
#begin{equation}
#la #psi_{1},H#psi_{2} #ra =  #la H#psi_{1},#psi_{2} #ra
#end{equation}
pero esto implica que,
#begin{equation}
( E_{1}-E_{2} )#, #la #psi_{1},#psi_{2} #ra=0
#end{equation}
pero $E_{1}-E_{2} #neq 0$, por lo que necesariamente $#la #psi_{1},#psi_{2} #ra=0$. #qed

#newpage

#section{El operador adjunto}

#subsubsection{Adjunto de un operador acotado}
Si $T$ es un operador lineal acotado, definido en todo el espacio de Hilbert $#H$ ($T: #H #rar #H$), su {#bf operador adjunto} denotado por $T^{*}$, es el operador para el cual,
#begin{equation}#label{adj-def}
( #phi , T #psi )= ( T^{*}#phi , #psi )
#end{equation}
para toda $ #phi , #psi #in #H $.##

Al tomar el adjunto, podemos mover la acci#'on del operador al otro lado del producto escalar.##
El adjunto de un operador lineal $T$ es #'unico, ya que de existir dos operadores adjuntos $T^{#dag}$ y $T^{*}$, tendr#'iamos que:
$$ (T^{#dag}#phi,#psi )=(#phi , T#psi)=(T^{*}#phi,#psi) #,,#,#,#,#,#,#,#,#,#, #forall #phi , #psi #in #H $$
esto implica que:
$$ (T^{#dag}#phi - T^{*} #phi ,#psi )=0 $$
$#forall #phi , #psi #in #H$, en particular para $#psi =T^{#dag}#phi - T^{*} #phi$, haciendo que, $#| T^{#dag}#phi - T^{*} #phi #|=0$ y por tanto $ T^{#dag}#phi = T^{*} #phi $, $#forall #phi #in #H$.##

#n Es f#'acil ver que $T^{*}$ es un operador lineal, ya que por un lado:
$$ (#phi_{1}+#phi_{2} , T#psi) = (T^{*}(#phi_{1}+#phi_{2}),#psi) $$
y por otro tenemos que:
$$ (#phi_{1}+#phi_{2} , T#psi) = (#phi_{1},T#psi) + (#psi_{2},T#psi) = (T^{*}#phi_{1},#psi) + (T^{*}#phi_{2},#psi) = (T^{*}#phi_{1}+T^{*}#phi_{2},#psi) $$
luego,
$$(T^{*}(#phi_{1}+#phi_{2}),#psi) =(T^{*}#phi_{1}+T^{*}#phi_{2},#psi) $$
para toda $#psi #in #H$. Esto implica que:
#begin{equation}
T^{*}(#phi_{1}+#phi_{2}) = T^{*}#phi_{1}+T^{*}#phi_{2}
#end{equation}
as#'i tambi#'en $ T^{* *} =T $, ya que:
#begin{equation}
(#phi , T#psi )= ( T^{*}#phi , #psi )= #ol{ ( #psi , T^{*} #phi ) } = #ol{ ( T^{* *} #psi , #phi ) }= ( #phi , T^{* *} #psi )
#end{equation}
$#forall #phi #in #H$.##

El siguiente teorema muestra que el adjunto de un operador tambi#'en es acotado.

#begin{theorem}#label{t-ad-nor}
Si $T$ es un operador lineal acotado definido en todo el espacio de Hilbert $#H$, su operador adjunto $T^{*}$ es acotado y sus normas son iguales:
#begin{equation}
#| T #| = #| T^{*} #|
#end{equation}
#end{theorem}
#n {#bf Demostraci#'on}##
Recordemos que la norma de un operador acotado esta dada por:
$$#sup_{ #|#hat{#psi}#|=1 } #| T #hat{#psi} #|= #| T #| $$

Sea $#hpsi #in #H$ tal que $ #| #hpsi #|=1 $, entonces tomando el hecho de que $T=T^{* *}$ y por la desigualdad de Schwarz tenemos que:
$$ #| T^{*} #hpsi #|^{2} = |( T^{*}#hpsi , T^{*}#hpsi  )| = |( TT^{*}#hpsi ,#hpsi  )| #leq #| TT^{*} #hpsi #| #cdot #| #hpsi #| #leq #|T#| #cdot #| T^{*} #hpsi #| #cdot #| #hpsi #| $$
de esta forma, $  #| T^{*} #hpsi #| #leq #|T#| $, $#forall #hpsi #in #H$, tal que $#| #hpsi #|=1$.##
Luego,
#begin{equation}
#|T^{*}#| = #sup_{#| #hpsi #|=1} #| T^{*} #hpsi #| #leq #|T#|
#end{equation}
y por lo tanto $T^{*}$ es acotado.##
Por otro lado tenemos que:
$$ #| T #hpsi #|^{2} = | ( T #hpsi ,T #hpsi ) | = | (T^{*} T #hpsi , #hpsi ) | #leq #| T^{*} T #hpsi #| #cdot #| #hpsi #| #leq #| T^{*} #| #cdot #| T #hpsi #| #cdot #| #hpsi #| $$
de esta forma $ #| T #hpsi #| #leq #| T^{*} #| $. Por tanto,
#begin{equation}
#| T #| = #sup_{ #| #hpsi #|=1 } #| T#hpsi #| #leq #| T^{*} #|
#end{equation}
y podemos concluir que, $#| T #| = #| T^{*} #|$. #qed

#vspace{5 mm}

Si $T$ y $S$ son dos operadores acotados definidos en todo el espacio, es f#'acil ver que:
#begin{equation}#label{ad-prs}
( ST )^{*} = T^{*} S^{*} #,;#,#, (S+T)^{*}= S^{*}+T^{*} #,;#,#, (#alpha S)^{*}= #ol{#alpha}S^{*}
#end{equation}
y tomando el complejo conjugado en la ecuaci#'on #eqref{adj-def},
#begin{equation}
( T#phi ,#psi )= ( #phi , T^{*} #psi )
#end{equation}
$#forall #phi , #psi #in #H$.

#subsubsection{Adjunto de un operador unitario}
Para un operador unitario tenemos:
#begin{equation}
( #phi , #psi )= ( U#phi , U#psi )= ( U^{*}U#phi , #psi  )
#end{equation}
para toda $#phi , #psi #in #H$. Por tanto $ U^{*} U #phi = #phi $, para toda $ #phi #in #H $ lo cual implica que $ U^{*}U= #1 $. Ya que el rango de $U$ es todo
estado $#H$ y $U^{*}$ esta definido en todo este espacio, concluimos que $U^{*}= U^{-1}$. Esto nos lleva a dar la siguiente definici#'on de operador unitario, la cual es completamente equivalente a las que hemos dado anteriormente:
#begin{definition}
Un operador lineal acotado definido en todo el espacio de Hilbert $#H$ es unitario, si y solo si,
#begin{equation}
U^{*}= U^{-1} 
#end{equation}
es decir, $ U^{*}U=UU^{*}= #1 $.
#end{definition}

#subsubsection{Adjunto de un operador no acotado}
Frecuentemente, los operadores que representan observables f#'isicos no son operadores acotados (por ejemplo el operador derivada $#frac{d}{dx}$). Esto requiere que modifiquemos la definici#'on anterior de adjunto para este tipo especial de operadores.
#begin{definition}
Sea $T$ un operador lineal con dominio $D(T)$ el cual es denso en el espacio de Hilbert $#H$. Su operador adjunto es el operador lineal $T^{*}$, para el cual:
#begin{equation}
( #phi , T#psi )=( T^{*}#phi ,#psi )
#end{equation}
para toda $ #psi #in D(T) $ y toda $ #phi #in D(T^{*}) $.
#end{definition}

El dominio de $T^{*}$ esta definido de la siguiente manera: $#phi$ esta en $D(T^{*})$, si y solo si, existe un vector $#xi #in #H$ tal que:
#begin{equation}#label{ad-na}
( #xi , #psi )= ( #phi , T #psi ) #,,#,#,#,#,#,#,#, #forall #psi #in D(T)
#end{equation}
Esta es la definici#'on mas general del dominio de un operador adjunto.##
La condici#'on de que el dominio $D(T)$ sea denso, garantiza que el vector $#xi$ este determinado de forma #'unica por $#phi$ [11], ya que de existir otro vector $#xi^{*} #in #H $ que cumpla la condici#'on #eqref{ad-na} tendr#'iamos que:
$$ #lim_{n #rar #infty} ( #xi-#xi^{*}, #psi_{n} )= ( #xi-#xi^{*}, #psi^{*} )=0 $$
para toda $ #psi^{*} #in #H $, en donde $ #{ #psi_{n} #} #subset D(T) $, tal que $ #psi_{n} #rar #psi^{*} $, lo que implica $ #xi = #xi^{*} $. De esta forma podemos definir sin ambiguedad $T^{*}#phi = #xi $.##

El doble adjunto $T^{* *}$, existe si el dominio $D(T^{*})$ es denso. En este caso $T^{* *}$ es una extensi#'on de $T$, es decir, un operador definido en un dominio mas grande que el de $T$ pero cuyas evaluaciones coinciden en $D(T)$. La demostraci#'on de estos hechos hace uso del teorema de la grafica cerrada y de una teor#'ia mas elaborada [9].

#newpage

#section{Operadores autoadjuntos y el teorema de Stone}
Los operadores autoadjuntos son una clase especial de operadores sim#'etricos con algunas restricciones sobre el dominio de su operador adjunto, las cuales pueden parecer insignificantes aunque son sin embargo importantes.

#begin{definition}
Un operador lineal es {#bf auto adjunto} si $D(T)=D(T^{*})$ con,
#begin{equation}
T^{*}= T
#end{equation}
#end{definition}
#n Es claro que los operadores autoadjuntos son una clase de operadores sim#'etricos, ya que si## 
$#psi ,#phi #in D(T)$, entonces:
$$ ( #psi ,T#phi )=(T^{*} #psi , #phi)=(T#psi , #phi) $$
#rightline{$#dag$}
#vspace{2 mm}

Observamos que la definici#'on de los operadores autoadjuntos requiere en particular que los dominios de $T$ y $T^{*}$ sean iguales. Si $T$ es solo un operador sim#'etrico (acotado o no acotado) entonces sabemos que:
#begin{equation}
(#phi , T#psi )=( T#phi ,#psi )#,,#,#,#,#,#,#,#,#, #forall #phi , #psi #in D(T)
#end{equation}
Recordando la definici#'on del dominio del operador adjunto, concluimos que toda $#phi #in D(T)$, pertenece tambi#'en al dominio de $T^{*}$ y que,
#begin{equation}
T^{*} #phi = T #phi #,,#,#,#,#,#,#,#,#, #forall #phi #in D(T) 
#end{equation}
Por tanto, si un operador es sim#'etrico solo podemos concluir que $ D(T) #subset D(T^{*}) $ y as#'i el operador $T^{*}$ es solo una extensi#'on de $T$ a un dominio mas grande. Notemos que seg#'un lo anterior, los operadores sim#'etricos acotados definidos en todo el espacio, son autoadjuntos.##
En general podemos decir que:
#begin{equation}
T #hbox{es sim#'etrico si }#,#,#,#, T=T^{*} #hbox{ en } D(T) #subset D(T^{*}) 
#end{equation}
#begin{equation}
T #hbox{es auto adjunto si }#,#,#,#, T=T^{*} #hbox{ en } D(T) = D(T^{*}) 
#end{equation}

Un ejemplo de operador auto adjunto en el espacio de Hilbert $L^{2}(R^{n})$ es el operador de multiplicaci#'on por una funci#'on real $f(#x)$. Ya que si,
$$ #psi , #phi #in D(f)= #left#{ #xi #in L^{2}(#R^{n}) #,#Big|#, #int_{#R^{n}} |f(#x) #, #xi (#x)|^{2} #, d#x < #infty #right#} $$
entonces,
$$ (#phi , f #psi)= #int_{#R^{n}} #phi(#x)#, f(#x)#, #ol{#psi}(#x) #, d#x =#int_{#R^{n}}f(#x) #phi(#x)#,#ol{#psi}(#x) #, d#x=(f #phi ,  #psi) $$
Por tanto, los operadores de posici#'on $#x$ y energ#'ia potencial $V(#x)$, son autoadjuntos. Esto tambi#'en es valido en el espacio de momentos para una funci#'on real $g(k)$, en particular el operador de momento es auto adjunto en este espacio. En el espacio de posiciones podemos ver a trav#'es del teorema de Fourier-Plancherel #ref{FPt}, que el operador de momento tambi#'en es auto adjunto aqu#'i:
#begin{equation}
(#phi , p #psi ) = ( #hphi , p#hpsi ) =(p #hphi , #hpsi)= #ol{ ( #hpsi , p#hphi ) }= #ol{ ( #psi , p #phi ) }= ( p#phi , #psi ) 
#end{equation}

Si $T$ es un operador sim#'etrico pero no auto adjunto, puede suceder entonces que $T^{*}$ no sea sim#'etrico. En este caso el operador $T^{*}$ puede tener incluso valores propios complejos. ## Si el adjunto $T^{*}$ resulta ser sim#'etrico, entonces este operador resulta ser auto adjunto, es decir $T^{**}= T^{*}$ [9]. En este caso diremos que el operador $T$ es {#bf esencialmente auto adjunto}.##

Una de las razones por las cuales los operadores autoadjuntos son tan importantes en la mec#'anica cu#'antica, es el teorema de Stone, el cual establece que existe una correspondencia uno a uno entre los grupos unitarios uniparametricos y los operadores autoadjuntos.

#vspace{3 mm}

#begin{theorem}#label{stone} {#bf (Stone)}
Si $U(t)$ es un grupo unitario de un par#'ametro, entonces su generador $H$ es un operador auto adjunto. as#'i tambi#'en, si $H$ es un operador auto adjunto, este es el generador de un #'unico grupo unitario de un par#'ametro $U(t)$.
#end{theorem}
Para una demostraci#'on matem#'atica rigurosa de este hecho nos referimos a [8] y a [9], ya que esta requiere de una teor#'ia mas elaborada.##

Usualmente al grupo unitario generado por el operador auto adjunto $H$ se suele escribir como:
#begin{equation}
U(t)= e^{-iHt}
#end{equation}
Esta notaci#'on tiene sentido en vista de que la ecuaci#'on de evoluci#'on asociada con $U(t)$ es la derivada temporal de la funci#'on exponencial de $H$:
#begin{equation}
i #, #frac{#partial}{#partial t}e^{-iHt}#, #psi = H#, e^{-iHt}#psi #,,#,#,#,#,#,#,#, #forall #psi #in D(H)
#end{equation}
sin embargo la raz#'on de esta notaci#'on tiene sus ra#'ices en el teorema espectral para operadores autoadjuntos, de donde se desprende de forma natural la definici#'on de las funciones de operadores. Para el estudio de estas propiedades y del teorema espectral se requiere de una teor#'ia del an#'alisis funcional mas elaborada, para la cual nos referimos a [10].##

Si $#psi$ es una funci#'on propia de $H$ con valor propio $E$, entonces:
#begin{equation}
e^{-iHt} #psi = e^{-iEt}#psi 
#end{equation}

Del algebra lineal conocemos el teorema espectral para operadores sim#'etricos (Hermitianos) en espacios vectoriales de dimensi#'on finita, el cual establece que siempre es posible formar una base ortonormal con los vectores propios correspondientes a este tipo de operadores.  En el caso de los operadores lineales en espacios de dimensi#'on finita es f#'acil ver que el dominio de los operadores y de los adjuntos asociados a estos es siempre el mismo. Sin embargo, en los espacios de dimensi#'on infinita, tales como los espacios de Hilbert de funciones, hemos visto que esto no sucede siempre de esta forma en especial para los operadores sim#'etricos, al menos que estos resulten ser autoadjuntos. ##
Antes de establecer la definici#'on formal de un observable f#'isico de un sistema cu#'antico, introducimos las siguientes clases de operadores lineales.

#begin{definition}
Diremos que un operador lineal $T: #H #rar #H$, es de {#bf rango finito} si puede ser representado como:
#begin{equation}#label{frop}
T #psi = #sum_{i=1}^{N} ( #phi_{i},#psi )#, #xi_{i} #,,#,#,#,#,#,#,#, (N <#infty)
#end{equation}
en donde $ #phi_{1},...,#phi_{N},#xi_{1},...,#xi_{N} #in #H $.
#end{definition}

Es claro que el rango de un operador de rango finito es el subespacio de dimensi#'on finita generado por el conjunto $#{ #xi_{1},...,#xi_{N} #} #subset #H$ y que $T#psi =0$, si $#psi$ es ortogonal al espacio generado por el conjunto $#{ #phi_{1},...,#phi_{N} #} #subset #H$. Por otro lado, todo operador cuyo rango sea de dimensi#'on finita puede ser representado en la forma #eqref{frop}. Notemos que todo operador de rango finito es acotado. 

#begin{definition}
Diremos que el operador acotado $A$ es compacto, si existe una sucesi#'on $#{ T_{n} #}$ de operadores de rango finito, tal que, 
$$ #| A - T_{n} #| #rar 0 $$
cuando $n #rar #infty$.
#end{definition}

Si $A$ es un operador compacto y auto adjunto, entonces es posible encontrar una base ortonormal $#{ #hpsi_{i} #}$ formada por vectores propios de $A$ ( $A#hpsi_{i}= #lambda_{i}#, #hpsi_{i} #,,#,#, #lambda #in #R$ ). Lo anterior se conoce como el {#bf teorema espectral para operadores compactos} [10]. Para operadores autoadjuntos y no compactos tal base de vectores propios puede no existir. Ahora estamos listos para dar la definici#'on formal de un observable f#'isico.
#begin{definition}#label{obs}
En el formalismo de la mec#'anica cu#'antica un {#bf observable f#'isico} es representador por un operador auto adjunto apropiado del espacio de Hilbert del sistema f#'isico que se esta estudiando.
#end{definition}

En particular en este texto estamos iteresados solamente en observables que no tienen espectro continuo; esto es, solo consideraremos observables, representados por operadores auto adjuntos, cuyo conjunto de valores propios forme un conjunto de numeros reales discreto y el correspondiente conjunto de vectores propios forme una base ortonormal del espacio de Hilbert que estamos estudiando.

#vspace{3 mm}

El teorema de Stone establece que a todo operador auto adjunto se le puede asociar un grupo unitario (y solo uno) de un par#'ametro, el cual describe la evoluci#'on temporal de un sistema f#'isico. As#'i, por la definici#'on anterior tenemos que para todo observable f#'isico podemos encontrar tal grupo unitario, en particular para el Hamiltoniano cu#'antico, con lo que la ecuaci#'on de Schr#"odinger y el problema de valor inicial asociado a esta, quedan establecidos sin ninguna ambiguedad.##



#newpage

#chapter{Postulados de la mec#'anica cu#'antica}
En los capitulos anteriores nos hemos dedicado a estudiar algunos de los aspectos matem#'aticos mas importantes que dan forma y solides a la teor#'ia cu#'antica, sin profundizar demasiado en el sentido f#'isico que estos tienen. En este capitulo nos dedicaremos a crear la conexi#'on entre los aspectos matem#'aticos mas abstractos de la teor#'ia cu#'antica que nos hemos encontrado hasta ahora, con lo que en verdad podemos apreciar en el mundo real por medio de mediciones y experimentos. Esta conexi#'on quedara establecida a trav#'es de los seis postulados de la mec#'anica cu#'antica. Podemos decir que la interpretaci#'on f#'isica que establecen los primeros postulados ya a sido discutida en las secciones anteriores, sin embargo es importante mencionarlos para que no queden ambiguedades en lo que establecen los postulados restantes. As#'i tambi#'en debemos destacar la generalidad de los siguientes postulados ya que estos son validos para cualquier sistema cu#'antico, incluso para aquellos formados por mas de una part#'icula.

#section{Descripci#'on del estado de un sistema}
En las secciones anteriores introdujimos el concepto de estado cu#'antico, caracteriz#'andolo por medio de una funci#'on de onda de cuadrado integrable, en un espacio de Hilbert apropiado. Esta funci#'on de onda describe en su totalidad a un sistema cu#'antico a un cierto tiempo $t_{0}$ y a trav#'es del operador de evoluci#'on es posible conocer el estado a cualquier otro tiempo de este sistema. Parte de esto se resume en el primer postulado de la mec#'anica cu#'antica, el cual (por su condici#'on de postulado en esta teor#'ia) es valido para {#bf cualquier sistema f#'isico}.

#paragraph{Primer postulado de la mec#'anica cu#'antica:} Al tiempo $t_{0}$ el estado de un sistema f#'isico esta determinado por una funci#'on de onda $#psi(t_{0})$ la cual pertenece a un cierto espacio de Hilbert $#H$.
#rightline{$#dag$}
#vspace{3 mm} 

El primer postulado establece que el estado de un sistema cu#'antico, al tiempo $t_{0}$, solo puede ser representado a trav#'es de elementos del espacio de Hilbert $#H$. Por esta raz#'on la funci#'on delta de Dirac, la cual como hemos vimos anteriormente presume ser la funci#'on propia de los operadores de posici#'on y de momento, no puede describir el estado cu#'antico de un sistema ya que esta no pertenece al espacio de Hilbert $L^{2}(#R^{n})$ (sin mencionar que con esta se violar#'ia el principio de incertidumbre).

#section{Descripci#'on de cantidades f#'isicas}
En las secciones anteriores ya hemos estudiado las caracter#'isticas que un operador lineal debe cumplir para que este sea un observable, a saber este debe de ser auto adjunto y su conjunto de vectores propios debe de formar una base del espacio de Hilbert en el que estamos trabajando. Al decir que un operador es un #emph{observable}, nos referimos a que este es capaz de darnos informaci#'on f#'isica del sistema, la cual es posible verificar en la practica a trav#'es de mediciones o experimentos. El segundo postulado de la mec#'anica cu#'antica establece precisamente esto.##

#paragraph{Segundo postulado de la mec#'anica cu#'antica:} En un sistema f#'isico, cualquier cantidad medible $#mathcal{A}$, es descrita por un operador auto adjunto $A$, el cual es un observable (def. #ref{obs}) que act#'ua en el espacio de Hilbert del sistema.
#rightline{$#dag$}
#vspace{3 mm}

A diferencia de la mec#'anica cl#'asica, la cual describe estados como puntos en el espacio fase y observables como simples cantidades num#'ericas, la mec#'anica cu#'antica describe estados por medio de vectores de un espacio de Hilbert y observables a trav#'es de operadores que act#'uan en este mismo espacio.

#section{Medici#'on de cantidades f#'isicas}
Hemos visto que si el estado de un sistema f#'isico es descrito por una funci#'on de onda $#phi$ la cual resulta ser una funci#'on propia del observable $A$ con valor propio $#lambda$, entonces toda medici#'on de $A$ en este estado va a dar como resultado el valor $#lambda$. El tercer postulado de la mec#'anica cu#'antica generaliza lo anterior a cualquier posible estado de un sistema f#'isico.

#paragraph{Tercer postulado de la mec#'anica cu#'antica:} En un sistema cu#'antico, el #'unico resultado posible de la medici#'on de una cantidad f#'isica $#mathcal{A}$, es uno de los valores propios correspondientes de su observable asociado $A$.

#rightline{$#dag$}
#vspace{3 mm}

Una medida de la cantidad f#'isica $#mathcal{A}$, va a dar siempre como resultado un n#'umero real, ya que por definici#'on el observable $A$ es sim#'etrico. En caso de que el {#bf espectro} de $A$ (su conjunto de valores propios) sea un conjunto discreto, diremos que la cantidad f#'isica $#mathcal{A}$ esta {#bf cuantizada}. En este punto debemos hacer notar que al decir, una medici#'on de $A$, en realidad nos estamos refiriendo a una medici#'on de la cantidad f#'isica asociada al operador (observable) $A$.

#section{Principio de descomposici#'on espectral}
Por definici#'on, el conjunto de funciones propias de un observable $A$ forma una base ortonormal del espacio de Hilbert del sistema f#'isico que estamos estudiando, por lo que todo posible estado de nuestro sistema (funci#'on de onda) puede ser escrito como una combinaci#'on lineal de estas funciones propias. Por el tercer postulado sabemos que el resultado de cualquier medici#'on del observable $A$ va a dar como resultado un valor propio de este, pero en el caso de un estado en general ?`cual de todos los posibles valores propios vamos a obtener como resultado de una medici#'on? mas aun ?`Es posible predecir con certeza este valor? El cuarto postulado de la mec#'anica cu#'antica establece que podemos obtener cualquiera de estos valores, cada uno con cierta probabilidad.## 
Primero analizaremos el caso en el que el espectro del observable $A$ es un conjunto discreto. Si todos los valores propios $#lambda_{n}$ de $A$ son no degenerados, a cada uno de estos le corresponde una #'unica funci#'on propia normalizada $#phi_{n}$:
$$ A #phi_{n}= #lambda_{n}#, #phi_{n} $$
Ya que el conjunto de funciones propias $#{ #phi_{n} #}$ es una base, podemos escribir a cualquier estado $#psi$ del sistema como:
$$ #psi = #sum_{n} c_{n} #phi_{n} $$
En donde $c_{n} #in #C$.

#paragraph{Cuarto postulado de la mec#'anica cu#'antica(Espectro discreto no degenerando):} Cuando la cantidad f#'isica $#mathcal{A}$ es medida en un sistema f#'isico en el estado normalizado $#psi$, la probabilidad $P(#lambda_{n})$ de obtener el valor propio no degenerado $#lambda_{n}$, correspondiente al observable $A$, esta dada por:
#begin{equation}
P(#lambda_{n})= #left| ( #phi_{n},#psi ) #right|^{2}
#end{equation}
En donde $ ( #cdot , #cdot ) $ es el producto interno del espacio de Hilbert del sistema y $ #phi_{n} $ es la funci#'on propia normalizada correspondiente al valor propio $#lambda_{n}$.##
#rightline{$#dag$}
#vspace{3 mm}

Ahora, si algunos de los valores propios del observable $A$ son degenerados, a cada uno de estos le corresponde un conjunto de funciones propias $ #phi_{n}^{i} $:
$$ A #phi_{n}^{i} = #lambda_{n}#, #phi_{n}^{i} #,,#,#,#,#,#,#, #forall i=1,2,...,g_{n} $$
En donde $g_{n}$ es el grado de degeneraci#'on del valor propio $#lambda_{n}$. Notemos que el conjunto de funciones propias $#phi_{n}^{i}$ de $#lambda_{n}$, es un conjunto ortonormal ya que este es sub conjunto de una base ortonormal. El estado $#psi$ puede ser representado en t#'erminos de la base ortonormal $#{ #phi_{n}^{i} #}$ forma de la siguiente forma:
$$ #psi = #sum_{n} #sum_{i=1}^{g_{n}} c_{n}^{i}#, #phi_{n}^{i} $$
En donde $c_{n}^{i} #in #C$.
Es claro que cuando $g_{n}=1$, tenemos el caso de un valor propio no degenerado, por lo que la expresi#'on anterior es el caso general de la descomposici#'on de un estado en t#'erminos de las funciones propias de un observable. La siguiente versi#'on del cuarto postulado es el caso general para un observable con espectro discreto.
 
#paragraph{Cuarto postulado de la mec#'anica cu#'antica(Espectro discreto):}
Cuando la cantidad f#'isica $#mathcal{A}$ es medida en un sistema f#'isico en el estado normalizado $#psi$, la probabilidad $P(#lambda_{n})$ de obtener el valor propio $#lambda_{n}$ correspondiente al observable $A$, esta dada por:
#begin{equation}
P(#lambda_{n})= #sum_{i=1}^{g_{n}} #left| ( #phi_{n}^{i},#psi ) #right|^{2}
#end{equation}
En donde $g_{n}$ es el grado de degeneraci#'on de $#lambda_{n}$ y $#{ #phi_{n}^{i} #}_{i=1}^{g_{n}}$ es el conjunto de vectores propios ortonormales de $A$ con valor propio $#lambda_{n}$.##
#rightline{$#dag$}
#vspace{3 mm}

Es f#'acil ver que la probabilidad de obtener alg#'un valor propio como resultado de una medici#'on del observable $A$ es igual a uno, ya que:
$$ #sum_{n} P(#lambda_{n}) = #sum_{n} #sum_{i=1}^{g_{n}} |( #phi_{n}^{i},#psi )|^{2} = #sum_{n} #sum_{i=1}^{g_{n}} |c_{n}^{i}|^{2}= #| #psi #|^{2}=1 $$
Es claro que si $#phi$ es una funci#'on propia de $A$ con valor propio $#lambda$, entonces:
$$P(#lambda)= |(#phi,#phi)|^{2}=1$$
y que para cualquier otro valor propio $#beta #neq #lambda$ correspondiente a la funci#'on propia $#xi$:
$$ P(#beta)= |(#xi,#phi)|^{2}=0$$
ya que $#phi$ y $#xi$ son parte de un conjunto ortonormal.##

#paragraph{Cuarto postulado de la mec#'anica cu#'antica(Espectro continuo no degenerado):}

Los operadores de posici#'on $#x$ y de momento $#P$ son operadores auto adjuntos, por tanto, por la definici#'on #ref{obs} estos representan observables de un sistema cuantico, a saber los de posicion y momento respectivamente. Sin embargo estos dos operadores no tienen un espectro discreto de operadores por lo que el cuanto postulado debe de ser modificado para este tipo de observables.## 

Supongamos ahora que el espectro del observable $A$ es un conjunto continuo y supongamos por simplicidad que es no degenerado.##
En conjunto de vectores propios $#{ #phi_{#alpha} #}_{#alpha #in #Lambda} $ de $A$ forma un conjunto ortonormal y es una base #emph{continua} del espacio de Hilbert del sistema que estamos estudiando. De esta forma, a un estado $#psi$ de este subespacio lo podemos expresar como:
$$ #psi = #int_{#Lambda} c(#alpha) #, #phi_{#alpha} #, d#alpha $$

Ya que el posible resultado de una medida de $A$ forma un conjunto continuo, debemos de definir una densidad de probabilidad, justo como lo hicimos para la interpretaci#'on de la funci#'on de onda para una part#'icula. De esta forma, la probabilidad $dP(#alpha)$ de que el valor de una medicion de $A$ de como resultado un valor entre $#alpha$ y $#alpha + d #alpha$ esta dada por:
$$ dP(#alpha)= #rho(#alpha) #, d #alpha $$

en donde,
$$ #rho(#alpha) = |c(#alpha)|^{2} = | ( #phi_{#alpha},#psi ) |^{2} $$
con $#phi_{#alpha}$ el vector propio correspondiente al valor propio $#alpha$.##


#section{Reducci#'on del paquete de ondas}
Supongamos que a un cierto tiempo queremos medir una cantidad f#'isica $#mathcal{A}$ en un sistema cu#'antico, el cual justo antes de la medici#'on esta descrito por la funci#'on de onda normalizada $#psi$. Como vimos anteriormente, el cuarto postulado nos permite calcular las probabilidades de obtener los diferentes valores posibles de esta medici#'on, pero es claro que cuando esta es realizada obtendremos solamente un valor. Inmediatamente despu#'es de realizar la medici#'on del observable ya no podemos hablar de la probabilidad de obtener uno u otro valor en una medici#'on, debido a que ya hemos obtenido uno en concreto. Por tanto, ahora poseemos mas informaci#'on acerca del estado del sistema, por lo cual es entendible que el estado de este despu#'es de la medici#'on, el cual incorpora esta nueva informaci#'on, sea diferente del estado inicial descrito por $#psi$.##
Consideremos primero el caso en el que una medici#'on del observable $A$, da como resultado a un valor propio no degenerado $#lambda_{n}$. De esta forma postulamos ahora que el estado del sistema inmediatamente despu#'es de la medici#'on de $A$, va a estar descrito por la funci#'on propia $#phi_{n}$ asociada al valor propio $#lambda_{n}$:
$$ #psi #overset{ #lambda_{n} }{#Rightarrow} #phi_{n} $$

Si realizamos una segunda medici#'on de $#mathcal{A}$ inmediatamente despu#'es de la primera (antes de que el sistema tenga tiempo de evolucionar) vamos a obtener como resultado el mismo valor $ #lambda_{n} $ ya que el estado del sistema inmediatamente antes de la segunda medici#'on ya esta dado por la funci#'on de onda $#phi_{n}$ y no por $#psi$.

Cuando una medici#'on da como resultado un valor propio degenerado $#lambda_{n}$, es posible generalizar la discusi#'on anterior de la siguiente forma. Si escribimos al estado $#psi$, el cual describe al sistema justo antes de la medici#'on en t#'erminos de los vectores propios del observable $A$, 
$$ #psi = #sum_{n} #sum_{i=1}^{g_{n}} c_{n}^{i}#, #phi_{n}^{i} $$
tenemos que la modificaci#'on de la funci#'on de onda justo despu#'es de la medici#'on esta dada por:
#begin{equation}#label{m-pr}
#psi #overset{ #lambda_{n} }{#Rightarrow} #frac{1}{ #sqrt{ #sum_{i=1}^{g_{n}} |c_{n^{i}}|^{2}} } #sum_{i=1}^{g_{n}} c_{n}^{i}#, #phi_{n}^{i} 
#end{equation}
definimos a la funci#'on $#psi_{n}$, como:
$$ #psi_{n}= #sum_{i=1}^{g_{n}} c_{n}^{i}#, #phi_{n}^{i} $$
la cual, es la proyecci#'on de la funci#'on de onda $#psi$ sobre el subespacio propio de las funciones propias $#phi_{n}^{i}$, asociadas al valor $#lambda_{n}$. Notemos que $#psi_{n}$ es tambi#'en una funci#'on propia de $A$. De esta forma, definimos tambi#'en al operador de proyecci#'on $P_{n}$ asociado al subespacio propio del valor $#lambda_{n}$ el cual act#'ua de la siguiente forma:
$$  P_{n} #psi =#psi_{n} $$ 
as#'i podemos re-escribir a #eqref{m-pr} de la siguiente forma:
#begin{equation}
#psi #overset{ #lambda_{n} }{#Rightarrow} #frac{P_{n} #psi}{#| P_{n}#psi #|} 
#end{equation}

Todo lo anterior se resume en el siguiente postulado.

#paragraph{Quinto postulado de la mec#'anica cu#'antica:}
Si la medici#'on de una cantidad f#'isica $#mathcal{A}$ en un sistema en el estado normalizado $#psi$ da como resultado el valor $#lambda_{n}$, el estado del sistema inmediatamente despu#'es de la medici#'on esta dado por la {#bf proyecci#'on normalizada},
#begin{equation}
#psi #overset{ #lambda_{n} }{#Rightarrow} #frac{P_{n} #psi}{#| P_{n}#psi #|}
#end{equation}
de $#psi$ sobre el subespacio propio del valor $#lambda_{n}$.##
#rightline{$#dag$}
#vspace{2 mm}

Por tanto, el estado de un sistema f#'isico inmediatamente despu#'es de una medici#'on es siempre una funci#'on propia del observable $A$ con valor propio $#lambda_{n}$. Debemos remarcar el hecho de que en el caso degenerado, el vector propio al cual se reduce el estado $#psi$ despu#'es del proceso de medici#'on, es solo la $parte$ de $#psi$ que pertenece al subespacio propio correspondiente al valor $#lambda_{n}$.##

#subsubsection{Proyecciones y propiedades}
Un operador de proyecci#'on ortogonal, es un operador auto adjunto acotado $P$ que solo tiene como valores propios a $0$ y $1$. Por tanto, esta caracterizado por la propiedad $P^{2}=P=P^{*}$. El observable f#'isico representado por un operador de proyecci#'on, describe cuando un sistema posee cierta propiedad o no. El medir si un sistema tiene o no una propiedad, solo puede dar los resultados #emph{si} (valor propio $1$) o #emph{no} (valor propio $0$). El rango de $P$, esto es el subespacio propio correspondiente al valor propio $1$, es el subespacio de estados que si poseen esta propiedad, mas aun, para $#psi #in #hbox{Ran}(P)$, $P#psi = #psi$. Es f#'acil ver de la definici#'on [15], que estos operadores cumplen con la propiedad:
#begin{enumerate}
#item $ #|P#|=1 $, siempre que $P#neq 0$.
#item $(#phi ,P #psi)= (P#phi ,P #psi)$
#end{enumerate}
Otra propiedad importante de este tipo de operadores, es que el conjunto de todas las proyecciones ortogonales de un espacio de Hilbert $#H$, tiene una relaci#'on uno a uno, con el conjunto de todos los subespacios de $#H$, es mas, el conjunto $#hbox{Ran}(P)$ es un subespacio de $#H$ [15].##
Para cada observable $A$, existe una #'unica familia de operadores de proyecci#'on $#{ P_{B}(A) | B #subset #R #hbox{ ,medible} #}$. El operador de proyecci#'on $P_{B}(A)$ describe si el observable $A$ tiene un valor en el conjunto $B #subset #R$ o no, cuando es aplicado a un cierto estado $#psi$. 

#section{Evoluci#'on temporal de los sistemas cu#'anticos}
En las secciones anteriores ya hemos introducido a la ecuaci#'on general de Schr#"odinger para una part#'icula en unidades en las cuales tanto $#hbar$ como la masa eran iguales a uno. Es en este punto en el que la introduciremos en el caso general con las unidades est#'andar.##

#paragraph{Sexto postulado de la mec#'anica cu#'antica:} 
La evoluci#'on temporal de la funci#'on de onda $#psi (t)$ esta dada por la ecuaci#'on de Schr#"odinger:
#begin{equation}
i #hbar #, #frac{#partial}{#partial t} #psi (t)= H #psi(t)
#end{equation}
en donde $H$ es el observable Hamiltoniano asociado con la energ#'ia total del sistema, el cual act#'ua como:
#begin{equation}
H #psi= -#frac{#hbar^{2}}{2m} #Delta #psi + V(#x) #psi
#end{equation}
#rightline{$#dag$}
#vspace{2 mm}

#newpage

#chapter{Sistemas compuestos}
Hasta este momento (aparte de los seis postulados de la mec#'anica cu#'antica) solo hemos considerado sistemas formados por una sola part#'icula. Sin embargo, los verdaderos problemas f#'isicos comienzan cuando consideramos sistemas formados al menos por dos part#'iculas. En este capitulo, como un primer ejemplo, consideraremos a la ecuaci#'on de Schr#"odinger para dos part#'iculas libres, con lo cual mostraremos como construir estados cu#'anticos para sistemas de dos part#'iculas como el producto de dos estados de una sola part#'icula. La formulaci#'on abstracta de este m#'etodo esta dada por el producto tensorial entre espacios de Hilbert. as#'i el espacio de Hilbert de un sistema cu#'antico compuesto, esta dado por el producto tensorial de los espacios de Hilbert de los sistemas individuales. Como hemos visto con anterioridad, el producto tensorial de espacios de Hilbert no solo contiene al producto tensorial de sus elementos, sino que tambi#'en contiene a sus combinaciones lineales y que es en realidad la cerradura de este ultimo conjunto. Es por esta raz#'on que los estados de los sistemas cu#'anticos compuestos son, en general, {#bf estados entrelazados} esto es, estados que no pueden ser expresados siempre como el producto tensorial de elementos de los respectivos espacios de Hilbert de los sistemas individuales. Veremos que el entrelazamiento de los sistemas no es creado debido a procesos de medici#'on locales o manipulando a los sistemas individuales, si no que mas bien este solo puede ser creado por medio de las interacciones entre los sistemas individuales.  

#section{Estados de sistemas de dos part#'iculas}

#subsubsection{El Hamiltoniano para dos part#'iculas libres}
Para introducir la noci#'on de la funci#'on de onda para un sistema de dos part#'iculas, consideraremos un sistema de dos part#'iculas libres las cuales no interact#'uan de ninguna forma entre ellas. Por razones que aclararemos mas adelante, supondremos que las part#'iculas no son id#'enticas y que pueden ser distinguidas por alguna propiedad f#'isica (por ejemplo, su masa puede ser diferente $m_{1} #neq m_{2}$). Mas aun, supondremos que el espacio de Hilbert de cada una de las part#'iculas es $L^{2}(#R^{n})$. Vamos a utilizar las reglas de sustituci#'on (ecs. #eqref{sus1},#eqref{sus2},#eqref{sus3}) para formular el Hamiltoniano cu#'antico correspondiente a este sistema compuesto, a partir del Hamiltoniano cl#'asico para dos part#'iculas libres. De las soluciones de la ecuaci#'on de Schr#"odinger que obtengamos con este Hamiltoniano, vamos a aprender como es la estructura t#'ipica de las funciones de onda de sistemas de dos part#'iculas.

Suponiendo que cada part#'icula se mueve en un espacio de $n$ dimensiones, el espacio de configuraci#'on cl#'asico de este sistema de dos part#'iculas es $#R^{2n}$. Los puntos $#x = ( #x_{1} , #x_{2} )$, en este espacio describen la configuraci#'on del sistema, esto es, las posiciones $#x_{1}$ y $#x_{2}$ de las part#'iculas individuales. El Hamiltoniano de este sistema se obtiene a trav#'es de la expresi#'on cl#'asica de la energ#'ia por medio de las reglas de sustituci#'on, de la siguiente forma; El operador de momento $#P_{j}$ para la part#'icula $j$ esta dada por el operador diferencial $ -i #hbar #nabla_{j} $, (gradiente respecto a las coordenadas $#x_{j}$). El Hamiltoniano es entonces la suma de la energ#'ia cin#'etica de las dos part#'iculas, esto es,
#begin{equation}#label{Ham-2p-hb}
H= #frac{ (#P_{1})^{2} }{2m_{1}} + #frac{ (#P_{2})^{2} }{2m_{2}} = - #frac{ #hbar^{2} }{2m_{1}} #Delta_{1} - #frac{ #hbar^{2} }{2m_{2}} #Delta_{2}
#end{equation}
En donde $m_{1}$ y $m_{2}$ son las masas de las part#'iculas respectivamente. Aqu#'i suponemos que las part#'iculas se mueven libremente, ya que no tenemos un termino de energ#'ia potencial el cual pueda describir, en particular, alguna interacci#'on entre las dos part#'iculas.##
El operador lineal $H$ act#'ua sobre funciones diferenciables $#psi( #x_{1},#x_{2} )$ de $2n$ variables de posici#'on. El operador $#Delta_{j}$ es el operador de Laplace el cual se diferencia respecto a las variables $ #x_{j}= ( x_{j}^{(1)},...,x_{j}^{(n)} ) $,
$$ #Delta_{j} = #frac{ #partial^{2} }{ #partial(x_{j}^{(1)})^{2} } + #cdot #cdot #cdot + #frac{ #partial^{2} }{ #partial(x_{j}^{(n)})^{2} } #,,#,#,#,#,#,#, #hbox{ con } j=1,2 $$

#subsubsection{Ecuaci#'on de Schr#"odinger para un sistema de dos part#'iculas}
Consideremos a la ecuaci#'on general de Schr#"odinger dependiente del tiempo con el Hamiltoniano para dos part#'iculas obtenido anteriormente. Re escalando las unidades de longitud, podemos deshacernos de la constante $#hbar$ y escribir a la ecuaci#'on de Schr#"odinger como,
#begin{equation}#label{ec-2p}
i #frac{#partial}{ #partial t } #psi (#x_{1} , #x_{2},t)= - #left( #frac{1}{2m_{1}} #Delta_{1} + #frac{1}{2m_{2}} #Delta_{2} #right) #, #psi (#x_{1},#x_{2},t) 
#end{equation}
Notamos que el Hamiltoniano es la suma de dos t#'erminos que conmutan el uno con el otro. Por tanto, podemos encontrar un conjunto especial de soluciones de esta ecuaci#'on por el m#'etodo de separaci#'on de variables, haciendo:
#begin{equation}#label{sol-sep}
#psi (#x_{1} , #x_{2},t) = #psi_{1} (#x_{1},t) #cdot #psi_{2} (#x_{2},t)
#end{equation}
con $#psi #in L^{2}(#R^{2n})$, $#|#psi#|=1$ 
Es f#'acil ver que si la soluci#'on de la ecuaci#'on #eqref{ec-2p} es de la forma #eqref{sol-sep} entonces las funciones $#psi_{1}$ y $#psi_{2}$ son las funciones de onda libres de las part#'iculas uno y dos respectivamente. Esto se puede hacer de la siguiente forma; Seg#'un la interpretaci#'on estad#'istica de la funci#'on de onda en n#'umero $|#psi(#x_{1} , #x_{2},t)|^{2}$ expresa la densidad de probabilidad de encontrar a la part#'icula uno en la posici#'on $#x_{1}$  y a la part#'icula 2 en la posici#'on $#x_{2}$ al tiempo $t$. De esta forma la probabilidad de que la part#'icula uno se encuentre en la regi#'on $A #subset #R^{n}$ y que la part#'icula dos se encuentre en la regi#'on $B #subset #R^{n}$ al tiempo $t$ esta dada por:
#begin{equation}
P(A,B)= #int_{B} #int_{A} |#psi(#x_{1} , #x_{2},t)|^{2} #, d#x_{1}#, d#x_{2}= #int_{A} |#psi_{1}(#x_{1},t)|^{2}#, d#x_{1} #cdot #int_{B} |#psi_{2}(#x_{2},t)|^{2}#, d#x_{2}
#end{equation}
Luego de esta expresi#'on observamos que la densidad de probabilidad de que la part#'icula uno se encuentre en la posici#'on $#x_{1}$ mientras que la part#'icula dos esta #emph{en cualquier lugar} del espacio, esta dada por:
#begin{equation}
#rho_{1}(#x_{1})= #int_{#R^{n}} |#psi_{2}(#x_{2},t)|^{2}#, d#x_{2} #cdot |#psi_{1}(#x_{1},t)|^{2} 
#end{equation}
pero tomando el hecho de que la funci#'on $#psi( #x_{1},#x_{2},t )$ esta normalizada la expresi#'on anterior se puede re-escribir como:
#begin{equation}
#rho_{1}(#x_{1})= 1 #cdot |#psi_{1}(#x_{1},t)|^{2} = |#psi_{1}(#x_{1},t)|^{2}
#end{equation}
y por tanto, ya que las part#'iculas no son afectadas por ning#'un potencial y no interact#'uan entre si de ninguna manera, podemos concluir que $#psi_{1}(#x_{1},t)$ es la funci#'on de onda libre de la part#'icula uno y utilizando un argumento completamente an#'alogo tenemos que $#psi_{2}(#x_{2},t)$ es tambi#'en la funci#'on de onda libre de la part#'icula dos. De esta forma, ambas funciones deben de ser soluci#'on de las ecuaciones libres:
#begin{equation}#label{lib1}
 i #frac{#partial}{ #partial t } #psi_{1} (#x_{1},t)= -  #frac{1}{2m_{1}} #Delta_{1} #psi_{1} (#x_{1},t)
#end{equation}
#begin{equation}#label{lib2}
i #frac{#partial}{ #partial t } #psi_{2} (#x_{2},t)= -  #frac{1}{2m_{2}} #Delta_{2} #psi_{2} (#x_{2},t)
#end{equation}

Ahora, probemos que la funci#'on #eqref{sol-sep} con $#psi_{1}$ y $#psi_{2}$ satisfaciendo #eqref{lib1} y #eqref{lib2} respectivamente, es en efecto una soluci#'on de la ecuaci#'on #eqref{ec-2p}. Sustituyendo a #eqref{sol-sep} en la ecuaci#'on #eqref{ec-2p} y desarrollando las derivadas obtenernos:
#begin{equation}
i #frac{ #partial #psi_{1} }{#partial t} #, #psi_{2} + i #frac{ #partial #psi_{2} }{#partial t} #, #psi_{1} = - #psi_{2} #left( #frac{1}{2m_{1}} #Delta_{1} #psi_{1} #right) - #psi_{1} #left( #frac{1}{2m_{2}} #Delta_{2} #psi_{2} #right)
#end{equation}
#begin{equation}
#psi_{2} #left[ i #frac{#partial #psi_{1}}{#partial t} + #frac{1}{2m_{1}} #Delta_{1} #psi_{1} #right] = - #psi_{1} #left[ i #frac{#partial #psi_{2}}{#partial t} + #frac{1}{2m_{2}} #Delta_{2} #psi_{2} #right]=0
#end{equation}
De esta forma hemos mostrado que $#psi (#x_{1} , #x_{2},t) = #psi_{1} (#x_{1},t) #cdot #psi_{2} (#x_{2},t)$ es soluci#'on de #eqref{ec-2p}, si y solo si, $#psi_{1}$ y $#psi_{2}$ son soluciones libres de las part#'iculas uno y dos respectivamente.##
Concluimos que la ecuaci#'on de Schr#"odinger para dos part#'iculas libres puede ser separada completamente en dos problemas independientes de una sola part#'icula. Esto no deber#'ia ser una sorpresa, ya que un sistema de dos part#'iculas no interactuantes consiste de subsistemas completamente independientes de una sola part#'icula. Mas aun, cada una de las part#'iculas se mueve como si la otra no estuviera ah#'i.##
Si $#psi$ y $#phi$ son soluciones de la ecuaci#'on de Schr#"odinger para dos part#'iculas, entonces cualquier combinaci#'on lineal $ a#psi + b #phi $, es de nuevo una soluci#'on (principio de superposici#'on y linealidad de la ecuaci#'on de Schr#"odinger). Debemos observar sin embargo que aunque las dos soluciones $#psi$ y $#phi$ pueden estar expresadas como el producto de funciones de onda de sistemas individuales (tal y como sucede con la soluci#'on #eqref{sol-sep}) en general, es imposible expresar a la combinaci#'on lineal $a#psi + b #phi$ de nuevo como un producto de funciones de onda de los sistemas individuales. Por tanto la ecuaci#'on de Schr#"odinger para dos part#'iculas tiene tantas soluciones, las cuales no son solamente el producto de funciones de onda de una sola part#'icula.

#subsubsection{El espacio de Hilbert de un sistema de dos part#'iculas}
Para un sistema de dos part#'iculas ?`Cual es la forma mas general de la funci#'on de onda a un cierto tiempo $t$?##
Supongamos que el conjunto $#{ #psi_{j} | j=1,2,... #}$ es una base ortonormal del espacio de Hilbert $L^{2}(#R^{n})$ de una sola de part#'iculas. Entre los posibles estados del sistema de dos part#'iculas est#'an todos  los productos de la forma:
#begin{equation}#label{pbas}
#psi_{j}(#x_{1}) #psi_{k}(#x_{2}) #,,#,#,#,#,#,#,#, j,k = 1,2,3,...
#end{equation}
y ya que el conjunto de todos los posibles estados es un espacio lineal (principio de superposici#'on), podemos formar combinaciones lineales arbitrarias de estos productos, mas aun, por la proposici#'on #ref{pt-b} y el teorema #ref{l2-ten}, sabemos que el conjunto de todos los productos de la forma #eqref{pbas} forma una base del espacio $L^{2}(#R^{2n})$. as#'i, la funci#'on mas general para un sistema de dos part#'iculas tiene la forma:
#begin{equation}
#psi (#x_{1} , #x_{2})= #sum_{j=1}^{#infty} #sum_{k=1}^{#infty} c_{jk} #psi_{j}(#x_{1}) #psi_{k}(#x_{2})
#end{equation}
Esta es una funci#'on de cuadrado integrable de las $2n$ variables $#{ #x_{1} , #x_{2} #}$, siempre que la doble suma converja con respecto a la norma de $L^{2}(#R^{2n})$. Para que esto se cumpla solo es necesario que la suma del modulo cuadrado de los coeficientes $c_{jk}$ sea finita:
#begin{equation}
#sum_{j=1}^{#infty} #sum_{k=1}^{#infty} |c_{jk}|^{2} < #infty
#end{equation}
En este caso, la funci#'on de onda $#psi$ para el sistema de dos part#'iculas, es de cuadrado integrable sobre $#R^{2n}$, con norma:
#begin{equation}
#| #psi #|^{2} = #int_{#R^{2n}} |#psi( #x_{1},#x_{2} )|^{2}#, d^{2n}x = #sum_{jk} |c_{jk}|^{2} < #infty
#end{equation}

En general, no es posible re-escribir a una combinaci#'on lineal de productos de funciones, como el simple producto de dos funciones de onda de las part#'iculas individuales. La discusi#'on anterior la resumimos a continuaci#'on.

#paragraph{El espacio de Hilbert de un sistema de dos part#'iculas} 
Si el espacio de estados de una solo part#'icula es $L^{2}(#R^{n})$, entonces el espacio de Hilbert de las funciones de onda para un sistema de dos part#'iculas es $L^{2}(#R^{2n})$. Si $#{ #psi_{k} | k=1,2,... #}$ es una base ortonormal de $L^{2}(#R^{n})$, entonces el conjunto $ #{ #psi_{jk} | #psi_{jk}= #psi_{j}(#x_{1}) #psi_{k}(#x_{2})#,,#,#,#,#,#, j,k=1,2,... #} $ es una base ortonormal del espacio de Hilbert del sistema de dos part#'iculas.

#rightline{$#dag$}
#vspace{3 mm}

A cualquier tiempo $t$, la funci#'on de onda mas general es por tanto un combinaci#'on lineal (posiblemente infinita) de productos de funciones de onda de una sola part#'icula.

#subsubsection{Interpretaci#'on de la funci#'on de onda para un sistema de dos part#'iculas }
An#'alogamente a la interpretaci#'on de la funci#'on de onda de una sola part#'icula, decimos que:
#begin{equation}
#int_{B #subset #R^{2n}} |#psi (#x_{1})|^{2} #, d^{2n}x
#end{equation}
es probabilidad de encontrar la configuraci#'on $#x= ( #x_{1} , #x_{2} )$ en la regi#'on $B #subset #R^{2n}$. La elecci#'on natural de la regi#'on $B$ es la de un rect#'angulo $ B_{1} #times B_{2} $, con $B_{j} #subset #R^{n}$. La expresi#'on anterior da la probabilidad de encontrar a la part#'icula uno en la regi#'on $B_{1}$ y a la part#'icula dos en la regi#'on $B_{2}$. En el caso especial de que la funci#'on de onda sea un producto, esta probabilidad tambi#'en se convierte en un producto:
#begin{equation}
#int_{B #subset #R^{2n}} |#psi( #x_{1},#x_{2} ) |^{2}#, d^{2n}x = #int_{B_{1}} | #psi_{1}(#x) |^{2} #, d^{n}x #, #int_{B_{2}} | #psi_{2}(#x) |^{2} #, d^{n}x
#end{equation}
Esta es la probabilidad de la uni#'on de los eventos $E_{1}=$ ``La part#'icula 1 esta en $B_{1}$'' y $E_{2}=$ ``La part#'icula 2 esta en $B_{2}$''. Si la funci#'on de onda no es un producto, la probabilidad de la uni#'on de estos eventos no se factorizar#'a, lo cual significa que los eventos $E_{1}$ y $E_{2}$ no son independientes.##
A la cantidad,
#begin{equation}
#rho_{1}(#x)= #int_{#R^{n}} |#psi (#x,#y)|^{2} #, d#y
#end{equation}
la conoceremos como la funci#'on de densidad de la part#'icula uno. Interpretamos a $#rho_{1}(#x)$ como la densidad de probabilidad de encontrar a la part#'icula uno en la posici#'on $#x$ mientras que la part#'icula dos se encuentra en cualquier lugar del espacio. as#'i, $#int_{B_{1}} #rho_{1}(#x)#, d#x $ es la probabilidad de que la posici#'on de la part#'icula uno este en la regi#'on $B_{1}$ independientemente de la posici#'on de la part#'icula 2. De forma similar definimos,
#begin{equation}
#rho_{2}(#x)= #int_{#R^{n}} |#psi ( #y , #x ) |^{2}#, d#y
#end{equation}
como la densidad de probabilidad de posici#'on de la part#'icula dos, independientemente de la posici#'on de la part#'icula uno. Cuando $#psi$ sea el producto de funciones de onda normalizadas de una sola part#'icula, las cantidades $#rho_{1}$ y $#rho_{2}$ son solamente las correspondientes densidades de una sola part#'icula:
#begin{equation}
#rho_{1}(#x)=|#psi_{1}(#x)|^{2} #,,#,#,#,#,#, #rho_{2}(#x)=|#psi_{2}(#x)|^{2} #,,#,#,#,#, #hbox{ si } #psi(#x , #y )= #psi_{1}(#x) #psi_{2}(#y)  
#end{equation}

#section{El espacio de Hilbert de un sistema binario}
Consideremos de forma general a un sistema cu#'antico el cual esta compuesto de dos partes diferentes $A$ y $B$ (esto es llamado un sistema binario). Ahora ?`Como construimos el formalismo matem#'atico del sistema compuesto sabiendo como describir las partes que lo forman? Este era precisamente el problema que ten#'iamos que resolver cuando introdujimos la funci#'on de oda de para un sistema formado por dos part#'iculas libres. Resolvimos este problema introduciendo un espacio de Hilbert mas grande, el cual contiene los productos de las funciones de onda de una sola part#'icula.  Estas mismas ideas se pueden utilizar para sistemas cu#'anticos en general.

#subsubsection{Construcci#'on del espacio de Hilbert de un sistema binario}
Consideramos dos sistemas f#'isicos $A$ y $B$. Denotamos a los espacios de Hilbert de los sistemas individuales como $#H^{A}$ y $#H^{B}$, respectivamente. Cuando unimos las dos partes, tenemos que encontrar un espacio de Hilbert mas grande en cual sea capas de describir los estados del sistema compuesto. La forma de la funci#'on de onda #eqref{pbas} para un sistema de dos part#'iculas sugiere que los estados del sistema compuesto deben de ser el producto de estados individuales. Matem#'aticamente hablando, el producto tensorial provee un buen m#'etodo para construir un espacio de Hilbert que contenga a todos los productos posibles entre elementos de los dos espacios $#H^{A}$ y $#H^{B}$:
$$ #psi^{A} #pr #psi^{B} #,;#,#,#,#,#,#,#, #psi^{A} #in #H^{A} #,,#, #psi^{B} #in #H^{B} $$
Al espacio de Hilbert del sistema compuesto lo construiremos a partir de elementos de esta forma ya que al estar trabajando con espacios de Hilbert en general, la multiplicaci#'on de dos funciones de onda $#psi^{A} #psi^{B}$, puede no tener ning#'un sentido, ya que los espacios de Hilbert a los que pertenecen cada una de estas, pueden ser muy diferentes entre si. Si embargo, este no es el caso de los espacios $L^{2}(#R^{n})$ tal y como lo hemos visto y manejado anteriormente (teorema #ref{l2-ten}). Debemos hacer notar que en general, el producto tensorial $ #psi_{A} #otimes #psi_{B} $ solo representa una pareja ordenada de estados $#psi_{A}$ y $#psi_{B}$.##
Los estados producto de la forma $ #psi_{A} #otimes #psi_{B} $ no pueden ser los #'unicos estados posibles del sistema compuesto. De acuerdo con el principio de superposici#'on, tambi#'en debemos incluir a todas las combinaciones lineales de este tipo de productos en el espacio de estados del sistema compuesto $AB$, mas aun tambi#'en debemos de considerar a los elementos de la cerradura de este ultimo conjunto. Por tanto podemos concluir, que el espacio de Hilbert correspondiente a un sistema binario formado por los subsistemas $A$ y $B$, es el producto tensorial de los espacios de Hilbert $#H^{A}$ y $#H^{B}$, al cual denotamos por:
$$ #H^{AB}= #H^{A} #otimes #H^{B} $$

La introducci#'on del producto tensorial para la descripci#'on de sistemas compuestos es un ingrediente b#'asico nuevo en la teor#'ia de la mec#'anica cu#'antica.

#paragraph{Suposici#'on b#'asica acerca de los sistemas compuestos:}
El espacio de Hilbert de un sistema cu#'antico formado por dos subsistemas $A$ y $B$, es el producto tensorial de los espacios de Hilbert $#H^{A}$ y $#H^{B}$ de cada uno de estos subsistemas.##
#rightline{$#dag$}
%#vspace{3 mm}

#subsubsection{Estados entrelazados}
Siempre que el estado de un sistema binario sea un estado producto, esto es, un estado que puede ser escrito como:
#begin{equation}#label{sep-estt}
#psi= #psi^{A} #otimes #psi^{B} #,;#,#,#,#,#,#,#, #psi^{A} #in #H^{A} #,,#, #psi^{B} #in #H^{B}
#end{equation}
podemos decir que el sistema $A$ se encuentra en el estado $#psi^{A}$ y que el sistema $B$ se encuentra en el estado $#psi^{B}$. El estado $#psi^{A} #otimes #psi^{B}$, del sistema compuesto, se obtiene si los dos subsistemas son preparados por separado en los estados $#psi^{A}$ y $#psi^{B}$ respectivamente y luego son yuxtapuestos sin que estos interaccionen.## 
Pero en el espacio de Hilbert del sistema compuesto, muchos de los estados no pueden ser escritos como un simple producto tensorial. Por ejemplo, el producto tensorial,
$$ #psi = #frac{1}{#sqrt{2}} ( #psi_{0}^{A} #otimes #psi_{0}^{B} + #psi_{1}^{A} #otimes #psi_{1}^{B} ) $$
no es un estado producto, al menos si suponemos que los conjuntos $#{ #psi_{0}^{A} , #psi_{1}^{A} #}$, $#{ #psi_{0}^{B} , #psi_{1}^{B} #}$, son linealmente independientes. 

#paragraph{Estados entrelazados y separables:} 
Diremos que un sistema compuesto se encuentra en un {#bf estado entrelazado},
si su estado no puede ser escrito como un solo producto tensorial de los estados de los subsistemas que lo componen. A un estado de la forma #eqref{sep-estt} lo conoceremos como un estado {#bf no entrelazado} o {#bf separable}.

#rightline{$#dag$}
#vspace{3 mm}

Incluso si el estado inicial de un sistema compuesto es un estado separable, este en general, se puede transformar en un estado entrelazado durante su evoluci#'on temporal, si existe alguna interacci#'on o acoplamiento entre los subsistemas. Siempre que un sistema compuesto se encuentre en un estado entrelazado, no tiene ning#'un sentido el querer hablar del #emph{estado en el que se encuentra uno de sus subsistemas}, en el sentido que hemos utilizado hasta ahora (esto es, como algo que puede ser descrito por un solo vector del espacio de Hilbert del subsistema). Es por esta raz#'on que nos vemos forzados a generalizar nuestra noci#'on del estado de un sistema. 

#section{part#'iculas interactuantes}

#subsubsection{Interacci#'on entre dos part#'iculas}
Resulta ser que los estados entrelazados tienen algunas propiedades que a primera vista, resultan ser muy extra#~nas. Por tanto, uno debe preguntar si los estados entrelazados pueden existir realmente en la naturaleza. Para sistemas de part#'iculas no interactuantes, podemos evitar el hablar de estados entrelazados, siempre que escojamos las condiciones iniciales apropiadas, esto es, escogiendo un estado inicial separable del sistema compuesto, ya que en este caso al evolucionar el sistema, el estado de este va a permanecer siendo un estado separable. Pero, en el caso de las part#'iculas interactuantes, esto ya no es lo que sucede y el entrelazamiento de los subsistemas no puede ser evitado. ##
El Hamiltoniano para un sistema de dos part#'iculas interactuantes se obtiene de su an#'alogo cl#'asico (regla de substituci#'on). Este (en unidades en las que $#hbar=1$) tiene la forma:
#begin{equation}#label{Ham-2p-1}
H= - #frac{1}{2m_{1}} #Delta_{1} - #frac{1}{2m_{2}} #Delta_{2} + V(#x_{1} , #x_{2})
#end{equation}
en donde $#Delta^{(j)}$ es el Laplaciano actuando sobre las coordenadas de la part#'icula $j$. Consideremos como ejemplo al #'atomo de helio. En la aproximaci#'on de masa nuclear infinita, el n#'ucleo at#'omico es un centro puntual fijo de una fuerza Coulombiana (con carga $2e$). El #'atomo de helio tiene dos electrones que interact#'uan a trav#'es de la repulsi#'on Coulombiana. La energ#'ia potencial electrost#'atica de los dos electrones puede ser entonces descrita por la expresi#'on,
#begin{equation}
V(#x_{1},#x_{2})= - #frac{2 #gamma}{|#x_{1}|} - #frac{2 #gamma}{|#x_{2}|} + #frac{#gamma}{ |#x_{1} - #x_{2} |} 
#end{equation}
en donde $#gamma = e^{2}/4 #pi #epsilon_{0}$. Los dos primeros sumandos describen la energ#'ia potencial electrost#'atica de los electrones en el campo del n#'ucleo y el ultimo termino es la repulsi#'on Coulombiana entre los dos electrones. La presencia del ultimo termino hace que sea imposible obtener una soluci#'on de la ecuaci#'on de Schr#"odinger con la forma de un producto de funciones de onda de una sola part#'icula (esto es, en la forma de un estado separable). Incluso si el estado inicial es separable, la evoluci#'on temporal llevara al sistema a un estado entrelazado.  

#subsubsection{Separaci#'on del movimiento del centro de masa}
Un caso especial de #eqref{Ham-2p-1} es cuando tenemos un Hamiltoniano de la forma:
#begin{equation}#label{ham-pc}
H= - #frac{1}{2m_{1}} #Delta_{1} - #frac{1}{2m_{2}} #Delta_{2} + V(#x_{1} - #x_{2})
#end{equation}
Aqu#'i, la energ#'ia potencial depende #'unicamente de la posici#'on relativa de las dos part#'iculas. Un Hamiltoniano como este puede aparecer por ejemplo, en la descripci#'on del #'atomo de hidrogeno, en donde el prot#'on no es tratado como un centro de fuerza fijo, sino como una part#'icula cu#'antica. El #'atomo de hidrogeno consiste de un prot#'on con masa $m_{p}$ y de un electr#'on con masa $m_{e}$. Estas part#'iculas interact#'uan a trav#'es de una fuerza Coulombiana atractiva, que depende solamente de la distancia entre el prot#'on y el electr#'on. Por tanto el Hamiltoniano para el #'atomo de hidrogeno tiene la forma,
$$ H= - #frac{1}{2m_{1}} #Delta_{1} - #frac{1}{2m_{2}} #Delta_{2} - #frac{#gamma}{|#x_{1} - #x_{2} |} $$
De nuevo, en este caso la evoluci#'on temporal no preserva la separabilidad de las funciones de onda. Pero podemos encontrar otro conjunto de coordenadas, en las que la separaci#'on a un estado producto es posible. ##
Comenzado con las coordenadas cartesianas $ (#x_{1} , #x_{2}) $ del sistema de dos part#'iculas (podemos suponer que estas son las coordenadas del sistema de referencia del #emph{laboratorio} en el que estamos realizando las mediciones), introducimos a la {#bf coordenada del centro de masa} $#xx$ y a la coordenada relativa $#x$, dadas por:
#begin{equation}#label{cm-cr}
#xx = #frac{ m_{1}#x_{1} + m_{2}#x_{2} }{m_{1}+m_{2}} #,#,#,;#,#,#,#,#,#,#,#,#,#, #x= #x_{1} - #x_{2}
#end{equation}
Despu#'es de realizar la transformaci#'on de coordenadas $ (#x_{1} , #x_{2}) #rar ( #xx , #x ) $, el Hamiltoniano #eqref{ham-pc}, se descomponen en una parte que depende solamente de $#xx$ y en otra parte que depende solamente de $#x$,
#begin{equation}#label{Ham-2p}
H= -#frac{1}{2M} #Delta_{#xx} - #frac{1}{2 m} #Delta_{#x} + V(#x)
#end{equation}
en donde $M$, es la masa total y $m$ es la masa reducida:
#begin{equation}#label{Mt-mr}
M=m_{1}+m_{2} #,#,#,;#,#,#,#,#,#, m = #frac{m_{1}#, m_{2}}{ m_{1} + m_{2} }
#end{equation} 

Los s#'imbolos $#Delta_{#xx}$ y $#Delta_{#x}$ denotan al operador de Laplace respecto a las coordenadas $#xx$ y $#x$ respectivamente.## 

Entonces, hemos encontrado una nueva forma de descomponer al sistema de dos part#'iculas, en dos subsistemas. Uno de estos subsistemas describe el movimiento libre del centro de masa, el cual es dado por el Hamiltoniano,
#begin{equation}
H_{cm}= - #frac{1}{2M} #Delta_{#xx} 
#end{equation}
El otro subsistema describe el movimiento relativo. Este subsistema es descrito por el Hamiltoniano,
#begin{equation}
H_{rel}= - #frac{1}{2 m} #Delta_{#x} + V(#x)
#end{equation}
el cual es el mismo Hamiltoniano que se tendr#'ia para una sola part#'icula de masa $#mu$ con una energ#'ia potencial externa $V(#x)$. El Hamiltoniano del sistema de dos part#'iculas es entonces la suma,
#begin{equation}
H= H_{cm}  + H_{rel}
#end{equation}
Ya que los dos sumandos anteriores conmutan entre si, la evoluci#'on temporal del sistema compuesto esta dada por,
#begin{equation}
e^{-iHt}= e^{-iH_{cm}t} #, e^{-iH_{rel}t}
#end{equation}
De la expresi#'on anterior observamos que si el estado inicial del sistema compuesto es separable, entonces este permanece siendo de esta forma mientras el sistema evoluciona en el tiempo. Con la descomposici#'on anterior, podemos re-escribir a la ecuaci#'on de general de Schr#"odinger para un sistema de dos part#'iculas con un potencial de la forma $V(#x)$, como:
#begin{equation}
i#, #frac{#partial}{#partial t} #psi=(H_{cm}  + H_{rel}) #psi
#end{equation}

Por tanto, podemos resolver la ecuaci#'on de Schr#"odinger para dos part#'iculas, por medio de un producto de la forma:
#begin{equation}#label{sol-p2}
#psi(#xx,#x,t)= #Psi(#xx , t) #, #psi_{rel} (#x ,t)
#end{equation}
en donde $#Psi$ describe el movimiento libre del centro de masa y $#psi$ el movimiento relativo de las dos part#'iculas,

#begin{equation}#label{sch-cm}
i #frac{#partial}{#partial t} #Psi(#xx , t)= H_{cm}#Psi(#xx , t)
#end{equation}
#begin{equation}#label{sch-rel}
i #frac{#partial}{#partial t} #psi_{rel}(#x,t)= H_{rel} #psi_{rel}(#x ,t)
#end{equation}

La linealidad de la ecuaci#'on de Schr#"odinger implica que cualquier combinaci#'on lineal arbitraria de soluciones de la forma #eqref{sol-p2} es de nuevo una soluci#'on. De esta forma obtenemos una nueva factorizaci#'on del espacio de Hilbert del sistema binario $#H$, la cual esta dada por:
$$ #H=#H_{cm} #pr #H_{rel} $$
en donde $#H_{cm}$ y $#H_{rel}$ son los espacio de Hilbert en donde se encuentran las soluciones de las ecuaciones #eqref{sch-cm} y #eqref{sch-rel}, respectivamente.##

Es claro que en este caso, las energ#'ias de los estados estacionarios del sistema est#'an dadas por los valores propios del Hamiltoniano que describe el movimiento relativo de las part#'iculas.

#newpage

#chapter{Observables de un sistema binario}

#section{El producto tensorial de operadores}
Dado un operador lineal $S$ en un espacio de Hilbert $#H^{A}$ y otro operador lineal $T$ en el espacio $#H^{B}$, podemos definir un operador lineal en el espacio de Hilbert del sistema compuesto. A este nuevo operador lo conoceremos como el producto tensorial de los operadores $S$ y $T$ y lo denotaremos por $S #pr T$. Siempre que los operadores $S$ y $T$ correspondan a observables de los subsistemas (esto es, que ambos sean auto-adjuntos), el producto tensorial $S #pr T$, va a ser un observable del sistema compuesto (observable producto). Los valores medibles de $S #pr T$ son los productos de los valores propios correspondientes a $S$ y $T$. ##
Describiremos brevemente como es la construcci#'on del operador $S #pr T$. Consideramos el caso en el tanto $S$ y $T$ son operadores lineales acotados (por tanto continuos). ##
Primero, definimos como act#'ua el operador $S #pr T$, sobre estados separables:
#begin{equation}#label{pro-op}
(S #pr T) #psi^{A} #pr #psi^{B}= S#psi^{A} #pr T#psi^{B} 
#end{equation}
y por linealidad, extendemos esta definici#'on a todo el conjunto de superposiciones finitas de estados separables:
#begin{equation}#label{op-tsum}
(S #pr T)#sum_{j,k}^{m,n} c_{jk}#, #psi_{j}^{A} #pr #psi_{k}^{B} = #sum_{j,k}^{m,n} c_{jk}#, (S #pr T)#, #psi_{j}^{A} #pr #psi_{k}^{B}=
#sum_{j,k}^{m,n} c_{jk}#, S#psi_{j}^{A} #pr T#psi_{k}^{B}
#end{equation}

El resultado es un operador lineal acotado definido en un dominio denso, que puede ser extendido (por continuidad) a todo el espacio $#H^{A} #pr #H^{B}$. as#'i tambi#'en, tenemos que:
#begin{equation}
#| S #pr T #|= #| S #| #, #| T #|
#end{equation}

Un estado separable $#psi^{A} #pr #psi^{B}$, formado por vectores propios:
#begin{equation}
S#psi^{A}= s #psi^{A} #,,#,#,#,#,#,#,#, T #psi^{B}= t #psi^{B}
#end{equation}
va a ser de nuevo un vector propio del observable producto,
#begin{equation}
(S #pr T)#psi^{A} #pr #psi^{B}= st #, #psi^{A} #pr #psi^{B}
#end{equation}
en donde el valor propio es el producto de los valores propios individuales. Esto es una consecuencia inmediata de la definici#'on #eqref{pro-op}. Mas aun, todos los valores propios del producto tensorial de los operadores, se pueden escribir como productos de los valores propios de los factores. 

#paragraph{Valores propios del producto tensorial de operadores:}
Supongamos que $S$ y $T$ son operadores con un espectro discreto de valores propios. Entonces, los valores propios del operador $S #pr T$ est#'an dados por los productos $st$, en donde $s$ es un valor propio de $S$ y $t$ es un valor propio de $T$. 

#rightline{$#dag$}
#vspace{5 mm}

Si los observables con los que estamos trabajando, resultan ser adem#'as operadores acotados, las siguientes relaciones se cumplen [15]:

#begin{subequations}#label{opp-rel2}
#begin{align}
#alpha ( S #pr T )= (#alpha S) #pr T = S #pr (#alpha T) #,,#,#, #alpha #in #C. ##
( S_{1} #pr T_{1} ) ( S_{2} #pr T_{2} )= S_{1} S_{2} #pr T_{1} T_{2}. ##
( S #pr T )^{*} = S^{*} #pr T^{*}.
#end{align}
#end{subequations}


Para operadores no acotados (como los de posici#'on y momento) se debe de tomar en cuenta las cuestiones del dominio de estos operadores a la hora de hacer su producto tensorial. La ecuaci#'on #eqref{pro-op} solo tiene sentido para $#psi^{A} #in D(S)$ y $ #psi^{B} #in D(T) $. Usualmente los dominios $D(S)$ y $D(T)$ son conjuntos densos y por tanto el conjunto de combinaciones lineales finitas de estados separables $#psi^{A} #pr #psi^{B}$, es denso en $#H^{A} #pr #H^{B}$. as#'i la ecuaci#'on #eqref{op-tsum} nos da la definici#'on de un operador lineal en un conjunto denso del espacio $#H^{A} #pr #H^{B}$. De ser posible, se puede tomar la cerradura de este dominio denso para completar la definici#'on del producto tensorial $S #pr T$. De esta forma, el producto tensorial de operadores autoadjuntos es de nuevo un operador auto adjunto en el espacio de Hilbert del sistema compuesto. De acuerdo con un teorema general del an#'alisis funcional (mas all#'a del alcance de este texto, ver [9] y [10]) el espectro del producto tensorial de operadores, es el producto de los respectivos espectros de los factores.##

Existen por supuesto, observables del sistema compuesto que no pueden ser expresados como el producto tensorial de observables de los subsistemas individuales. Como ejemplo, tenemos a la energ#'ia potencial $V(#x_{1},#x_{2})$, de la ecuaci#'on #eqref{Ham-2p}, la cual describe la interacci#'on en un sistema de dos part#'iculas, excepto si este tuviera la forma, $V(#x_{1},#x_{2})=V_{1}(#x_{1})#, V_{2}(#x_{2}) $. ##
Otro ejemplo interesante es el de la {#bf suma de Kronecker}, la cual combina dos operadores $S$ y $T$ de los subsistemas respectivamente y los combina en el operador $ S #pr #1 + #1 #pr T $. En particular, generador del producto tensorial grupos unitarios es la suma de Kronecker de los generadores:
#begin{equation}
e^{-itS} #pr e^{-itT} = e^{ -it( S #pr #1 + #1 #pr T ) }
#end{equation}
Esto se puede ver de la siguiente forma. Si $U(t)$ es un producto tensorial de grupos unitarios, 
$$ U(t)= V(t) #pr W(t)  = e^{-itS} #pr e^{-itT}$$
entonces el generador de $U(t)$ se obtiene como es usual, derivando a este operador en $t=0$. La regla de la derivada de un producto tambi#'en es valida para los productos tensoriales:
$$ i#, #frac{#partial}{#partial t} U(t) = #left( #frac{#partial}{#partial t} V(t) #right) #pr W(t) + V(t) #pr #left( #frac{#partial}{#partial t}W(t) #right) $$
y encontramos que:
$$ i #frac{#partial}{#partial t} U(t) #Big|_{t=0} = S #pr #1 + #1 #pr T $$

#subsubsection{Manipulaciones locales}
En un sistema compuesto $AB$, algunas veces uno puede estar interesado en medir un observable que esta definido solamente para uno de los subsistemas. Una medici#'on que se realiza en el subsistema $A$, sin perturbar al subsistema $B$, es llamada una {#bf medici#'on local} del subsistema $A$. Esto solamente es posible si los dos subsistemas pueden ser aislados uno del otro, lo cual es ciertamente mas f#'acil, como es el caso algunas veces, si estos se encuentran separados espacialmente desde un inicio.##

En el espacio de Hilbert de un sistema compuesto ($#H^{A} #pr #H^{B}$), un observable $S$ del subsistema $A$ es representado por el operador,
#begin{equation}#label{op-loc}
S #pr #1
#end{equation}
El donde $#1$, denota al operador identidad del espacio de Hilbert $#H^{B}$ del subsistema $B$. Un operador de la forma #eqref{op-loc} solo act#'ua de una forma diferente al operador identidad, sobre los estados del subsistema $A$ (a menos claro que $S= #1$), tiene los mismos valores propios que el operador $S$ y es auto adjunto siempre que $S$ lo sea. Por supuesto, lo mismo se aplica a los observables de la forma $#1 #pr T$, del subsistema $B$.

#paragraph{Observables de los subsistemas:}
Los observables del tipo $S #pr #1$ o $ #1 #pr T $, son llamados {#bf observables locales} u {#bf observables de los subsistemas}. Observables locales, pertenecientes a diferentes subsistemas, conmutan entre si: 
$$ [ S #pr #1 , #1 #pr T ]=0 $$   

Si $S$ es un operador auto adjunto, el grupo unitario generado por $S #pr #1$, esta dado por:
#begin{equation}
e^{ -it (S #pr #1) }= e^{-itS} #pr #1
#end{equation}
y similarmente para el operador auto adjunto $#1 #pr T$.

#rightline{$#dag$}
#vspace{5 mm}

Una transformaci#'on unitaria de la forma $V #pr #1$, en donde $V$ es un operador unitario en el espacio de Hilbert del subsistema $A$, es conocida como una {#bf transformaci#'on del subsistema}. Una transformaci#'on de este tipo puede ser realizada aplicando el operador $V$ al subsistema $A$ (esto es, a los vectores del espacio de Hilbert $#H^{A}$), sin hacerle nada a los vectores del subsistema $B$. Entonces, el producto tensorial de transformaciones unitarias puede ser escrito como el producto de transformaciones de los subsistemas, 
#begin{equation}
V #pr W = (V #pr #1)(#1 #pr W)
#end{equation}
Las transformaciones unitarias de este tipo son conocidas como {#bf transformaciones unitarias locales}. F#'isicamente, podemos realizar una transformaci#'on unitaria local, cuando los subsistemas se encuentran aislados uno del otro. ##

Recordemos que una #emph{propiedad} $P$ de un subsistema $A$, es descrita por un {#bf operador de proyecci#'on ortogonal}. Un operador de este tipo es un operador auto adjunto acotado $P$, con la propiedad $P^{2}=P$. Sus #'unicos valores propios son $0$ y $1$. El rango del operador $P$, es el conjunto de estados del subsistema $A$, que tienen la propiedad que corresponde a este operador ($#hbox{Ran }P=$ espacio propio del valor propio $1$). Resulta ser que en el sistema compuesto $P #pr #1$ es un operador de proyecci#'on ortogonal, siempre que $P$ lo sea. Por tanto, podemos concluir que las propiedades de un subsistema, tambi#'en son propiedades del sistema compuesto. El observable $P #pr #1$ mide si el subsistema $A$ tiene la propiedad $A$, sin considerar al subsistema $B$.

#section{El operador de densidad}
Si un sistema binario se encuentra en el estado separable $ #psi^{A} #pr #psi^{B} $, entonces podemos decir que el subsistema $A$ se esta en el estado $#psi^{A}$. Pero, ?`Como caracterizamos al subsistema, si el sistema compuesto se encuentra en un estado entrelazado?

#subsubsection{?`Que significa el entrelazamiento para los subsistemas?}
Consideremos como ejemplo a un sistema cu#'antico formado por dos qubits. Un quibit es un sistema cu#'antico al cual le corresponde un espacio de Hilbert de dos dimensiones, es decir, un sistema para el cual solo existen dos estados posibles $#psi_{+}$ o $#psi_{-}$, los cuales son ortonormales entre ellos.##
Supongamos que este sistema de qubits se encuentra en el estado:
#begin{equation}
#psi_{e}^{+}= #frac{1}{#sqrt{2}} ( #psi_{+} #pr #psi_{+} + #psi_{-} #pr #psi_{-} )
#end{equation}
?`Que es lo que sabemos acerca del estado de los qubits $A$ y $B$?, ?`Que informaci#'on acerca de los qbits individuales esta contenida en el estado entrelazado del sistema compuesto? ##
El estado $#psi_{e}^{+}$ es una superposici#'on de dos estados producto ortonormales, en el cual la probabilidad de encontrar a los dos qubits en el estado $#psi_{+}$ es $1/2$, la misma que la de encontrar a ambos en el estado $#psi_{-}$. Por tanto, el qubit $A$ se encuentra en el estado $#psi_{+}$ o $#psi_{-}$ con igual probabilidad $1/2$. Es importante entender que el estado del qubit $A$ no es una superposici#'on de los estados $#psi_{+}$ y $#psi_{-}$, si no que mas bien es una mezcla estad#'istica de estos.

#paragraph{Mezcla estad#'istica:} Una {#bf mezcla estad#'istica} de dos estados cu#'anticos $#psi_{1}$ y $#psi_{2}$, es un ensamble de sistemas en donde cada sistema individual es preparado con probabilidad $p$ en el estado $#psi_{1}$ y con probabilidad $1-p$ en el estado $#psi_{2}$. Aqu#'i, los estados $#psi_{1}$ y $#psi_{2}$ no necesitan ser ortogonales.

#rightline{$#dag$}
#vspace{5 mm}

#subsubsection{Valores esperados de observables de los subsistemas}
Con el objetivo de desarrollar una teor#'ia que pueda describir los estados de un subsistema, deseamos saber como extraer informaci#'on de estos directamente del estado de un sistema compuesto. Con este objetivo, consideramos al observable $S #pr #1$ del subsistema $A$. Por simplicidad, supongamos que $S$ es un operador acotado del espacio de Hilbert de $A$. Ahora, calculemos el valor esperado de este observable. Para esto necesitamos a las bases ortonormales $#{ #psi_{i}^{A} #}$ de $#H^{A}$ y $#psi_{j}^{B}$ de $#H^{B}$. Como sabemos, los productos $ #psi_{i}^{A} #pr #psi_{j}^{B} $ forman una base ortonormal del espacio $#H^{A} #pr #H^{B}$. Por tanto, el estado $#psi$ del sistema compuesto, puede ser escrito como una combinaci#'on lineal de estos estados producto: 
$$ #psi = #sum_{i,j} c_{ij}#, #psi_{i}^{A} #pr #psi_{j}^{B} $$
en donde suponemos que, $#| #psi #|^{2}= #sum_{i,j} |c_{ij}|^{2} =1 $. Entonces, el valor esperado de $S #pr #1$ en el estado $#psi$ esta dado por el siguiente calculo:
$$ ( #psi, (S #pr #1)#psi )= #left( #sum_{i,j} c_{ij}#, #psi_{i}^{A} #pr #psi_{j}^{B} , #sum_{k,l} c_{kl}#, S#psi_{k}^{A} #pr #psi_{l}^{B}  #right) $$
$$= #sum_{i,j,k,l} #ol{c_{ij}}#, c_{kl} ( #psi_{i}^{A}#pr #psi_{j}^{B}, S#psi_{k}^{A}#pr #psi_{l}^{B} ) $$
$$=  #sum_{i,j,k,l} #ol{c_{ij}}#, c_{kl} (#psi_{i}^{A},S#psi_{k}^{A}) #, (#psi_{j}^{B},#psi_{l}^{B} ) $$
$$  = #sum_{i,j,k,l} #ol{c_{ij}}#, c_{kl} (#psi_{i}^{A},S#psi_{k}^{A}) #, #delta_{jl}  $$
Luego, usando la definici#'on de la delta de Kronecker $#delta_{jl}$, tenemos que:
#begin{equation}#label{exp-cs}
( #psi, (S #pr #1)#psi )= #sum_{i,k,l} #ol{c_{il}}#, c_{kl}#, (#psi_{i}^{A},S#psi_{k}^{A})
#end{equation}
Esta expresi#'on del valor esperado involucra a los ``elementos de matriz'' de $S$ entre muchos estados diferentes del subsistema. Usualmente el valor esperado de un observable $S$ en el estado $#psi$ es solamente $( #psi , S #psi )$, pero la ecuaci#'on #eqref{exp-cs} muestra que el valor esperado del observable de un subsistema es algo mucho mas complicado. Esto sugiere que para un estado dado del sistema compuesto, el estado del subsistema no puede ser descrito solo por un simple vector del espacio de Hilbert de este subsistema. ##
A continuaci#'on, vamos a verificar que el resultado #eqref{exp-cs} se puede escribir de otra forma. Para esto necesitamos definir la traza de un operador.

#subsubsection{Operadores de traza finita}
#begin{definition}
Supongamos que para un operador lineal $S$, la serie 
#begin{equation}#label{tr-op}
#Tr (S) = #sum_{j} ( #psi_{j},S #psi_{j} )
#end{equation}
converge y tiene el mismo valor para toda base ortonormal $#{ #psi_{j} #}$ del espacio de Hilbert en donde esta definido. Entonces, al n#'umero $#Tr(S)$ lo conoceremos como la {#bf traza del operador} $S$ y diremos que este operador es de {#bf traza finita}.
#end{definition}

En la literatura mas importante del an#'alisis funcional a los operadores de traza finita se les conoce tambi#'en como {#bf operadores nucleares} y estos forman la clase de operadores $S_{1}$ la cual es un sub conjunto de la familia de operadores compactos definidos en un cierto espacio de Banach [10].##

El valor de la serie #eqref{tr-op} es autom#'aticamente independiente de la selecci#'on de la base ortonormal, siempre que el espacio de Hilbert sea de dimensi#'on finita o siempre que el operador $S$ sea un {#bf operador no negativo} (esto es, un operador para el cual $(#psi,S #psi) #geq 0$, $#forall #psi $). Sin embargo, esto no es cierto en el caso general por lo que necesitamos la suposici#'on de la definici#'on anterior. Mas aun, notamos que todo operador de traza finita es acotado, pero que no todo operador acotado es de traza finita. Para un operador de traza finita no negativo tenemos en particular que, $ #|S#| #leq #Tr (S) $.##

En la siguiente definici#'on introducimos una importante sub clase de los operadores de traza finita.

#begin{definition}#label{dens-op}
Diremos que un operador lineal acotado (por tanto definido en todo el espacio) $#rho$, es un {#bf operador de densidad} si posee las siguientes propiedades:
#begin{enumerate}
#item $#rho$ es no negativo: $ (#psi , #rho #psi) #geq 0 $, $#forall #psi #in #H$.
#item $#rho$ es de traza finita y $ #Tr (#rho)=1 $.
#end{enumerate}
#end{definition}

Enunciamos el siguiente resultado matem#'atico como la principal caracterizaci#'on de los operadores de traza finita, la demostraci#'on de este echo es propia del an#'alisis funcional para la cual se necesita de una teor#'ia matem#'atica mas elaborada la cual puede ser encontrada en [10].

#paragraph{Forma can#'onica de un operador de traza finita:} 
Un operador lineal auto adjunto $S$, en un espacio de Hilbert $#H$ es de traza finita, si y solo si, existe una base ortonormal $#{ #psi_{j} #}$ de $#H$, para la cual:
#begin{equation}#label{can-for}
S  = #sum_{j} #lambda_{j} #psi_{j} ( #psi_{j}, #cdot ) 
#end{equation}
con n#'umeros reales $#lambda_{j}$, tales que $ #sum_{j}|#lambda| < #infty $. De esta forma, la traza de $S$ esta dada por:
#begin{equation}
#Tr (S) = #sum_{j} #lambda_{j}
#end{equation}

Un operador con la forma #eqref{can-for} tiene como valores propios a los valores $#lambda_{j}$ y como correspondientes vectores propios a los elementos de la base $#psi_{j}$. Cada valor propio no nulo tiene a lo mas un grado finito de degeneraci#'on.

#rightline{$#dag$}
#vspace{5 mm}

En este punto debemos hacer notar que un operador de traza finita $S$ es un operador compacto, ya que claramente este es el limite uniforme de una sucesi#'on de operadores de rango finito, esto es:
$$ #| S_{k} - S #| #rar 0 $$
cuando $k #rar #infty$, en donde $S_{k}=#sum_{j=1}^{k} #lambda_{j} #psi_{j} ( #psi_{j} , #cdot )$.##

Si $#rho$ es un operador de densidad, entonces es f#'acil ver de la definici#'on #ref{dens-op}, que sus valores propios satisfacen:
#begin{equation}
#lambda_{j} #geq 0 #,,#,#,#,#,#, #sum_{j} #lambda_{j}=1
#end{equation}

#subsubsection{Operador de densidad de un subsistema}
Ahora, retomaremos la tarea de re-escribir la expresi#'on #eqref{exp-cs}. Dado un estado normalizado de un sistema cu#'antico compuesto:
#begin{equation}
#psi = #sum_{i,j} c_{ij} #, #psi_{i}^{A} #pr #psi_{j}^{B}
#end{equation}
(en donde $#{ #psi_{i}^{A} #}$ y $#{ #psi_{j}^{B} #}$, son bases ortonormales), definimos al operador:
#begin{equation}
#rho^{A}= #sum_{i,k,l} #ol{c_{il}}#, c_{kl}#, #psi_{k}^{A} ( #psi_{i}^{A} , #cdot ) 
#end{equation}
Este operador tiene las siguientes propiedades:
#begin{enumerate}
#item $#rho^{A}$ es acotado, auto adjunto y no negativo.
#item $#rho^{A}$ es de traza finita, con $#Tr (#rho^{A})=1$.
#end{enumerate}

Por tanto $#rho^{A}$ es un operador de densidad. Este solo tiene valores propios no negativos cuya suma es uno. Si $S$ es un operador acotado, entonces el operador $S#rho^{A}$ es de traza finita (este es un resultado del an#'alisis funcional mas all#'a del alcance de este texto, ver [10] ). Calculemos la traza de este operador:
$$ #Tr (S #rho^{A})= #sum_{j} ( #psi_{j}^{A},S #rho^{A} #psi_{j}^{A} )  $$
$$= #sum_{j,i,k,l} #ol{c_{il}}#,c_{kl}#, ( #psi_{j}^{A},S#psi_{k}^{A} )#, ( #psi_{i}^{A},#psi_{j}^{A} )  $$
$$ = #sum_{j,i,k,l} #ol{c_{il}}#,c_{kl}#, ( #psi_{j}^{A},S#psi_{k}^{A} ) #delta_{ij} $$
#begin{equation}
=#sum_{i,k,l} #ol{c_{il}}#, c_{kl}#, ( #psi_{i}^{A},S #psi_{k}^{A})
#end{equation}

Si comparamos este resultado con la ecuaci#'on #eqref{exp-cs}, vemos que $#Tr S #rho^{A}$, es solo otra forma de expresar el valor esperado del observable $S #pr #1$ en el estado $#psi$.

#paragraph{Valores esperados para un subsistema:} Sea $#psi$ un estado normalizado del sistema binario descrito por el espacio de Hilbert $#H^{A} #pr #H^{B}$. Sean $c_{ij}$ los coeficientes de la expansi#'on de $#psi$ respecto a una base ortonormal $#{ #psi_{i}^{A} #pr #psi_{j}^{B} #}$. Entonces el operador:
#begin{equation}#label{den-op}
#rho^{A}= #sum_{i,k,l} #ol{c_{il}} c_{kl}#, #psi_{k}^{A}#, ( #psi_{i}^{A}, #cdot )
#end{equation} 
es un operador de densidad. Si $S$ es un operador acotado definido en $#H^{A}$, entonces:
#begin{equation}#label{den-esp}
#la S #pr #1 #ra_{#psi}= ( #psi , (S #pr #1)#psi )= #Tr (S #rho^{A})
#end{equation}
Un resultado similar es cierto para los observables del sistema $B$.
#rightline{$#dag$}



#chapter{Estados puros y entrelazados}
Todo lo que puede ser conocido de un sistema cu#'antico esta contenido en los valores esperados de los observables del sistema. Para un subsistema podemos calcular estos valores esperados a trav#'es del operador de densidad seg#'un la ecuaci#'on #eqref{den-esp}. Por tanto, el operador de densidad describe el estado del subsistema.

#paragraph{El estado de un subsistema:} Si un estado binario se encuentre en el estado $#psi$, entonces el estado del subsistema $A$ esta caracterizado por el operador de densidad $#rho^{A}$ definido en #eqref{den-op}. El estado del subsistema $B$ esta descrito por un operador similar $#rho^{B}$ 

#rightline{$#dag$}
#vspace{3 mm}

Al operador de densidad $#rho^{A}$, tambi#'en lo conoceremos como la {#bf matriz de densidad} del subsistema $A$. Lo mismo haremos para el operador $#rho^{B}$ del subsistema $B$.## 

Anteriormente en este texto caracterizamos a los estados de un sistema como subespacios de una dimensi#'on de un espacio de Hilbert (ec. #eqref{est-1d}). Por conveniencia utilizamos al vector normalizado $#psi$ del subespacio $ [#psi]= #{c #psi #,|#, c #in #C #} $, para representar al estado del sistema. De forma equivalente podemos describir a $[#psi]$ mediante el operador $#rho = #psi ( #psi ,#cdot )$, el cual es el #'unico operador de proyecci#'on ortogonal con rango $[#psi]$. Mas aun, es f#'acil ver que $#rho$ es un operador de densidad. De esta forma los operadores de densidad son una generalizaci#'on de nuestro concepto anterior de estado.

#paragraph{Estados puros y entrelazados}
En general, el estado de un sistema cu#'antico es descrito por un operador de densidad $#rho$. A un operador de proyecci#'on de una dimensi#'on:
#begin{equation}
#rho= #psi (#psi, #cdot ) #,,#,#,#,#,#, #hbox{ con } #| #psi #|=1
#end{equation}
lo conoceremos como un {#bf estado puro} y es representado usualmente por el vector $#psi$. De otra forma $#rho$ es conocido como un {#bf estado entrelazado}.

#rightline{$#dag$}
#vspace{3 mm}

Supongamos que un sistema binario se encuentra en el estado separable $#psi = #psi^{A} #pr #psi^{B}$. tambi#'en supongamos que $#psi^{A}$ pertenece a una cierta base ortonormal. Usando esta base en la expresi#'on #eqref{den-op}, vemos inmediatamente que la matriz de densidad del subsistema describe un estado puro:
#begin{equation}
#rho^{A}= #psi^{A}( #psi^{A}, #cdot )
#end{equation}
De la misma forma, el valor esperado del observable $S #pr #1$ se reduce a la expresi#'on:
#begin{equation}
#Tr(S #rho^{A})= ( #psi^{A} , S #psi^{A} )
#end{equation}
Siempre que un sistema compuesto se encuentre en un estado separable, entonces los subsistemas que lo componen se encuentran en estados puros.

#section{Forma can#'onica del operador de densidad}

Para un operador de densidad dado $#rho$, podemos encontrar una base ortonormal $#{ #psi_{j} #}$, tal que $#rho$ se puede escribir en su forma can#'onica (ec. #eqref{can-for}): 
#begin{equation}#label{r-can}
#rho= #sum_{j} p_{j}#, #psi_{j}#, (#psi_{j}, #cdot)
#end{equation}
cuando $#rho$ se escribe en la forma anterior, diremos que este es un operador diagonal respecto a la base $#{ #psi_{j} #}$. Aqu#'i los n#'umeros $p_{j}$ son los valores propios de $#rho$ y los vectores $#psi_{j}$ son sus correspondientes vectores propios. Ya que $#rho$ es no negativo con $#Tr (#rho)=1$, los vectores propios satisfacen las condiciones:
#begin{equation}#label{p-con}
p_{j} #geq 0 #,#,#,#, (#forall j) #,,#,#,#,#,#,#, #sum_{j} p_{j}=1
#end{equation}
El operador de proyecci#'on $P_{k} = #psi_{k} ( #psi_{k}, #cdot )$, es un observable que describe la propiedad de ``estar en el estado $#psi_{k}$''. La probabilidad de que un sistema descrito por $#rho$, se encuentre en el estado $#psi_{k}$ esta dada, como es usual, por el valor esperado de $P_{k}$, esto es:
$$ #Tr (P_{k}#rho)= #sum_{l} ( #psi_{l},P_{k}#rho #psi_{l} ) $$  
#begin{equation}
= #sum_{l,j} p_{j}#, ( #psi_{l},#psi_{k} )#, ( #psi_{k},#psi_{j} )#, (#psi_{j},#psi_{l})=p_{k}
#end{equation}
por tanto, el valor propio $p_{k}$ de $#rho$, es la probabilidad de que un sistema en el estado entrelazado $#rho$ se encuentre en el estado puro $#psi_{k}$ (por estado puro nos referimos a que el sistema ya no esta en una mezcla estad#'istica de estados, ya que ahora conocemos un estado en concreto). Un {#bf estado puro} es un elemento del espacio de Hilbert del sistema, el cual es capaz de describir el estado en el que este sistema se encuentra. Las condiciones #eqref{p-con} muestran que la interpretaci#'on de los n#'umeros $p_{j}$ como probabilidades, es consistente.

#paragraph{Significado del operador de densidad:}
Un estado entrelazado $#rho$ de un sistema f#'isico, es un ensamble estad#'istico de estados puros $#{ #psi_{1},#psi_{2},... #}$, los cuales forman un conjunto ortonormal, en donde cada estado $#psi_{j}$ ocurre con una probabilidad $p_{j}$. Las probabilidades $p_{j}$ son los valores propios del operador de densidad $#rho$ y los estados puros $#psi_{j}$ son sus correspondientes vectores propios.
 
#rightline{$#dag$}
#vspace{5 mm}

Debemos mencionar que en el caso en el que uno de los valores propios de $#rho$ sea degenerado, entonces la forma can#'onica de $#rho$ #eqref{r-can}, no es #'unica.

#section{Forma normal del vector de estado}
Regresemos a nuestro estudio de los sistemas cu#'anticos compuestos. Usando la forma can#'onica #eqref{r-can} del operador de densidad $#rho^{A}$, podemos obtener una forma normal para el vector de estado de un sistema compuesto. Con esta nueva forma normal es f#'acil determinar cuando el estado de un sistema compuesto es entrelazado o puro y si el vector $#psi$ esta determinado de forma #'unica por los estados de los subsistemas o no.##
Escribamos al estado $#psi$ del sistema compuesto como una combinaci#'on lineal de estados producto. Escogemos en $#H^{A}$ una base ortonormal $#{ #phi_{j}^{A} #}$, respecto a la cual el operador de densidad $#rho^{A}$ es diagonal. Supongamos que $#{ #psi_{j}^{B} #}$ es una base ortonormal arbitraria del espacio $#H^{B}$. Entonces,
#begin{equation}#label{nor-f}
#psi = #sum_{ij} c_{ij} #phi_{i}^{A} #pr  #psi_{j}^{B} = #sum_{i} #phi_{i}^{A} #pr #left( #sum_{j} c_{ij}#psi_{j}^{B}  #right) = #sum_{i} #phi_{i}^{A} #pr #chi_{i}^{B}
#end{equation}
en donde,
#begin{equation}
#chi_{i}^{B} = #sum_{j} c_{ij}#, #psi_{j}^{B}
#end{equation} 
Ya que $#rho^{A}$ es diagonal en la base $#{ #phi_{j}^{A} #}$, tenemos por la ecuaci#'on #eqref{den-op}, que:
#begin{equation}
#rho^{A}= #sum_{j} p_{j} #phi_{j}^{A} ( #phi_{j}^{A}, #cdot ) = #sum_{j,k} #sum_{l} #ol{c_{jl}} #, c_{kl} #, #phi_{k}^{A}#, ( #phi_{j}^{A}, #cdot )
#end{equation}
y por tanto:
#begin{equation}
#sum_{l} #ol{c_{jl}} #, c_{kl} = p_{j}#, #delta_{jk}
#end{equation}
Esta expresi#'on no es mas que el producto escalar de los vectores $#chi_{j}^{B}$ y $#chi_{k}^{B}$, ya que:
$$ ( #chi_{j}^{B},#chi_{k}^{B} )= #sum_{lr} #ol{c_{jl}}#, c_{kr}#, (#psi_{l}^{B},#psi_{r}^{B}) = #sum_{lr} #ol{c_{jl}}#, c_{kr}#, #delta_{lr} = #sum_{l} #ol{c_{jl}}#, c_{kl} $$ 
Por tanto:
$$ ( #chi_{j}^{B},#chi_{k}^{B} )= p_{j}#, #delta_{jk} $$
as#'i, los vectores $#{ #chi_{j}^{B} #}$ forman un conjunto ortogonal en $#H^{B}$. Despu#'es de normalizar a estos vectores, obtenemos al conjunto ortonormal formado por los vectores:
#begin{equation}
#phi_{j}^{B}= #frac{1}{#sqrt{p_{j}}} #chi_{j}^{B}
#end{equation} 
Insertando esta expresi#'on en la ecuaci#'on #eqref{nor-f}, obtenemos la forma normal de $#psi$: 
#begin{equation}#label{psi-fn}
#psi= #sum_{i} #sqrt{p_{i}}#, #phi_{i}^{A} #pr #phi_{i}^{B}
#end{equation}
De aqu#'i podemos concluir lo siguiente: $#psi$ es separable, si y solo si, solamente uno de los sumandos es diferente de cero, de otra forma es un estado entrelazado. En el caso separable tenemos $p_{k}=1$, precisamente para un solo #'indice $k$ y $#rho^{A}= #phi_{k}^{A} ( #phi_{k}^{A}, #cdot ) $, es un estado puro. Como consecuencia tenemos el siguiente resultado:

#paragraph{Estados puros y entrelazados:} Un subsistema esta en un estado puro (es decir, no es descrito por un ensamble estad#'istico de estados), si y solo si, el estado del sistema compuesto es separable. El subsistema se encuentra en un ensamble estad#'istico de estados, si y solo si, el estado del sistema compuesto es entrelazado.

#rightline{$#dag$}
#vspace{5 mm}
     
Si el sistema compuesto es descrito por #eqref{psi-fn}, entonces el estado del sistema $B$ esta dado por:
#begin{equation}
#rho^{B}= #sum_{i} p_{i} #phi_{i}^{B}#, ( #phi_{i}^{B}, #cdot )
#end{equation}
Por tanto, para los dos sistemas $A$ y $B$, los valores propios no nulos de los operadores de densidad correspondientes son los mismos. A cada vector propio   con valor propio no nulo del operador $#rho^{A}$, le corresponde un vector propio del operador $#rho^{B}$ con el mismo valor propio. Solo el grado de degeneraci#'on del valor propio cero puede ser diferente entre los sistemas, ya que las dimensiones de los espacios de Hilbert $#H^{A}$ y $#H^{B}$ pueden ser diferentes. Mas aun, los sistemas $A$ y $B$ tienen el mismo tipo de estados, es decir, ambos son puros o ambos son entrelazados.     

#paragraph{Forma normal del vector de estado:} 
Para un estado dado $#psi #in #H^{A} #pr #H^{B}$ de un sistema compuesto, podemos encontrar bases ortonormales $ #{ #phi_{j}^{A} #}#subset #H^{A} $ y $ #{ #phi_{j}^{B} #} #subset #H^{B} $, tales que:
#begin{equation}#label{FNVe}
#psi= #sum_{i} #sqrt{p_{i}} #, #phi_{i}^{A} #pr #phi_{i}^{B} #,,#,#,#,#,#,#, p_{i} #geq 0 #,,#,#,#,#,#,#,#, #sum_{i} p_{i}=1
#end{equation}
A la expresi#'on #eqref{FNVe} se le conoce como la {#bf representaci#'on de Gram-Schmidt} de $#psi$. ##
Si el estado del sistema compuesto esta dado por #eqref{FNVe}, entonces los estados de los subsistemas est#'an descritos por los operadores de densidad:
#begin{equation}#label{rho-AB}
#rho^{A}= #sum_{i} p_{i} #phi_{i}^{A} #, ( #phi_{i}^{A},#cdot) #,#,;#,#,#,#,#, #rho^{B}= #sum_{i} p_{i} #phi_{i}^{B} #, ( #phi_{i}^{B},#cdot) 
#end{equation}
Por tanto, $#rho^{A}$ y $#rho^{B}$ tienen los mismos valores propios no nulos. $#rho^{A}$ es un estado puro, si y solo si, $#rho^{B}$ es tambi#'en un estado puro.

#rightline{$#dag$}
#vspace{5 mm}

#section{Pureza del estado de un sistema binario}
Tomemos al estado $#psi #in #H^{A} #pr #H^{B}$ de un sistema binario, escrito en su forma normal respecto a las bases ortonormales $ #{ #phi_{i}^{A} #} #subset #H^{A} $ y $ #{ #phi_{i}^{B} #} #subset #H^{B} $,
$$ #psi= #sum_{i} #sqrt{p_{i}} #, #phi_{i}^{A} #pr #phi_{i}^{B} $$
en donde $p_{i}#geq 0$ y $#sum_{i} p_{i}=1 $. El operador de densidad del sistema $A$ esta dado por: 
$$#rho_{A} = #sum_{i} p_{i}#, #phi_{i}^{A} ( #phi_{i}^{A}, #cdot ) $$
Ahora para $ #phi_{k}^{A} #in #{ #phi_{i}^{A} #} #subset #H^{A}$, tenemos que:
#begin{equation}
#rho_{A}(#phi_{k}^{A}) = #sum_{i} p_{i}#, #phi_{i}^{A} #, ( #phi_{i}^{A},#phi_{k}^{A} ) = #sum_{i} p_{i}#, #phi_{i}^{A} #, #delta_{ik} = p_{k}#, #phi_{k}^{A}
#end{equation}
Luego, ya que $#rho_{A}$ es lineal:
#begin{equation}
#rho_{A}^{2} (#phi_{k}^{A})= #rho_{A} #circ #rho_{A} (#phi_{k}^{A}) = p_{k}^{2}#, #phi_{k}^{A}
#end{equation}
De esta forma, podemos ahora calcular la traza $#Tr ( #rho_{A}^{2} )$, respecto a la base ortonormal $ #{ #phi_{i}^{A} #} #subset #H^{A} $:
#begin{equation}
#Tr ( #rho_{A}^{2} )= #sum_{j} ( #phi_{j}^{A} , #rho_{A}^{2} #phi_{j}^{A} )= #sum_{j} ( #phi_{j}^{A}, p_{j}^{2}#, #phi_{j}^{A} )= #sum_{j} p_{j}^{2} ( #phi_{j}^{A},#phi_{j}^{A} )= #sum_{j} p_{j}^{2} 
#end{equation}

De aqu#'i podemos separar dos casos, uno cuando $#rho_{A}$ es un estado puro ($p_{j} #neq 0$, solo para $j=1$) y otro cuando $#rho_{A}$ es un sistema entrelazado ($p_{j} #neq 0$, para mas de un #'indice $j$).##

Examinemos primero el caso en el que $#rho_{A}$ representa a un sistema entrelazado. Ya que $#psi$ esta escrito en su forma normal, tenemos por las condiciones #eqref{FNVe}, que para todo #'indice $j$, $0 < p_{j} < 1$; esto implica que, $ 0 < p_{j}^{2} < p_{j} <1 $, luego tenemos que:
#begin{equation}
0< #Tr ( #rho_{A}^{2} )= #sum_{j} p_{j}^{2} < #sum_{j} p_{j}= 1
#end{equation}
Por tanto, podemos concluir que para un sistema en un estado entrelazado:
#begin{equation}
#Tr ( #rho_{A}^{2} ) < 1
#end{equation}

En caso de que $#rho_{A}$ represente a un sistema en un estado puro, tenemos que $ #rho_{j} #neq 0  $, solo para un #'indice $j$ y mas aun, para este #'indice, $p_{j}=1$. Luego es claro que:
#begin{equation}
#Tr (#rho_{A}^{2}) =1
#end{equation}

Por #eqref{rho-AB} tenemos que, $#Tr (#rho_{A}^{2}) = #Tr (#rho_{B}^{2})$ y por tanto esta cantidad solo depende del estado del sistema binario $#psi$. A esta cantidad la definimos como la #emph{pureza} del estado $#psi$.

#paragraph{Pureza del estado de un sistema binario:}
La pureza del estado $#psi #in #H^{A} #pr #H^{B}$ de un sistema binario, esta dada por:
#begin{equation}#label{pur-ps}
P(#psi)= #Tr(#rho_{A}^{2})= #Tr(#rho_{B}^{2})
#end{equation}
en donde, $0 #leq P(#psi) #leq 1$. $P(#psi) < 1$, si y solo si, $#psi$ es un estado entrelazado y $ P(#psi)=1 $, si y solo si, $#psi$ es un estado separable. De esta forma, la pureza $P(#psi)$ es una medida del entrelazamiento que existe en un sistema binario cuando este se encuentra en el estado $#psi$.

#rightline{$#dag$}
#vspace{5 mm}

En caso de que tengamos $#H^{A}= #H^{B}= L^{2}(#R^{n})$, existe una forma mas directa de calcular la pureza del estado $#psi$, sin tener que buscar los vectores propios del operador de densidad de alguno de los subsistemas. Como en lo anterior, supongamos que $#{ #phi_{i} #} #subset L^{2}(#R^{n})$ es una base ortonormal de $L^{2}(#R^{n})$, respecto a la cual el estado del sistema binario $ #psi #in L^{2}(#R^{n}) #pr L^{2}(#R^{n}) $ se puede escribir en su forma normal,
$$ #psi(#x_{1},#x_{2}) = #sum_{i} #sqrt{p_{i}} #, #phi_{i}^{A}(#x_{1}) #pr #phi_{i}^{B}(#x_{2}) = #sum_{i} #sqrt{p_{i}} #, #phi_{i}^{A}(#x_{1}) #, #phi_{i}^{B}(#x_{2}) $$
en donde $#x_{i} #in #R^{n}$, ($i=1,2$). Ahora calculemos la siguiente integral:
$$ #int_{#R^{4n}} #psi(#x_{1},#x_{2}) #, #psi(#x_{1}',#x_{2}') #, #ol{#psi(#x_{1}',#x_{2})} #, #ol{#psi(#x_{1},#x_{2}')}#, d#x_{1}#,#x_{2}#,#x_{1}' #,#x_{2}' $$

$$= #int_{#R^{4n}} #sum_{i} #sqrt{p_{i}}#, #phi_{i}(#x_{1})#, #phi_{i}(#x_{2}) #cdot #sum_{j} #sqrt{p_{j}}#, #phi_{j}(#x_{1}')#, #phi_{j}(#x_{2}') #cdot #sum_{k} #sqrt{p_{k}}#, #ol{#phi_{k}(#x_{1}')}#, #ol{#phi_{k}(#x_{2})} #cdot #sum_{l} #sqrt{p_{l}}#, #ol{#phi_{l}(#x_{1})}#, #ol{#phi_{l}(#x_{2}')}  $$

$$= #sum_{i,j,k,l} #int_{#R^{4n}} #sqrt{p_{i}#,p_{j}#,p_{k}#,p_{l}}#, #left( #phi_{i}(#x_{1}) #cdot #ol{#phi_{l}(#x_{1})} #right) #left(#phi_{i}(#x_{2}) #cdot #ol{#phi_{k}(#x_{2})} #right) #left(#phi_{j}(#x_{1}') #cdot #ol{#phi_{k}(#x_{1}')} #right)
#left(#phi_{j}(#x_{2}') #cdot #ol{#phi_{l}(#x_{2}')} #right) $$

$$ = #sum_{i,j,k,l} #sqrt{p_{i}#,p_{j}#,p_{k}#,p_{l}}#, #delta_{i,l} #, #delta_{i,k} #, #delta_{j,k}#, #delta_{j,l}= #sum_{i} #sqrt{p_{i}^{4}} = #sum_{i} p_{i}^{2} $$
Por tanto, tenemos que:
#begin{equation}#label{pur-i}
#int_{#R^{4n}} #psi(#x_{1},#x_{2}) #, #psi(#x_{1}',#x_{2}') #, #ol{#psi(#x_{1}',#x_{2})} #, #ol{#psi(#x_{1},#x_{2}')}#, d#x_{1}#,#x_{2}#,#x_{1}' #,#x_{2}' = #sum_{i} p_{i}^{2} = P(#psi)
#end{equation}
Mas aun, observamos que la integral de la expresi#'on anterior, se puede ver como el producto escalar de dos funciones de $L^{2}(#R^{4n})$ (a saber, las funciones $#psi(#x_{1},#x_{2}) #, #psi(#x_{1}',#x_{2}') #in L^{2}(#R^{4n}) $ y $#psi(#x_{1}',#x_{2}) #, #psi(#x_{1},#x_{2}') #in L^{2}(#R^{4n})$), de esta forma por el teorema de Fourier-Plancherel (#ref{FPt}), tenemos que la expresi#'on para la pureza (#ref{pur-i}) tambi#'en es valida para la transformada de Fourier de $#psi$, $#F (#psi) = #hat{#psi}$ (ec. #eqref{TF-nD}), lo cual es claramente #'util cuando estamos trabajando en el espacio de momentos. as#'i, tenemos que la pureza para una funci#'on de onda $#hat{#psi} #in L^{2}(#R^{n}) #pr L^{2}(#R^{n})$ en el espacio de momentos, esta dada por:
#begin{equation}#label{pur-im}
#int_{#R^{4n}} #hat{#psi}(#P_{1},#P_{2}) #, #hat{#psi}(#P_{1}',#P_{2}') #, #ol{#hat{#psi}(#P_{1}',#P_{2})} #, #ol{#hat{#psi}(#P_{1},#P_{2}')}#, d#P_{1}#,#P_{2}#,#P_{1}' #,#P_{2}' = #sum_{i} p_{i}^{2} = P (#hat{#psi})= P(#psi)
#end{equation}

Es en este punto en donde debemos hacer notar que toda la teor#'ia de sistemas compuestos que hemos desarrollado ha sido en espacios de Hilbert en general, no importando si las funciones de onda se encuentran en el espacio de posiciones o de momentos; por tanto esta es igualmente valida en ambas representaciones. En caso de que tengamos $#H^{A}= #H^{B}= L^{2}(#R^{n})$, la linealidad y la propiedad unitaria de la transformada de Fourier (ec. #eqref{TF-nD}), nos aseguran que un estado es separable en el espacio de posiciones, si y solo si, es separable en el espacio de momentos; as#'i tambi#'en que un estado es entrelazado en el espacio de posiciones, si y solo si, es entrelazado en el espacio de momentos. Estos dos #'ultimos hechos tambi#'en se pueden concluir f#'acilmente de las ecuaciones #eqref{pur-i} y #eqref{pur-im}.##
#newpage

#chapter{Teor#'ia de dispersi#'on}
Estamos familiarizados con la teor#'ia de dispersi#'on cl#'asica, en la cual se estudian los procesos de dispersi#'on entre varias part#'iculas, part#'iculas y blancos,  entre ondas y obst#'aculos materiales, e.c.t. 

En nuestro caso, estamos interesados en la parte de esta teor#'ia que estudia la dispersi#'on entre part#'iculas, a trav#'es de la interacci#'on que existe entre ellas. Sobre todo nuestro caso de estudio es aquel cuando dispersi#'on o colisi#'on entre estas part#'iculas se da a bajas energ#'ias, en procesos de dispersi#'on el#'asticos. En un proceso de dispersi#'on el#'astico, la energ#'ia y el momento total del sistema se conservan durante todo este proceso, mas aun, las part#'iculas no sufren ning#'un cambio en su estructura interna. En general, la teor#'ia de dispersi#'on ha demostrado ser de vital importancia en la f#'isica, ya que por ejemplo casi todo lo que sabemos acerca de f#'isica nuclear y at#'omica actualmente a podido ser descubierto a trav#'es de experimentos de dispersi#'on (El descubrimiento del n#'ucleo at#'omico por parte de Rutherford, el descubrimiento de las part#'iculas subat#'omicas, e.c.t.), es por esto que vale la pena abordar este desarrollo te#'orico mas all#'a del punto de vista de la f#'isica cl#'asica y desarrollarlo seg#'un el formalismo de la mec#'anica cu#'antica. Sin embargo, como hemos visto anteriormente, en la teor#'ia cu#'antica no tiene ning#'un sentido el hablar de part#'iculas bien localizadas, tal como sucede en el desarrollo de la teor#'ia de dispersi#'on al surgir directamente de conceptos de la f#'isica cl#'asica. Es por esto que primero debemos examinar los aspectos f#'isicos mas generales de la teor#'ia de dispersi#'on, para despu#'es poder formularla en t#'erminos de conceptos propios de la mec#'anica cu#'antica.

#section{Caracter#'isticas f#'isicas de los sistemas de dispersi#'on}


Como hemos mencionado anteriormente, en nuestro caso particular, estamos interesados en los procesos de dispersi#'on entre part#'iculas a trav#'es de la interacci#'on que existe entre estas. Es por esta raz#'on que al referirnos a un sistema dispersor debemos tener en mente a un conjunto de part#'iculas que son capaces de interactuar entre ellas de alguna forma.##

En los procesos de dispersi#'on en general, podemos distinguir tres etapas de la evoluci#'on temporal de los sistemas que pasan por este proceso. En la primera etapa, el estado del sistema esta preparado en el #emph{pasado remoto}. En esta etapa suponemos que las part#'iculas del sistema est#'an tan alejadas unas de otras, que la interacci#'on entre estas es despreciable y por tanto no tiene ning#'un efecto en la evoluci#'on temporal de ninguna de estas. Esta suposici#'on siempre es hecha en la interpretaci#'on de un experimento de dispersi#'on t#'ipico, donde las condiciones iniciales son valores de un conjunto de cantidades f#'isicas, las cuales caracterizan a un conjunto de part#'iculas libres (por ejemplo, momentos, spin y masas).##

Durante la segunda etapa, las part#'iculas interact#'uan entre ellas y la evoluci#'on del sistema es gobernada por una ecuaci#'on de movimiento para la cual el termino de interacci#'on ya no es despreciable y juega un papel crucial en el comportamiento del sistema. Es de hecho este termino de interacci#'on el cual produce el fen#'omeno de dispersi#'on de las part#'iculas.##

En la tercera etapa nos encontramos con la misma situaci#'on que en la primera. Aqui suponemos que despu#'es de que ha ocurrido el proceso de dispersi#'on, las part#'iculas se separan tanto unas de otras, que de nuevo el factor de interacci#'on entre estas es despreciable y por tanto no tiene ning#'un efecto en la evoluci#'on del estado del sistema completo. En el futuro distante, un detector puede observar este nuevo estado de las part#'iculas, el cual ha sido creado por el proceso de dispersi#'on y medir todas las propiedades que sean de interes para el experimenrto.##

El requisito de que los estados que describen a los eventos de dispersi#'on, deban ser caracterizables a tiempos muy grandes, tanto en el pasado remoto como en el futuro distante, por estados propios de los sistemas formados por part#'iculas libres, es conocida como la {#bf condici#'on asint#'otica}. Esta condici#'on se puede expresar en t#'erminos del formalismo de la mec#'anica cu#'antica de forma matem#'atica, ya que conocemos la descripci#'on de la evoluci#'on temporal de sistemas cu#'anticos (en particular de los sistemas formados por part#'iculas libres). Adem#'as conocemos la topolog#'ia de los espacios de Hilbert, a los cuales pertenecen las funciones de onda que describen a los sistemas cu#'anticos, haci#'endonos capaces de describir de esta manera las diferencias entre los estados del sistema, tanto entre la primera y la segunda etapa del proceso de dispersi#'on, como entre la segunda y tercera etapa de este proceso.


#newpage

#chapter{Teor#'ia cu#'antica de la dispersi#'on}
%#chapter{Teor#'ia cu#'antica de la dispersi#'on dependiente del tiempo}

Es este capitulo discutiremos la #emph{teor#'ia cu#'antica de la dispersi#'on dependiente del tiempo}. Comenzaremos con la formulaci#'on matem#'atica de la condici#'on asint#'otica, introducida en el capitulo anterior, pero esta vez en el formalismo de la mec#'anica cu#'antica, para procesos de dispersi#'on el#'asticos, en donde los constituyentes del sistema no sufren ning#'un cambio en su estructura f#'isica durante el proceso de dispersi#'on. As#'i tambi#'en, deduciremos algunas consecuencias notables de esta condici#'on, en particular algunas del operador de dispersi#'on. 

#section{La condici#'on asint#'otica}
Consideremos el caso posible mas simple, a saber, aquel en el que un sistema cuyos constituyentes van a pasar por un proceso de dispersi#'on, puede ser descrito completamente por dos grupos unitarios de un par#'ametro $U_{t}$ y $V_{t}$, los cuales act#'uan en el espacio de Hilbert $#H$, formado por todos los estados (puros) del sistema f#'isico bajo estudio. Al grupo unitario $U_{t}$, lo interpretaremos como aquel que describe la evoluci#'on temporal de los estados del sistema en la ausencia de perturbaciones o de cualquier interacci#'on entre sus constituyentes. Por esta raz#'on al grupo $U_{t}$ lo conoceremos como el {#bf grupo de evoluci#'on no perturbado}. Supondremos que el grupo $V_{t}$ describe la evoluci#'on temporal del sistema, tomando en cuenta las interacciones entre los constituyentes de este y las perturbaciones que puedan existir. Al grupo $V_{t}$ lo conoceremos como el {#bf grupo total de evoluci#'on}.##

Desarrollaremos matem#'aticamente a la condici#'on asint#'otica en t#'erminos de estos grupos unitarios. Por supuesto, estos dos grupos est#'an relacionados entre ellos. La relaci#'on entre $U_{t}$ y $V_{t}$ usualmente se da en t#'erminos de sus respectivos generadores infinitesimales, los cuales por el teorema de Stone, sabemos que son un par de operadores autoadjuntos determinados de forma #'unica por cada grupo. Denotaremos por $H_{0}$ y $H$ a los generadores infinitesimales de los grupos unitarios $U_{t}$ y $V_{t}$ respectivamente, en donde la relaci#'on entre estos esta dada por:
#begin{equation}
U_{t}= e^{-i H_{0}t } #,#,#,;#,#, V_{t}= e^{-i H t}
#end{equation}

A $H_{0}$ lo interpretaremos como el operador de energ#'ia del sistema en la ausencia de interacciones dentro de este y lo conoceremos como el #emph{Hamiltoniano no perturbado}. Por otro lado, a $H$ lo conoceremos como el #emph{Hamiltoniano total del sistema} (ya que este toma en cuenta las interacciones dentro de este). $H$ es la suma del Hamiltoniano no perturbado $H_{0}$ y del Hamiltoniano de interacci#'on $V$, es decir, $H= H_{0} + V$. ##
Un ejemplo de lo anterior, el cual es conveniente tener en mente durante el siguiente desarrollo, es el de la dispersi#'on causada por un potencial $V(#x)$, a una part#'icula no relativista sin esp#'in en el espacio $#R^{3}$. Este caso en general es conocido como {#bf dispersi#'on potencial}. Aqu#'i el espacio de Hilbert es $L^{2}(#R^{3})$, el Hamiltoniano no perturbado es el mismo que el de una part#'icula libre en el espacio $#R^{3}$, $H_{0}= - #frac{1}{2} #Delta $, el cual esta asociado a la energ#'ia cin#'etica de la part#'icula y $V(#x)$ es un operador auto adjunto de multiplicaci#'on por un n#'umero real con dominio denso en $#H$. Es muy com#'un que cuando de habla de #emph{dispersi#'on potencial}, se usen los t#'erminos {#bf Hamiltoniano libre}, para $H_{0}$ y {#bf grupo de evoluci#'on libre}, para $U_{t}$.##

En presencia del factor de interacci#'on, la evoluci#'on temporal de un vector de estado dado $#Psi #in #H$, esta determinada por el grupo $V_{t}$, es decir, al tiempo $t$ el vector de estado correspondiente es $V_{t} #Psi$. Como mencionamos en el capitulo anterior, en un sistema de dispersi#'on t#'ipico, uno busca aproximar la evoluci#'on total del sistema a trav#'es de la evoluci#'on de estados correspondientes a sistemas libres (sistemas sin interacciones entre sus constituyentes) en los limites cuando $ t #rar #pm #infty $. Esto se puede hacer f#'acilmente si suponemos que dado un vector $#Psi #in #H$, existen dos vectores de estado $ #psi_{+} $ y $#psi_{-}$ en $#H$, tales que, $V_{t} #Psi$ converge en la norma de $#H$ a $U_{t} #psi_{#pm}$, cuando $t #rar #pm #infty$, respectivamente, es decir: 
#begin{equation}#label{cas-1}
#lim_{t #rar -#infty} #left#| V_{t}#Psi - U_{t}#psi_{-}  #right#|=0 #,#,;#,#,#, #lim_{t #rar +#infty} #left#| V_{t}#Psi - U_{t}#psi_{+}  #right#|=0
#end{equation}
Esto significa que $V_{t}#Psi$ es pr#'acticamente indistinguible de $U_{t}#psi_{-}$, en el pasado remoto y de $U_{t}#psi_{+} $, en el futuro distante. La necesidad de la existencia de los vectores $#psi_{#pm}$ que satisfagan la condici#'on #eqref{cas-1} es, por supuesto, una severa restricci#'on sobre el par de grupos unitarios $U_{t}$ y $V_{t}$ que podamos utilizar y corresponde esencialmente a imponer la condici#'on asint#'otica al sistema de dispersi#'on que se esta estudiando. En particular la condici#'on #eqref{cas-1}, implica que para todo operador de proyecci#'on $P$:
#begin{equation}
#lim_{t #rar -#infty} #left( #|PV_{t}#Psi #|^{2} - #| PU_{t}#psi_{-} #|^{2}  #right)=0 #,#,;#,#,#, #lim_{t #rar +#infty} #left( #|PV_{t}#Psi #|^{2} - #| PU_{t}#psi_{+} #|^{2}  #right)=0
#end{equation}
en el caso de dispersi#'on potencial y tomando como $P$ al operador de multiplicaci#'on de $L^{2}(#R^{3})$ por la funci#'on caracter#'istica de una cierta regi#'on $#Delta$, observamos que la diferencia entre las mediciones de probabilidad de posici#'on, sobre cualquier sub conjunto medible $#Delta$ de $#R^{3}$, para los estados $V_{t} #Psi$ y $ U_{t}#psi_{#pm} $, converge a cero cuando $t #rar #pm$ respectivamente. Por la linealidad y la continuidad de la transformada de Fourier, lo mismo sucede para las mediciones de la probabilidad del momento.##

Necesitamos aun especificar algunos puntos clave sobre la descripci#'on anterior que acabamos de discutir sobre la condici#'on asint#'otica, sobre todo debemos de especificar para que conjunto de vectores $#Psi #in #H$ (de ahora en adelante {#bf estados de dispersi#'on de $H$}) la condici#'on anterior se satisface.##

#n En la practica el conjunto de #emph{estados de dispersi#'on de} $H$ esta formado por funciones de onda que describen haces convergentes de part#'iculas a tiempos en el pasado remoto y haces divergentes a tiempos en el futuro distante. Por ejemplo, en el caso de dispersi#'on potencial, cada vector que evoluciona bajo el grupo $U_{t}$ esta muy alejado del centro dispersor (regi#'on donde ocurre la dispersi#'on mayormente) para tiempos remotos (tanto en el pasado como en el futuro). Es claro que por #emph{alejado} nos referimos a que la probabilidad de encontrar a las part#'iculas es despreciable cerca de la regi#'on de dispersi#'on (esta es la regi#'on del espacio en donde el potencial de interacci#'on deja de ser despreciable). As#'i tambi#'en, es claro que #eqref{cas-1} se cumple, solo si, $V_{t} #Psi$ satisface la propiedad anterior a tiempos remotos.##

Podemos obtener mas pistas acerca del conjunto de estados de dispersi#'on al considerar por ejemplo a los estados propios del operador $H$. Si $#Psi$ es un vector propio de $H$, es decir, $ H #Psi = #lambda #Psi $, para alguna $#lambda #in #R$, entonces el estado, $ V_{t}#Psi= #exp( -i #lambda t )#Psi $, es simplemente un m#'ultiplo de $#Psi$, haciendo que por tanto este vector no pueda describir una situaci#'on de dispersi#'on (ya que la probabilidad de posici#'on es claramente constante en toda regi#'on del espacio considerado). Por esta raz#'on, se espera que los estados de dispersi#'on est#'en asociados con el espectro  continuo de $H$. Sin embargo, en este momento no deseamos utilizar ninguna caracterizaci#'on particular de los estados de dispersi#'on ya que el principal objetivo de esta secci#'on es solo presentar las bases de la condici#'on asint#'otica. Simplemente vamos a suponer que a cada Hamiltoniano $H$ le podemos asociar un conjunto de estados de dispersi#'on $M_{#infty}(H)$, con las siguientes propiedades:
#begin{enumerate}
#item $M_{#infty}(H)$ es un subespacio de $#H$.
#item $M_{#infty}(H)$ es invariante bajo el grupo $#{ #exp( -iHt ) #}$.
#end{enumerate}

Claramente la condici#'on $1)$ significa que $M_{#infty}(H)$ debe de ser un conjunto lineal cerrado (ya que es parte de un espacio de Hilbert). La propiedad de ser un ``estado de dispersi#'on'' corresponde entonces a un observable, el cual esta representado por el operador auto adjunto $E_{#infty}(H)$, el cual es la proyecci#'on ortogonal con rango $M_{#infty}(H)$. La condici#'on $2)$ es una expresi#'on de la homogeneidad temporal que de los sistemas dispersores que consideramos aqu#'i: La definici#'on de los estados de dispersi#'on, no debe depender del tiempo (sin embargo, esto no siempre se cumple para Hamiltonianos dependientes del tiempo). De esta forma, la condici#'on $2)$ se puede re-escribir como:
#begin{equation}
E_{#infty}(H) #, #exp (-iHt) = #exp (-iHt) #, E_{#infty}(H)
#end{equation}

Ahora, reformularemos la condici#'on asint#'otica de una manera mas precisa, la cual es mas cercana a las condiciones f#'isicas de los fen#'omenos de dispersi#'on t#'ipicos. En la practica, uno comienza un proceso de dispersi#'on a tiempos muy negativos (es decir, tiempos en el pasado remoto) preparando un estado f#'isico, cuya evoluci#'on temporal esta dada por el grupo unitario no perturbado $U_{t}$. Esto corresponde a dar directamente el estado $#psi_{-}$ de #eqref{cas-1}. Debemos notar que en la condici#'on #eqref{cas-1}, $#psi_{-}$ es interpretado como el estado inicial al tiempo $t=0$, es decir, el estado que se prepara en el pasado remoto es mas bien $U_{t}#psi_{-}$, de esta forma vemos que $#psi_{-}$ debe pertenecer al espacio $M_{#infty} (H_{0})$. La primera parte de la condici#'on asint#'otica requiere entonces la existencia de un vector $#Psi #in M_{#infty} (H)$, tal que se cumpla la primera ecuaci#'on de #eqref{cas-1}.##
Ahora, la segunda parte de la condici#'on asint#'otica, demanda la existencia de un vector $#psi_{+} #in M_{#infty}(H_{0})$, tal que se cumpla la segunda ecuaci#'on de #eqref{cas-1}, en donde $#Psi$ es el vector que se obtuvo de la funci#'on $#psi_{-}$, a trav#'es de la primera parte condici#'on de la condici#'on asint#'otica. El estado $#psi_{+}$, tambi#'en es interpretado aqu#'i como el estado inicial al tiempo $t=0$, al cual la evoluci#'on temporal de $#Psi$, converge en el futuro remoto.##

Entonces, seg#'un la figura 1 que ilustra los procesos de dispersi#'on que cumplen con la condici#'on asint#'otica, la descripci#'on de los fen#'omenos de dispersi#'on consiste en asociar a cada vector de estado inicial $#psi_{-} #in M_{#infty}(H_{0})$, un vector de estado final $#psi_{+} #in M_{#infty}(H_{0}) $, ambos interpretados como estados al tiempo $t=0$. Si en el sistema que estamos estudiando no existe ning#'un tipo de interacci#'on, es decir, si $U_{t}=V_{t}$, claramente tenemos que $#psi_{-} = #psi_{+}$. Mas adelante veremos que la correspondencia $ #psi_{-} #rar #psi_{+} $, define un operador lineal en el subespacio $M_{#infty}(H_{0})$, de los estados de dispersi#'on del Hamiltoniano no perturbado. Este operador es conocido como el {#bf operador de dispersi#'on} $#S$ y es el objeto central de toda la teor#'ia de dispersi#'on. La diferencia entre el operador $#S$ y el operador identidad, es una expresi#'on que tiene que ver directamente con la interacci#'on entre los constituyentes del sistema dispersor. Es claro que si no existe ninguna interacci#'on dentro del sistema, el operador $#S$ coincide con el operador identidad por lo menos para todos los estados del subespacio $M_{#infty}(H_{0})$. ##

Ya que $V_{t}$ y $V_{t}^{*}$, son operadores unitarios, tenemos que:
$$ #| V_{t}#Psi - U_{t}#psi #|^{2}= ( V_{t}#Psi - U_{t}#psi , V_{t}#Psi - U_{t}#psi ) $$
$$= (V_{t}#Psi,V_{t}#Psi) - (V_{t}#Psi , U_{t}#psi) - (U_{t}#psi,V_{t}#Psi ) + (U_{t}#psi,U_{t}#psi) $$
$$= (#Psi , #Psi) - (#Psi ,V_{t}^{*} U_{t}#psi) - (V_{t}^{*} U_{t}#psi,#Psi ) + (V_{t}^{*}U_{t}#psi ,V_{t}^{*} U_{t}#psi) $$
$$ = #| #Psi - V_{t}^{*} U_{t}#psi #|^{2} $$

Debemos notar que este resultado es valido en general ( $#| Uf-g #|= #| f-U^{*}g #|$, $#forall f,g #in #H$, $U$ un operador unitario). Luego, tenemos que:
#begin{equation}#label{cas-2}
#lim_{t #rar -#infty} #| V_{t}#Psi - U_{t}#psi #| = #lim_{t #rar -#infty} #| #Psi - V_{t}^{*} U_{t}#psi #| =0
#end{equation}

En la primera parte de la condici#'on asint#'otica se asumi#'o que para cada $ #psi_{-} #in M_{#infty}(H_{0}) $, existe un vector $#Psi$, tal que la ecuaci#'on #eqref{cas-2} se cumpliera. Por tanto, esta condici#'on es equivalente a la existencia del limite de $V_{t}^{*} U_{t} #psi $, cuando $t #rar -#infty$, para toda $ #psi #in M_{#infty}(H_{0}) $. De esta forma, definimos al {#bf operador de onda} $#Omega_{-}$ en $M_{#infty}(H_{0})$, de la siguiente manera:
$$ #Omega_{-} #psi = #lim_{t #rar -#infty} V_{t}^{*} U_{t} #psi #,,#,#,#,#,#, #forall #psi #in M_{#infty}(H_{0}) $$
Es claro que el limite se toma con respecto a la norma del espacio de Hilbert $#H$. Este operador se interpreta de la siguiente forma: cuando se aplica a un vector $#psi #in M(H_{0})$, vamos a obtener un vector de estado, al tiempo $t=0$, el cual va a evolucionar en el pasado remoto al estado $U_{t} #psi$ (en el sentido de la primera ecuaci#'on de #eqref{cas-1}). Podemos definir al operador $#Omega_{-}$ en todo el espacio de Hilbert en el que estemos trabajando, simplemente definiendo $ #Omega_{-} #phi =0$ para toda $#phi$ en el complemento ortogonal de $M_{#infty}(H_{0})$. De esta forma podemos definir al operador de onda $#O_{-}$ de la siguiente manera:
#begin{equation}#label{wop-}
#Omega_{-}  = #slim_{t #rar -#infty} V_{t}^{*} U_{t} E_{#infty}(H_{0}) 
#end{equation}
De forma similar, podemos ver que la segunda parte de la condici#'on asint#'otica es equivalente a que se requiera la existencia del limite, $ U_{t}^{*}V_{t} #xi $ cuando $t #rar +#infty$ y $#xi #in #hbox{Ran}(#Omega_{-})$ ya que:
#begin{equation}#label{wap-a}
#lim_{t #rar +#infty} #| V_{t}#xi - U_{t} #psi #|= #lim_{r #rar +#infty} #| U_{t}^{*}V_{t} #xi - #psi #|=0
#end{equation}
Por otro lado, tambi#'en tenemos que:
#begin{equation}#label{wap-b}
#lim_{t #rar +#infty} #| V_{t}#xi - U_{t} #psi #|= #lim_{r #rar +#infty} #|  #xi - V_{t}^{*}U_{t} #psi #|=0
#end{equation}
De esta forma, definimos un segundo {#bf operador de onda} $#Omega_{+}$, el cual esta dada por:
$$ #Omega_{+} #psi = #lim_{t #rar +#infty} V_{t}^{*} U_{t} #psi #,,#,#,#,#, #psi #in M_{#infty}(H_{0}) $$
As#'i tambi#'en, podemos extender a $#O_{+}$ a todo el espacio de Hilbert en el que estemos trabajando, simplemente definiendo $#O_{+} #phi=0 $, para toda $#phi$ en el complemento ortogonal de $M_{#infty}(H_{0})$. De esta forma podemos definir al operador de onda $#O_{+}$ de la siguiente manera:
#begin{equation}#label{wop+}
#Omega_{+}  = #slim_{t #rar +#infty} V_{t}^{*} U_{t} E_{#infty}(H_{0}) 
#end{equation}

La condici#'on asint#'otica se puede expresar entonces como la necesidad de la existencia de los limites #eqref{wop-} y #eqref{wop+}.## 
El operador $#Omega_{+}$ se interpreta de la siguiente forma: cuando se aplica a un vector $#psi #in M_{#infty}(H_{0})$, vamos a obtener un vector de estado al tiempo $t=0$, el cual va a evolucionar en el futuro distante al vector $U_{t}#psi$ (en el sentido de la segunda ecuaci#'on de #eqref{cas-1}).##
De las ecuaciones #eqref{wap-a} y #eqref{wap-b}, podemos observar que, $ #xi = #lim_{t #rar + #infty} V_{t}^{*}U_{t} #psi = #Omega_{+} #psi$, si y solo si, $#psi = #lim_{t #rar +#infty} U_{t}^{*}V_{t}#xi $. Entonces, de esta forma podemos ver que la existencia de $#Omega_{+}$, implica la existencia del limite $ #lim_{t #rar +#infty } U_{t}^{*}V_{t} #xi $, cuando $#xi$ esta en el rango de $#Omega_{+}$. Pero ya que $#xi #in #hbox{Ran}(#Omega_{-})$ tenemos que, la segunda parte de la condici#'on asint#'otica se cumple, siempre que el rango del operador $#Omega_{-}$ este contenido en el rango del operador $#Omega_{+}$. Este postulado se debe de a#~nadir a las condiciones de existencia de los limites #eqref{wop-} y #eqref{wop+}, con el objetivo de que la descripci#'on matem#'atica, corresponda con la situaci#'on f#'isica dada anteriormente.##

%#includegraphics[scale=0.9]{as-con.png}

#newpage

#subsubsection{Los operadores de onda}
En esta parte nos dedicaremos a estudiar algunas de las propiedades que poseen los operadores de onda $#Omega_{-}$ y $#Omega_{+}$, las cuales nos van a ser #'utiles para dar la formulaci#'on final de la condici#'on asint#'otica y para poder definir al operador de dispersi#'on $#S$.## 
Comenzamos definiendo un nuevo tipo de operadores lineales.
#begin{definition}
Diremos que un operador lineal $#Omega$, es una {#bf isometr#'ia parcial}, si cumple con las siguientes propiedades:
#begin{enumerate}
#item $D(#Omega)= #H$.
#item $#Omega^{*} #Omega= P $, con $P$ una proyecci#'on ortogonal.
#end{enumerate}
Aqu#'i, $#H$ es el espacio de Hilbert del sistema f#'isico que estamos estudiando.
#end{definition}

Ya que $#Omega^{*} #Omega$, es una proyecci#'on ortogonal, es f#'acil ver que para este tipo de operadores tenemos que:
#begin{equation}#label{isp-p}
( #O #psi , #O #phi )= ( #psi , #O^{*}#O #phi )= ( #psi , P #phi )= (P #psi , P #phi)
#end{equation}
Por tanto, si $ #psi , #phi #in #hbox{Ran}(P) $, entonces $ ( #O #psi , #O #phi )=(#psi , #phi) $ (ya que en este caso, $P#psi=#psi$ y $P#phi= #phi$). Esto muestra que un operador como $#O$, es isom#'etrico en un sub conjunto de $#H$, a saber, en $#hbox{Ran}(P)$, es decir, preserva la norma y el #'angulo entre vectores que pertenezcan a este sub conjunto. Esto explica por que a estos operadores se les conoce como ``isometr#'ias parciales ". De la expresi#'on #eqref{isp-p} es f#'acil ver que $#O$ es cero en el complemento ortogonal de $#hbox{Ran}(P)$: si $#psi #in (#hbox{Ran}(P))^{#perp} $, entonces $#| #O #psi #|^{2} = #| P #psi #|^{2}=0 $, implicando que $#O #psi =0$. Esto tambi#'en se puede escribir como:
#begin{equation}#label{pr-om}
#O ( I - P )#, #psi = 0 #,,#,#,#,#,#, #forall #psi #in #H
#end{equation}
es decir, $ #O = #O P $.##
Una isometr#'ia parcial tambi#'en puede ser caracterizada por la propiedad #eqref{isp-p}:
#begin{proposition}#label{isop-2}
Si $#O$ es un operador lineal con dominio $#H$ y $P$ una proyecci#'on ortogonal, tal que, $#| #O #psi #| = #| P #psi #|$, para toda $#psi #in #H$, entonces $#O^{*} #O = P$. 
#end{proposition} 
#n {#bf Demostraci#'on}##
Ya que $ #| #O #psi #| = #| P #psi #| $, para toda $#psi #in #H$, tenemos que $#| #O #|= #|P#|$, es decir, $#| #O#| =1 $ (A menos claro que $P=0$). Luego por la proposici#'on #ref{t-ad-nor}, tenemos que $ #| #O^{*} #|=#|P#| $ adem#'as $#O^{*}$ esta definido tambi#'en en todo $#H$. Ahora, por la identidad de polarizaci#'on y por las propiedades del producto interno de la proyecci#'on $P$, tenemos que para cualquier $#psi , #phi #in #H$:
$$ (#O^{*} #O #psi , #phi )=( #O #psi , #O #phi )=  $$
$$ #frac{1}{4} #left#{ #| #O( #psi + #phi ) #|^{2} - #| #O( #psi - #phi ) #|^{2} - i#, #| #O( #psi +i#, #phi ) #|^{2} + i#, #| #O( #psi - i#,#phi ) #|^{2} #right#} = $$

$$ #frac{1}{4} #left#{ #| P( #psi + #phi ) #|^{2} - #| P( #psi - #phi ) #|^{2} - i#, #| P( #psi +i#, #phi ) #|^{2} + i#, #| P( #psi - i#,#phi ) #|^{2} #right#} = $$

$$ ( P#psi , P #phi )= (  P#psi , #phi ) $$

esto implica que, $  ( #O^{*} #O #psi - P#psi , #phi ) = 0 $, $ #forall #phi #in #H $. Por tanto podemos concluir que, $ #O^{*} #O #psi = P #psi $, $ #forall #psi #in #H $. #qed 

#vspace{5 mm}

Es claro que en caso de que tengamos $P= I$, $#O$ es simplemente un operador isom#'etrico.##

El adjunto de una isometr#'ia parcial $#O^{*}$, tambi#'en es una isometr#'ia parcial. De hecho, $ P' = #O^{**} #O^{*}= #O #O^{*} $ es una proyecci#'on, ya que $ P'^{2}= #O #O^{*} #O #O^{*} = #O P #O^{*}= #O #O^{*}=P' $ y $ P'=( #O #O^{*} )^{*}= #O^{**} #O^{*} =P' $.##
En la siguiente proposici#'on establecemos algunas de las propiedades mas importantes de las isometr#'ias parciales, las cuales nos ser#'an #'utiles mas adelante en la definici#'on del operador de dispersi#'on.## 

#begin{proposition}#label{isop-ps}
Sea $#O$ una isometr#'ia parcial, $ #O^{*}#O=P $ y definamos, $ #O #O^{*}= Q $. Entonces:
#begin{enumerate}
#item $ #| #O #|= #| #O^{*} #|=1 $, a menos que $P=0$.

#item $ #O P = #O $, $ P#O^{*}= #O^{*} $, $ Q #O = #O $, $ #O^{*}Q= #O $. 

#item El rango de $#O$ es un subespacio y $Q$ es la proyecci#'on ortogonal sobre este.

#item La restricci#'on de $#O$ al subespacio $ #hbox{Ran}(P) $, es invertible y su inversa esta dada por $#O^{*}$ ( mas precisamente, por la restricci#'on de $#O^{*}$ a $#hbox{Ran}(Q)$ ).
#end{enumerate}
#end{proposition}
#n {#bf Demostraci#'on}##
$1.$ Ya ha sido probado en la proposici#'on anterior.##
$2.$ Ya hemos probado que $#O P = #O$ (ec. #eqref{pr-om}). Usando el hecho de que $P$ es una proyecci#'on ortogonal ($D(P)=#H$ y $P^{2}=P=P^{*}$) y las propiedades del operador adjunto (ecs. #eqref{ad-prs}) tenemos que, $ P#O^{*}=P^{*} #O^{*}= (#O P)^{*}= #O^{*} $. Ahora, $ Q#O= #O #O^{*} #O=#O P=#O $. Luego por lo que hemos mostrado anteriormente, $ #O^{*}Q= #O^{*} #O #O^{*}= P #O^{*}= #O^{*} $.##
$3.$ La definici#'on $ Q= #O #O^{*} $, muestra que el rango de $Q$, esta contenido en el rango de $#O$: si $#phi #in #H$ y $ #phi = Q #psi $, entonces $ #phi= #O ( #O^{*}#psi ) #in #hbox{Ran}(#O) $. Por tanto, $ #hbox{Ran}(Q) #subset #hbox{Ran}(#O) $. De forma similar, usando el hecho $ Q#O = #O P =#O $, tenemos que, $#hbox{Ran}(#O) #subset #hbox{Ran}(Q)  $. Por lo tanto, $#hbox{Ran}(#O)= #hbox{Ran}(Q)$. Ya que $#O$ es un operador lineal y acotado ($#| #O #|=1$, por tanto continuo) definido en todo el espacio, es inmediato ver que su rango es un subespacio (cerrado) de $#H$.##
$4.$ Supongamos que $#phi #in #hbox{Ran}(P)$ y que $#O #phi = 0$. Por #eqref{isp-p}, tenemos que, $#| #phi #|^{2}= #| #O #phi #|^{2}=0 $, es decir $ f=0 $. Por tanto, la restricci#'on de $#O$ a $#hbox{Ran}(P)$ es invertible. Ya que $ #O^{*} #O #phi = #phi $, para $ #phi #in #hbox{Ran}(P) $, es claro que la inversa de la restricci#'on de $#O$, esta dada por la restricci#'on de $#O^{*}$ a $ #hbox{Ran}(P) $.  #qed

#vspace{5 mm}

Para una isometr#'ia parcial $#O$, al conjunto $#hbox{Ran}(P)$ se le conoce como {#bf conjunto inicial} de $#O$, mientras que al conjunto $#hbox{Ran}(Q)$ se le conoce como {#bf conjunto final} o simplemente {#bf rango} de $#O$.##
Es claro que si $#O$ es una isometr#'ia, tenemos que $P=I$, sin embargo no siempre es cierto que en este caso, el operador de proyecci#'on $Q$ tambi#'en sea igual al operador identidad (a menos que el espacio de Hilbert en el que estemos trabajando sea de dimensi#'on finita [15]). En particular $#O$ ser#'a un operador unitario, si y solo si, $Q$ es el operador identidad.

El siguiente resultado muestra que los operadores de onda $#O_{-}$ #eqref{wop-} y $#O_{+}$ #eqref{wop+}, son isometr#'ias parciales. 

#begin{proposition}
Los operadores de onda $#O_{#pm}$, son isometr#'ias parciales con conjunto inicial $M_{#infty}(H_{0})$.
#end{proposition}
#n {#bf Demostraci#'on}##
La demostraci#'on la haremos para $#O_{+}$, ya que para $#O_{-}$ es completamente an#'aloga.##
Por la ecuaci#'on #eqref{wop+}, podemos escribir a $#O_{+}$ como:
#begin{equation}#label{wop+2}
#O_{+} = #slim_{t #rar + #infty} V_{t}^{*} U_{t} E_{#infty}(H_{0})
#end{equation}
en donde $E_{#infty}(H_{0})$ es la proyecci#'on ortogonal con rango $M_{#infty}(H_{0})$. De esta forma tenemos que $D( #O_{+} )= #H$. Mas aun, la ecuaci#'on #eqref{wop+2} y tomando el hecho de que $U_{t}$ y $V_{t}$ son unitarios, tenemos que para toda $#psi #in #H$:
$$ #| #O_{+} #psi #| = #lim_{t #rar #infty} #| V_{t}^{*} U_{t} E_{#infty}(H_{0}) #psi #|= #| E_{#infty}(H_{0}) #psi #| $$
Haciendo $#O= #O_{+}$ y $P = E_{#infty}(H_{0}) $ en la proposici#'on #ref{isop-2}, podemos concluir que $#O_{+}$ es una isometr#'ia parcial. Luego por la proposici#'on #ref{isop-ps} tenemos que el conjunto inicial de $#O_{+}$ es $M_{#infty}(H_{0})$. #qed

#vspace{5 mm}

En particular por la proposici#'on #ref{isop-ps}, tenemos ahora que los operadores de onda $#O_{#pm}$, son operadores acotados definidos en todo el espacio.##

En muchos casos de inter#'es f#'isico, el conjunto $M_{#infty}(H_{0})$, de estados de dispersi#'on del Hamiltoniano no perturbado, resulta ser todo el espacio de Hilbert $#H$, en cuyo caso los operadores de onda resultan ser isometr#'ias, en particular esto es cierto para la dispersi#'on potencial [15].##

De acuerdo con la proposici#'on #ref{isop-ps}, los operadores:
#begin{equation}
Q_{#pm} = #O_{#pm} #O_{#pm}^{*}
#end{equation}
son las proyecciones ortogonales sobre el rango del correspondiente operador de onda. Ahora estudiaremos a los adjuntos de los operadores de onda.

#begin{proposition}
Los adjuntos de los operadores $#O_{#pm}$, est#'an dados por:
#begin{equation}
#O_{#pm}^{*} = #slim_{t #rar #pm #infty} U_{t}^{*} V_{t} Q_{#pm}
#end{equation}
#end{proposition}
#n {#bf Demostraci#'on}##
La demostraci#'on la haremos para $#O_{+}$, ya que para $#O_{-}$ es completamente an#'aloga.##
Utilizando el hecho de que para un operador unitario $U$, $#| Uf-g #|= #| f-U^{*}g #|$, $#forall f,g #in #H$, tenemos que $#forall #psi #in #H$:
$$ #lim_{t #rar #infty} #| U_{t}^{*}V_{t} #O_{+} #psi - E_{#infty}(H_{0}) #psi #|= #lim_{t #rar #infty} #| #O_{+} #psi - V_{t}^{*}U_{t} E_{#infty}(H_{0}) #psi #|=0 $$
de esta forma obtenemos:
#begin{equation}
#slim_{t #rar #infty} U_{t}^{*}V_{t}#O_{+}= E_{#infty}(H_{0})
#end{equation}
Multiplicando la ecuaci#'on anterior por $#O_{+}^{*}$ del lado derecho, utilizando la definici#'on de $Q_{+}$ y la proposici#'on #ref{isop-ps}, tenemos que:
$$ #slim_{t #rar #infty} U_{t}^{*}V_{t}#O_{+} #O_{+}^{*}= E_{#infty}(H_{0}) #O_{+}^{*} = #O_{+}^{*} $$
#begin{equation}
#slim_{t #rar #infty} U_{t}^{*}V_{t} Q_{+}= #O_{+}^{*}
#end{equation}
#qed

#newpage
 
Ahora estamos lisos para dar la formulaci#'on final de la condici#'on asint#'otica.

#paragraph{La condici#'on asint#'otica (formulaci#'on final):}
#begin{enumerate}
#item Existen los limites, $ #slim_{ t #rar #pm #infty } V_{t}^{*}U_{t} E_{#infty}(H_{0})= #O_{#pm} $. 

#item $ #hbox{Ran}(Q_{-}) #subset #hbox{Ran} (Q_{+}) $.##
Los subespacios $#hbox{Ran}(Q_{#pm})$, van a estar contenidos en el subespacio $M_{#infty}(H)$ de estados de dispersi#'on de $#H$. La situaci#'on mas simple, es aquella en la que:

#item $ #hbox{Ran}(Q_{+})= #hbox{Ran}(Q_{-}) =M_{#infty}(H) $.
#end{enumerate}

#rightline{$#dag$}
#vspace{5 mm}

Si la condici#'on $3$ se cumple, entonces se dice que nuestra teor#'ia es {#bf asint#'oticamente completa}, ya que a evoluci#'on $V_{t} #Psi $ de cualquier estado de dispersi#'on $ #Psi$ de $H$, puede ser descrita asint#'oticamente mediante la evoluci#'on de un estado libre, tal y como sucede en la expresi#'on #eqref{cas-1}. Por tanto, el tipo mas simple de sistema de dispersor que podemos encontrar, es aquel para el cual ambos operadores de onda $#O_{#pm}$ existen y sus rangos son iguales a $M_{#infty}(H)$, es decir, un sistema para el cual las condiciones $1$ y $3$ de la condici#'on asint#'otica se cumplen. En este caso hablamos de un {#bf sistema de dispersi#'on simple}.##
Ahora, enunciaremos algunas consecuencias de la condici#'on asint#'otica sobre los operadores $#O_{#pm}$. Las demostraciones de estos hechos requieren de una teor#'ia del an#'alisis funcional mas elaborada, por lo que nos referimos a [15] para la demostraci#'on completa de estos.

#begin{proposition}#label{oms-pr}
Si se cumple la condici#'on $1$, de la de la formulaci#'on final de la condici#'on asint#'otica, entonces para todo $#tau #in #R$:
#begin{enumerate}
#item $V_{#tau} #O_{#pm}= #O_{#pm} U_{#tau}$.
#item  $ U_{#tau} #O_{#pm}^{*}= #O_{#pm}^{*} V_{#tau} $.
#item $ Q_{#pm}V_{#tau} = V_{#tau} Q_{#pm} $.

Las dos primeras ecuaciones significan que los operadores de onda $#O_{#pm}$ ``entrelazan", a los grupos unitarios $U_{t}$ y $V_{t}$ y son conocidas com#'unmente como las {#bf relaciones de entrelazamiento}.

#item Si $ #psi #in D(H_{0}) $, entonces $#O_{#pm} #psi #in D(H)$ y $ H #O_{#pm} #psi=#O_{#pm} H_{0} #psi $.

#item Si $#phi #in D(H)$, entonces $ #O_{#pm}^{*} #phi #in D(H_{0}) $ y $ H_{0}#O_{#pm}^{*} #phi = #O_{#pm}^{*} H #phi $.
#end{enumerate}
#end{proposition}

Ahora estamos en posici#'on de definir al operador de dispersi#'on $#S$. Es f#'acil ver de la figura 1, que el operador $#S$ esta dado por:
#begin{equation}
#S = #O_{+}^{-1} #O_{-}
#end{equation}
cuando nos restringimos al subespacio $M_{#infty}(H_{0})$.##
Esta definici#'on tiene sentido si se cumple la condici#'on $2$ de la formulaci#'on final de la condici#'on asint#'otica. De hecho, el rango de $#O_{-}$ esta contenido en el de $#O_{+}$ y adem#'as por la proposici#'on #ref{isop-ps} es invertible en su rango, mas aun por esta misma proposici#'on, podemos remplazar a $#O_{+}^{-1}$ por $#O_{+}^{*}$. Por tanto, adoptaremos la siguiente definici#'on del operador de dispersi#'on, valida si se cumplen las condiciones $1$ y $2$ de la formulaci#'on final de la condici#'on asint#'otica:
#begin{equation}#label{op-dis}
#S = #O_{+}^{*} #O_{-}
#end{equation}

#newpage

#section{El operador de dispersi#'on}
En esta parte estudiaremos algunas propiedades matem#'aticas del operador de dispersi#'on $#S$, para sistemas que cumplen la formulaci#'on final de la condici#'on asint#'otica. En particular veremos que $#S$ tambi#'en es una isometr#'ia parcial, que el conjunto $D(H_{0})$ es invariante frente a este operador y que cuando nuestra teor#'ia es asint#'oticamente completa $#S$ es unitario.

#begin{proposition}
El operador $#S$ es una isometr#'ia parcial, con conjunto inicial $M_{#infty}(H_{0})$, es decir:
#begin{equation}#label{s-ppr}
#S^{*} #S = E_{#infty}(H_{0})
#end{equation}
El rango de $#S$ es un subespacio de $M_{#infty}(H_{0})$ el cual es invariante bajo el grupo unitario $U_{t}$.
#end{proposition}
#n {#bf Demostraci#'on}##
Ya que $D(#O_{-})=D(#O_{+}^{*})=#H $, tenemos que $D(#S)=#H$. Mas aun, de las ecuaciones #eqref{op-dis} y #eqref{ad-prs}, tenemos que, $#S^{*} #S= #O_{-}^{*}#, #O_{+} #, #O_{+}^{*} #, #O_{-}=#O_{-}^{*}#, Q_{+}#, #O_{-} $. Por la segunda condici#'on de la formulaci#'on final de la condici#'on asint#'otica, tenemos que el rango de $#O_{-}$ esta contenido en el rango de $Q_{+}$, de tal forma que $Q_{+} #O_{-} =#O_{-}$. Por tanto, $ #S^{*} #S = #O_{-}^{*} #O_{-}= E_{#infty}(H_{0}) $, lo cual prueba que $#S$ es una isometr#'ia parcial, aso como la ecuaci#'on #eqref{s-ppr}.##

El rango de $#S$ es un subespacio del rango de $#O_{+}^{*}$, y el rango de este ultimo es $M_{#infty}(H_{0})$, por la proposici#'on #ref{isop-ps}.## 
De las relaciones de entrelazamiento (proposici#'on #ref{oms-pr}) y por la definici#'on del operador $#S$ (ec. #eqref{op-dis}), tenemos que para $t #in #R$:
$$ SU_{t}= #O_{+}^{*} #, #O_{-} #, U_{t}= #O_{+}^{*} #, V_{t}#, #O_{-}= U_{t}#, #O_{+}^{*}#, #O_{-}= U_{t} S $$

Por tanto $#S U_{t}=U_{t} #S$, $#forall t #in #R$. Esto muestra que si $#xi #in #hbox{Ran}(#S) $, entonces $U_{t} #xi #in #hbox{Ran}(#S)$ y por lo tanto, $#hbox{Ran}(#S)$ es invariante bajo el grupo unitario $U_{t}$, $#forall t #in #R$. #qed

#vspace{5 mm}

En la demostraci#'on de la proposici#'on anterior, en particular hemos probado que:
#begin{equation}#label{su-us}
#S U_{t}=U_{t} #S #,,#,#,#,#, #forall t #in #R
#end{equation}
este hecho es fundamental para establecer el siguiente resultado.

#begin{proposition}
$D(H_{0})$, es invariante ante el operador $#S$, es decir, si $#psi #in D(H_{0})$, entonces $#S #psi #in D(H_{0})$. 
#end{proposition}
#n {#bf Demostraci#'on}##
Sea $#psi #in D(H_{0})$, luego por el teorema de Stone (teorema #ref{stone}), tenemos que:
$$ #left#| #frac{i}{t} #, #S (U_{t}-I) #psi - #S H_{0} #psi  #right#| #leq #| #S #| #cdot #left#| #frac{i}{t}  (U_{t}-I) #psi -  H_{0} #psi  #right#| #rar 0 $$
cuando $t #rar 0$. Esto implica que:
$$ i #frac{#partial}{#partial t} ( #S U_{t} #psi)= #S H_{0} #psi $$
luego el teorema de Stone y la ecuaci#'on #eqref{su-us}, implican que:
$$ H_{0} #S #psi = i #, #frac{#partial}{#partial t}( U_{t}#S #psi )= i #, #frac{#partial}{#partial t}(#S U_{t} #psi )= #S H_{0} #psi  $$
Por tanto tenemos que, $ #S #psi #in D(H_{0}) $. #qed

#vspace{5 mm}

Debemos hacer notar que, en general: 
#begin{equation}
#S H_{0} #subset H_{0} #S
#end{equation}
Sin embargo, la igualdad se da si nos restringimos al subespacio $M_{#infty} (H_{0})$, es decir:
#begin{equation}
#S H_{0} E_{#infty} (H_{0})= H_{0} #S
#end{equation}
Estas dos ecuaciones son expresan una propiedad fundamental del operador de dispersi#'on, ya que en t#'erminos f#'isicos muestran que la energ#'ia del Hamiltoniano libre $H_{0}$ se conserva y que en este caso el proceso de dispersi#'on es el#'astico.##

Todas las propiedades que hemos deducido hasta ahora del operador $#S$, dependen solamente de la valides de las condiciones $1$ y $2$ de la formulaci#'on final de la condici#'on asint#'otica. Observamos que la restricci#'on de $#S$ a $M_{#infty}(H_{0})$ es una isometr#'ia en el espacio de Hilbert $M_{#infty}(H_{0})$. En particular, si $M_{#infty}(H_{0})= #H $ (tal como sucede en la dispersi#'on potencial) $#S$ es una isometr#'ia en $#H$. Sin embargo, las condiciones $1$ y $2$ antes mencionadas, no implican que $#S$ sea unitario en $M_{#infty}(H_{0})$. $#S$ va a ser unitario, si y solo si, su rango es igual a $M_{#infty}(H_{0})$ o de forma equivalente, si y solo si, $#S #S^{*} = E_{#infty}(H_{0})$. Ahora, $#S #S^{*}=#O_{+}^{*}#, #O_{-} #, #O_{-}^{*} #, #O_{+}=#O_{+}^{*}#, Q_{-} #, #O_{+} $. Si el rango de $#O_{+}$ esta contenido en el de $#O_{-}$, resulta que $Q_{-} #O_{+}= #O_{+}$ y por tanto $#S #S^{*}= #O_{+}^{*} #, #O_{+}= E_{#infty}(H_{0}) $. Aunque si el rango de $#O_{-}$ resulta ser estrictamente menor que el de $#O_{+}$, $Q_{-} #O_{+}$ va a ser cero en un sub conjunto no trivial de $ M_{#infty}(H_{0}) $. Por tanto $#S #S^{*}$ va a ser una proyecci#'on estrictamente menor que $E_{#infty}(H_{0})$ y $#S$ no va a ser unitario.##
Lo anterior es un esquema de la demostraci#'on de la siguiente proposici#'on.

#begin{proposition}#label{prop-s-u}
El operador de dispersi#'on $#S$ es unitario en $M_{#infty}(H_{0})$, si y solo si, el rango de $#O_{+}$ es igual al de $#O_{-}$. En particular $#S$ es unitario si nuestra teor#'ia es asint#'oticamente completa. 
#end{proposition} 

#vspace{5 mm}

Ahora, resumimos los principales resultados de esta secci#'on para el caso mas usual, el cual se da cuando $M_{#infty}(H_{0})=#H$. Hemos visto que la condici#'on asint#'otica nos conduce de una manera muy natural a postular la existencia de los operadores de onda $#O_{#pm}= #slim V_{t}^{*}U_{t} $, cuando $t #rar #pm #infty$ (recordemos que aqu#'i el limite se toma respecto a la norma del espacio de Banach de los operadores acotados). Estos son operadores isom#'etricos, los cuales ``entrelazan" a los Hamiltonianos $H_{0}$ y $H$. El operador de dispersi#'on $#S$ se relaciona de una forma muy sencilla con estos operadores a trav#'es de la ecuaci#'on $#S= #O_{+}^{*}#O_{-} $. $#S$ es una isometr#'ia que conmuta con el Hamiltoniano libre $H_{0}$. El hecho de que en este caso $#S$ sea un operador unitario, no es una consecuencia directa de la existencia de los operadores de onda solamente, si no que mas bien es una hip#'otesis adicional, la cual sin embargo se verifica si nuestra teor#'ia es asint#'oticamente completa.


#chapter{Representaciones espectrales}

Como hemos visto anteriormente, el espacio $L^{2} (#R^{n})$ esta formado por funciones de cuadro integrable (o mas bien por clases de equivalencia de este tipo de funciones), con dominio en $#R^{n}$ y valores en $#C$, el cual es un espacio de Hilbert de una dimensi#'on. Una generalizaci#'on natural de una estructura como la anterior, seria el considerar funciones que toman valores en un espacio vectorial mas general, por ejemplo en un cierto espacio de Hilbert $#H_{0}$ de dimensi#'on arbitraria. Para esto, supongamos que $#Lambda$ es un sub conjunto de la recta real, medible en el sentido de Lebesgue y que $#H_{0}$ es un espacio de Hilbert separable. Ahora, consideremos una funci#'on $F$ de la forma:
$$F: #Lambda #rar #H_{0}$$
esto es, $F$ es una funci#'on que toma un elemento de $#lambda #in #Lambda$ (un n#'umero real) y le asocia un vector $f$ del espacio de Hilbert $#H_{0}$. A una funci#'on de este tipo la conoceremos simplemente como una {#bf funci#'on vectorial}.##
Usaremos la notaci#'on $f_{#lambda}$ para el valor (en $#H_{0}$) de la funci#'on $F$ en el punto $#lambda$ y denotaremos a la funci#'on $F$ por medio de la colecci#'on $ #{ f_{#lambda} #} $ de vectores de $#H_{0}$ ($F=#{ f_{#lambda} #}$). Diremos que una funci#'on como $F$ es {#bf medible}, si para cada vector $h #in #H_{0}$ la funci#'on con valores complejos $( h,f_{#lambda} )_{0}$ es medible en el sentido de Lebesgue, en donde $( #cdot , #cdot )$ es el producto escalar de $#H_{0}$.##
Si $F=#{ f_{#lambda} #}$ y $G=#{ g_{#lambda} #}$, son dos funciones vectoriales medibles, entonces la funci#'on compleja $( f_{#lambda} , g_{#lambda} )_{0}$ tambi#'en es medible, como funci#'on de $#lambda #in #l$. Esto es consecuencia de la expresi#'on:
$$ ( f_{#lambda} , g_{#lambda} )_{0} = #sum_{k=1}^{#infty} ( f_{#lambda},e_{k} )_{0}#, (e_{k}, g_{#lambda} )_{0} $$
en donde $#{ e_{k} #}$ es una base ortonormal de $#H_{0}$ y del hecho de que el limite de una sucesi#'on de funciones medibles es de nuevo una funci#'on medible (en caso de que tal limite exista).##
Diremos que las funciones vectoriales medibles $F=#{ f_{#lambda} #}$ y $G=#{ g_{#lambda} #}$, son equivalentes ($F #sim G$) , si y solo si, son diferentes solamente en un conjunto de medida cero, es decir:
#begin{equation}#label{eqv-2}
F #sim G #, #sss #, m#{ #lambda #in #Lambda #,|#, f_{#lambda} #neq g_{#lambda} #}=0
#end{equation}
en donde $m$ denota la medida de Lebesgue. La demostraci#'on de que en efecto la relaci#'on $#sim$, es de equivalencia en el caso de las funciones vectoriales, es completamente an#'aloga a la demostraci#'on de la proposici#'on #ref{eqv-l2}.##
Tal y como sucede con el espacio de funciones $L^{2}(#R^{n})$, denotaremos por $L^{2}( #Lambda , #H_{0} )$ al espacio de funciones vectoriales medibles (o mas bien de clases de equivalencia seg#'un #eqref{eqv-2}), las cuales satisfacen:
#begin{equation}#label{L2lc}
#| F #|^{2} = #int_{#Lambda} #| f_{#lambda} #|_{0}^{2} #,#, d#lambda < #infty 
#end{equation}
Las operaciones de suma de vectores, multiplicaci#'on de un vector por un escalar (el cual pertenece al campo sobre el cual esta $#H_{0}$) y la definici#'on del un producto escalar de dos vectores de $L^{2}(#Lambda , #H_{0})$, est#'an dadas por las relaciones:
#begin{equation}
F+G= #{ f_{#lambda} + g_{#lambda} #}
#end{equation}
#begin{equation}
#alpha F = #{ #alpha f_{#lambda} #}
#end{equation}
#begin{equation}#label{prod-fg}
(F,G)= #int_{#Lambda} ( f_{#lambda},g_{#lambda} )_{0} #,#, d#lambda
#end{equation} 
En donde $F=#{ f_{#lambda} #}$ y $G=#{ g_{#lambda} #}$. Es f#'acil ver que la integral #eqref{prod-fg} existe para $F,G #in L^{2}(#Lambda,#H_{0})$ (es decir, de que $|(F,G)| < #infty$). Primero tenemos que $ ( #|f_{#lambda}#|_{0} - #|g_{#lambda}#|_{0}  )^{2} #geq 0 $, lo cual implica que:
$$ #|f_{#lambda}#|_{0} #cdot #|g_{#lambda}#|_{0} #leq #frac{1}{2} #left( #|f_{#lambda}#|_{0}^{2} + #|g_{#lambda}#|_{0}^{2} #right) $$
Ahora, por la desigualdad de Schwarz y la condici#'on #eqref{L2lc}, tenemos que:
$$ |(F,G)| #leq #int_{#Lambda} |( f_{#lambda},g_{#lambda} )_{0}|#, d #lambda #leq #int_{#Lambda} #|f_{#lambda}#|_{0} #cdot #|g_{#lambda}#|_{0}#, d #lambda #leq #frac{1}{2} #left( #int_{#Lambda} #|f_{#lambda}#|_{0}^{2} #, d #lambda + #int_{#Lambda} #|g_{#lambda}#|_{0}^{2} #, d #lambda #right) < #infty  $$

Se puede verificar que el espacio $L^{2}(#Lambda , #H_{0})$, cumple con todos los axiomas de un espacio de Hilbert, incluyendo la propiedad de completitud, de la misma forma como se hiso con $L^{2}(#R^{n})$. Incluso de esta misma forma se puede mostrar que $L^{2}(#Lambda , #H_{0})$ es un espacio separable. ##

Supongamos que $#{ e_{k} #}$ es una base ortonormal de $#H_{0}$ y hagamos:
#begin{equation}
#alpha_{k} (#lambda) = (e_{k},f_{#lambda})_{0}
#end{equation}
algunas veces es conveniente expresar todas las propiedades de $L^{2}(#Lambda ,#H_{0})$ en t#'erminos de las ``funciones coordenadas" $#alpha_{k} (#lambda)$. En particular, una funci#'on $F=#{ f_{#lambda} #}$ pertenece a $L^{2}(#Lambda ,#H_{0})$, si y solo si:
#begin{equation}
#sum_{k} #int_{#Lambda} | #alpha_{k}(#lambda) |^{2}#, d#lambda < #infty
#end{equation}

Una propiedad muy importante del espacio de Hilbert $L^{2}(#Lambda ,#H_{0})$ que usaremos con frecuencia en el desarrollo siguiente, es la siguiente: Existe una sucesi#'on $#{ F_{n} #}_{n} #subset L^{2}(#Lambda ,#H_{0})$ ( $F_{n}=#{ f_{n,#lambda} #}$ ), tal que, para cada $#lambda #in #Lambda$ fijo, el conjunto $#{ f_{n,#lambda} #}_{n} #subset #H_{0}$, es un {#bf conjunto fundamental} de $#H_{0}$. En un espacio de Hilbert se dice que un conjunto $C$ es fundamental, si el conjunto de todas las combinaciones lineales finitas de elementos de $C$, es denso en este espacio de Hilbert. Esto se puede ver en el espacio $#H_{0}$, escribimos $f_{n,#lambda} = e_{n} #, t(#lambda) $, en donde $#{ e_{n} #}$ es una base ortonormal de $#H_{0}$ y $t(#lambda)$ es una funci#'on medible de cuadrado integrable, la cual es diferente de cero para todo $ #lambda #in #Lambda $.##

Con la notaci#'on que acabamos de introducir, observamos que podemos escribir a $L^{2}(#R^{n})$, como $L^{2}(#R^{n},#C)$. Ahora, veamos como podemos representar al espacio de Hilbert $L^{2}(#R^{3})$, en la forma $L^{2}(#Lambda , #H_{0})$. Esta representaci#'on nos va a ser #'util para dar una nueva definici#'on del operador de dispersi#'on $#S$ en t#'erminos del momento de un sistema.##

Denotemos por $S^{2}$, a la esfera unitaria con centro en el origen de $#R^{3}$. Ya que $S^{2}$ es una superficie de dos dimensiones, a cada punto de esta lo podemos identificar mediante el #'angulo polar $#t$ ($0 #leq #t #leq #pi$) y el #'angulo azimutal $#phi$ ($0 #leq #phi #leq 2 #pi  $). Sea $L^{2}(S^{2})$ el espacio de funciones (o mas bien clases de equivalencia) $f(#t , #phi)$, tales que:
#begin{equation}
#int_{0}^{2#pi} #int_{0}^{#pi} | f(#t , #phi) |^{2} #, #sen (#t) #, d #t #, d #phi < #infty 
#end{equation}

Introduciendo coordenadas esf#'ericas en el espacio $#R^{3}$, tenemos que para cualquier funci#'on $f #in L^{2}(#R^{3})$:
#begin{equation}
#| f #|^{2}= #int_{#R^{3}} |f(#x)|^{2} #, d#x = #int_{0}^{#infty} #int_{0}^{#pi} #int_{0}^{2#pi} | f(r,#t , #phi) |^{2} #, r^{2} #, #sen(#t) #, d#phi #, d#t #, dr 
#end{equation}

de la ecuaci#'on anterior podemos observar claramente que, $r#, f(r , #cdot ,#cdot ) #in L^{2}(S^{2})$, para casi toda $r$ en el intervalo $[ 0,#infty )$ respecto a la medida de Lebesgue. Por tanto podemos definir a la funci#'on $F = #{  r#, f(r , #cdot ,#cdot ) #} #in L^{2}( [0,#infty),L^{2}(S^{2}) ) $. De esta forma podemos concluir que $ L^{2}(#R^{3}) #subset L^{2}( [0,#infty),L^{2}(S^{2}) ) $.##

Por otro lado, tomemos una funci#'on $F=#{ f_{r} #} #in L^{2}( [0,#infty),L^{2}(S^{2}) )$. Luego esta funci#'on satisface la condici#'on #eqref{L2lc}:
#begin{equation}
#| F #|^{2}= #int_{0}^{#infty} #| f_{r} #|_{S^{2}}^{2} #, dr = #int_{0}^{#infty} #int_{0}^{#pi} #int_{0}^{2#pi} | f_{r}(#t,#phi) |^{2} #, #sen(#t) #, d#phi #, d#t #, dr < #infty
#end{equation}
de aqu#'i observamos que $f_{r}(#t,#phi)$, es una funci#'on de las variables $(r,#t,#phi)$, es decir, podemos escribir $f_{r}(#t,#phi)=f(r,#t,#phi)$, cuya integral en todo $#R^{3}$ es finita (es claro que, para que lo anterior tenga sentido $f$ debe de ser de la forma $#hat{f}/r$, para una cierta funci#'on medible $#hat{f}$).##
De esta forma, hemos mostrado que:
#begin{equation}
L^{2}( [0,#infty),L^{2}(S^{2}) ) = L^{2}(#R^{3})
#end{equation}
Mas adelante veremos que esta igualdad tambi#'en se puede dar en t#'erminos de un isomorfismo.##

Regresemos ahora al estudio de los espacios $L^{2}( #Lambda , #H_{0} )$ en general. Supongamos ahora que $#phi (#cdot)$ es una funci#'on compleja esencialmente acotada en $#Lambda$ (respecto a la medida de Lebesgue de $#Lambda$). Ahora hagamos,
#begin{equation}#label{opll}
L_{#phi} F= #{ #phi(#lambda) #, f_{#lambda} #}
#end{equation}
en donde $F= #{ f_{#lambda} #} #in L^{2}( #Lambda , #H_{0} ) $. Observamos que:
$$ #| L_{#phi} F #|^{2}= #int_{#Lambda} |#phi(#lambda)|^{2} #cdot #| f_{#lambda} #|_{0}^{2} #, d #lambda #leq #hbox{ess-sup}(|#phi|^{2})#, #|F#|^{2} $$

Por tanto, podemos concluir que $L_{#phi}$ definido en #eqref{opll}, pertenece al conjunto de operadores lineales acotados de $L^{2}(#Lambda , #H_{0})$. A este conjunto de operadores lo denotaremos como $B( L^{2}(#Lambda , #H_{0}) )$. Mas aun, es f#'acil ver que:
#begin{equation}
#| L_{#phi} #|= #hbox{ess-sup}_{#lambda #in #Lambda} |#phi (#lambda)| 
#end{equation}

De lo anterior podemos ver que se sugiere la siguiente generalizaci#'on. Para cada $#lambda #in #Lambda$ supongamos que $A(#lambda)$ es un operador que pertenece a $B(#H_{0})$ (el conjunto de operadores lineales acotados de $#H_{0}$). Vemos que esto define a una funci#'on $A: #Lambda #rar B(#H_{0})$ cuyos valores son operadores, la cual diremos que es medible si para cada $ F=#{ f_{#lambda} #} #in L^{2}(#Lambda ,#H_{0}) $, la funci#'on vectorial $#{ A(#lambda) f_{#lambda} #}$ es medible. ##
Luego, para esta funci#'on medible con valores en $B(#H_{0})$, definimos en el espacio de Hilbert $L^{2}(#Lambda , #H_{0})$ a un operador $A$, $A: D(A) #subset L^{2}(#Lambda , #H_{0}) #rar L^{2}(#Lambda , #H_{0}) $, cuyo dominio esta dado por:
#begin{equation}#label{A-dom}
D(A)= #left#{ F #in L^{2}(#l , #H_{0}) #, #Big| #, #int_{#Lambda} #| A(#lambda)f_{#lambda} #|_{0}^{2} #,#, d #lambda < #infty   #right#}
#end{equation}
el cual act#'ua como:
#begin{equation}#label{A-def}
AF= #{ A(#lambda) f_{#lambda} #}
#end{equation}

Para este tipo de operadores, la funci#'on $#| A(#lambda) #|_{0}$ es medible y no negativa. Mas aun, estos operadores tienen dominio denso y son cerrados [15].##
Un operador $A$, definido seg#'un #eqref{A-dom} y #eqref{A-def} es conocido como un {#bf operador de descomposici#'on}. Si el operador $A$ se puede escribir de la forma $A(#lambda)= #phi(#lambda) I_{0}$, en donde $#phi(#cdot)$ es una funci#'on compleja medible, finita en casi todo punto y en donde $I_{0}$ denota al operador identidad de $#H_{0}$, entonces diremos que $A$ es un {#bf operador diagonalizable}.##
Es f#'acil ver que un operador diagonalizable $A$, dado por una funci#'on $#phi$, es auto adjunto, si y solo si, la funci#'on $#phi$ es real [15].##

Com#'unmente a los operadores de descomposici#'on se les denota como:
#begin{equation}
A= #{ A(#lambda) #}
#end{equation}

Ahora, estableceremos dos proposiciones las cuales caracterizan a la clase de los operadores acotados de descomposici#'on. Para la demostraci#'on de estos resultados nos referimos a [15].##

#begin{proposition}
Un operador de descomposici#'on $A$, es acotado, si y solo si, $#| A(#lambda) #|_{0}$ es una funci#'on esencialmente acotada y ese caso:
#begin{equation}
#| A #| = #hbox{ess-sup}_{#lambda #in #Lambda} #| A(#lambda) #|_{0}
#end{equation}
#end{proposition}

#vspace{5 mm}

#begin{proposition}
Sean $A$ y $B$, dos operadores de descomposici#'on acotados en $L^{2}(#Lambda , #H_{0})$, dados por $A= #{ A(#lambda) #}$ y $B= #{ #lambda #}$. Entonces:
#begin{enumerate}
#item $A + B = #{ A(#lambda) + B(#lambda) #} #,#,;#,#, #alpha A = #{ #alpha A(#lambda) #}$.
#item $AB= #{ A(#lambda) B(#lambda) #}$.
#item $A^{*} = #{ A(#lambda)^{*} #}$.
#end{enumerate}
#end{proposition}

#vspace{5 mm}

Ahora, consideremos un operador diagonalizable en especial $A$, a saber, el cual en el que $#phi (#lambda)= #lambda$. as#'i, $A$ esta dado por:
#begin{equation}#label{A-l}
A= #{ #lambda I_{0} #}
#end{equation}
y ya que $#lambda #in #R$, es claro que $A$ es un operador auto adjunto. Definimos ahora al operador:
#begin{equation}
E_{#mu}= #{ #chi_{#mu}(#lambda)I_{0} #}
#end{equation}
en donde $#chi_{#mu}$ es la funci#'on caracter#'istica del conjunto $#Lambda_{#mu}= #Lambda #cap ( -#infty , #mu ]$, y es f#'acil ver que:
#begin{equation}
( F , E_{#mu}G )= #int_{#Lambda_{#mu}} ( f_{#lambda}, g_{#lambda} )_{0}#, d#lambda
#end{equation}
Luego, para toda $G #in D(A)$, 
#begin{equation}
(F,AG) = #int_{#Lambda} #lambda #, (f_{#lambda} , g_{#lambda})_{0}#, d#lambda = #int_{#Lambda} #lambda #, d(F,E_{#lambda} G)
#end{equation}
esto muestra que podemos asociar un operador auto adjunto del tipo dado por #eqref{A-l} (de hecho, una cierta clase de operadores que conmutan, a saber, los operadores diagonalizables) con el espacio $L^{2}(#Lambda , #H_{0})$. El reciproco de este enunciado tambi#'en es valido y es equivalente al teorema espectral [15].##

Todo operador de descomposici#'on $A= #{ A(#lambda) #}$, conmuta con todos los operadores $L_{#phi}$, definidos en #eqref{opll}. Esto se puede ver de la siguiente manera. Ya que $#phi$ es esencialmente acotada, tenemos que:
$$ #| AL_{#phi}G #|^{2}= #int_{#Lambda} |#phi(#lambda)|^{2} #, #| A(#lambda)g_{#lambda} #|_{0}^{2} #, d#lambda #leq #hbox{ess-sup}_{#lambda #in #Lambda}(|#phi(#lambda)|^{2}) #, #int_{#Lambda} #| A(#lambda)g_{#lambda} #|_{0}^{2}#, d #lambda < #infty $$
para toda $G #in D(A)$. as#'i tambi#'en, para estas mismas funciones $G$, tenemos:
$$ AL_{#phi} G = #{ A(#lambda)(L_{#phi}g)_{#lambda} #}= #{ #phi(#lambda) #, A(#lambda) g_{#lambda} #}= L_{#phi} A G  $$
lo cual implica que $ L_{#phi}A #subset A L_{#phi} $. El reciproco de esta propiedad es el contenido de la siguiente proposici#'on, la cual tiene una importancia relevante en la teor#'ia de dispersi#'on.

#begin{proposition}#label{prop11}
Sea $B$ un operador acotado en $L^{2}(#Lambda , #H_{0})$ que conmuta con el operador auto adjunto $A$, dado por #eqref{A-l}. Entonces $B$ es un operador de descomposici#'on, dado por $B=#{ B(#lambda) #}$, con norma:
#begin{equation}
#| B #|= #hbox{ess-sup}_{#lambda #in #Lambda} #| B(#lambda) #| < #infty
#end{equation}
#end{proposition}

#vspace{5 mm}

La demostraci#'on de esta proposici#'on (y del siguiente corolario) requiere de una teor#'ia mas elaborada del an#'alisis funcional, por lo que nos referimos a [15] para su demostraci#'on.

#begin{corollary}#label{coll11}
Un operador de descomposici#'on acotado $B=#{ B(#lambda) #}$ es #emph{invertible acotado} (auto adjunto, unitario, parcialmente isom#'etrico respectivamente), si y solo si, $B(#lambda)$ es #emph{invertible acotado} (auto adjunto, unitario, parcialmente isom#'etrico respectivamente), para casi toda $#lambda #in #Lambda$.
#end{corollary}

#vspace{5 mm}

Regresemos ahora al caso concreto en el que est#'abamos trabajando con $L^{2}(#R^{3})$ en su representaci#'on como $L^{2}(#Lambda , #H_{0})$, solo que en esta ocasi#'on en la representaci#'on del espacio de momentos (es decir, en lugar de trabajar con la funciones $f(#x)$, $#x #in #R^{3}$ como lo hicimos anteriormente, trabajaremos con sus transformadas de Fourier $#hat{f}(#k)$, $#k #in #R^{3}$). Haciendo $#lambda = |#k|^{2}=k^{2}$. Ahora para cada funci#'on $f #in L^{2}(#R^{3})$ y para casi toda $#lambda #in [ 0 , #infty )$ definimos al vector $f_{#lambda} #in L^{2}(S^{2})$ como:
#begin{equation}#label{g0}
f_{#lambda}( #t , #phi )= #frac{#lambda^{1/4}}{#sqrt{2}}#, #hat{f} ( #lambda^{1/2},#t ,#phi ) 
#end{equation}

Luego, por el teorema #ref{FPt}, pasando a coordenadas esf#'ericas y haciendo el cambio de variable $ k #rar #lambda^{1/2} $, tenemos que:
$$ #| f #|^{2} = #| #hat{f} #|^{2}= #int_{0}^{#infty} #int_{S^{2}} k^{2} #,| #hat{f}(k,#t,#phi) |^{2} #, dk #, #sen (#t) #, d#t #, d #phi $$
$$ = #int_{0}^{#infty} #int_{S^{2}} #lambda #, |#hat{f}( #lambda^{1/2} , #t , #phi ) |^{2} #, #frac{1}{2}#, #lambda^{-1/2} #, d #lambda #, #sen (#t) #, d #t #, d #phi $$
#begin{equation}#label{nor-g0}
= #int_{0}^{#infty} #int_{S^{2}} #left| #lambda^{1/4} #cdot #sqrt{2} #cdot #hat{f}(#lambda^{1/2},#t , #phi ) #right|^{2} #, #sen(#t) #, d#t #, d#phi #, d#lambda = #int_{0}^{#infty} #| f_{#lambda} #|_{L^{2}(S^{2})}^{2} #, d#lambda
#end{equation}

Supongamos ahora que $G_{0}$ es el espacio de Hilbert definido por #eqref{g0} y #eqref{nor-g0}. Es claro que,
#begin{equation}#label{G0}
G_{0}= L^{2} #left( [0 , #infty),L^{2}(S^{2}) #right)
#end{equation}
y que $L^{2}(#R^{3})$ es unitariamente isomorfo a $G_{0}$, con el isomorfismo $#U_{0}$, dado por:
#begin{equation}
( #U_{0} f )_{#lambda} (#ol{#omega})= 2^{-1/2} #cdot #lambda^{1/4} #cdot #hat{f} ( #lambda^{1/2} #, #ol{#omega} ) #,#,,#,#,#, #ol{#omega} #in S^{2}
#end{equation}
Recordando la definici#'on del Hamiltoniano de una part#'icula libre $H_{0}$ #eqref{H0} y utilizando la propiedad #eqref{f-dern} de la transformada de Fourier de la derivada de una funci#'on, tenemos que para toda $#psi #in D(H_{0})$:
#begin{equation}#label{UHP}
(#U_{0} H_{0} #psi )_{#lambda}= #lambda #, ( #U_{0} #psi )_{#lambda}
#end{equation}
o de forma equivalente, $ #U_{0} #, H_{0} #, #U_{0}^{-1}= #{ #lambda #, I_{0} #} $ (con $I_{0}$ el operador identidad de $L^{2}(S^{2})$). La representaci#'on #eqref{G0} de $L^{2}(#R^{3})$ es conocida como la {#bf representaci#'on espectral} del espacio de Hilbert $L^{2}(#R^{3})$ relativa a $H_{0}$, o simplemente coma la {#bf representaci#'on espectral} de $H_{0}$. Para simplificar la notaci#'on, usualmente se identifica a $f #in L^{2}(#R^{3})$ con $ #U_{0} f #in G_{0} $ y en lugar de #eqref{UHP}, se escribe $(H_{0} #psi)_{#lambda}= #lambda #, #psi_{#lambda} $.##

De la secci#'on anterior, sabemos que el operador de dispersi#'on $#S$ definido en la ecuaci#'on #eqref{op-dis}, conmuta con el operador $H_{0} #, E_{#infty}(H_{0})$. Cuando $H_{0}$ es el Hamiltoniano libre, tenemos que $E_{#infty}(H_{0})= I$, y por tanto en este caso, $#S$ conmuta con $H_{0}$. De esta forma por la proposici#'on #ref{prop11} y el corolario #ref{coll11}, concluimos que $#S$ es un operador de descomposici#'on acotado, en la representaci#'on #eqref{G0}  de $L^{2}(#R^{3})$. Lo anterior quiere decir que:
#begin{equation}#label{s-esp-des}
#U_{0}#, #S #, #U_{0}^{-1} = #{ #S (#lambda) #} #,,#,#,#, #hbox{ y que } #,#, #S(#lambda)^{*}#, #S (#lambda)= I_{0}
#end{equation}
para casi toda $#lambda #in [0,#infty)$, en donde $I_{0}$ es el operador identidad de $L^{2}(S^{2})$.##
Si $#S$ es unitario, tambi#'en lo es $#S(#lambda)$ para casi toda $#lambda #in [0,#infty)$. Al operador $#S(#lambda)$ se le conoce frecuentemente como la matriz $#S$ a energ#'ia $#lambda$.##

Hasta el momento toda la teor#'ia de dispersi#'on que hemos desarrollado es abstracta y muy general, en el sentido que esta solo presupone el conocimiento de dos grupos unitarios uniparametricos y la existencia de ciertos limites en un espacio de Hilbert, a saber los que definen a lo operadores de onda $#O_{#pm}$. No hemos usado ning#'un tipo de propiedad espectral del Hamiltoniano no perturbado y los operadores de posici#'on y momento, los cuales son los observables b#'asicos de la mec#'anica cu#'antica, no han jugado ning#'un papel en ninguno de nuestros argumentos. Sin embargo, al observar detalladamente la hip#'otesis asint#'otica (que es la condici#'on que cumplen todos los sistemas de dispersi#'on que estamos considerando) vemos que no tiene ning#'un sentido el preguntarnos algo acerca de estos observables, ya que con los estados con los que en verdad estamos trabajando, gracias al operador de dispersi#'on $#S$, son $#psi_{-}$ y $#psi_{+}$ y no $#Psi$ (ver la expresi#'on #eqref{cas-1}). La evoluci#'on de el estado $#psi_{-}$ esta dada por:
$$ e^{-iH_{0}t} #psi_{-} $$ 
y esta preparado de tal forma que para tiempos en el pasado remoto el valor esperado de la posici#'on es tan grande que el potencial $V(#x)$ es despresiable y no tiene ningun efecto sobre su evoluci#'on. Luego el estado $#psi_{+}$ esta dado por la acci#'on del operador de dispersi#'on sobre el estado inicial $#psi_{-}$:
$$ #psi_{+}= #S #psi_{-} $$
de nueva cuenta, la evoluci#'on de $#psi_{+}$ esta dada por:
$$ e^{-iH_{0}t} #psi_{+} $$ 
y es tal que, para tiempos en el futuro lejano el valor esperado de la posici#'on vuelve a ser muy grande, de tal forma que el potencial es despresiable y no tiene ningun efecto en su evoluci#'on temporal.## 

Luego ya que el estado $#Psi$ evoluciona asintoticamente a los estados $#psi_{#pm}$ cuando $t #rar #pm #infty$ respectivamente, observamos que solo para tiempos en estos limites los valores esperados de la posici#'on del estado $#Psi$ se aproximaran a los de los estados asintoticos $#psi_{#pm}$.##

Por otro lado, ya que estamos trabajando con sistemas de dispersi#'on el#'astica, es de esperarse que el valor esperado del momento del sistema sea una cantidad constante en el tiempo.##
Sin embargo esto no marca el fin de la informaci#'on que podamos extraer de un sistema de dispersi#'on, ya que como veremos mas adelante (y tal como es el prop#'osito de este texto) una cantidad de inter#'es para los sistemas de dispersi#'on que involucran mas de una part#'icula (en nuestro caso dos) es la #emph{pureza} (ec. #eqref{pur-ps}) que estos poseen, antes del proceso de dispersi#'on (como siempre en el pasado lejano, $t #rar -#infty$) y despu#'es de este (en el futuro remoto $t #rar #infty$).##
Para hacer esto, primero debemos de estudiar la descripci#'on que tiene la teor#'ia de dispersi#'on que hemos desarrollado hasta ahora, en el caso particular en el que nuestro sistema dispersor esta compuesto de dos part#'iculas (sistema binario) tales como los que hemos estudiado en secciones anteriores.

#newpage

#chapter{Dispersi#'on de dos part#'iculas}
El objetivo principal de este capitulo es el de mostrar que el problema de dispersi#'on para un sistema formado por dos part#'iculas cu#'anticas, se puede reducir al problema de la dispersi#'on de una sola part#'icula en un campo externo, bajo la suposici#'on de que la interacci#'on entre los dos subsistemas es invariante bajo traslaciones del sistema completo. Un ejemplo de tal interacci#'on, es la de un potencial $V$ que depende solamente de la distancia relativa $#x_{1} - #x_{2}$ de los subsistemas, del cual ya hemos hecho menci#'on en el capitulo de sistemas binarios.##

Supongamos que $#H_{1}$ y $#H_{2}$ son los espacios de Hilbert de los estados de la primera y de la segunda part#'icula respectivamente. Si las dos part#'iculas son distinguibles, entonces, el espacio de Hilbert $#H$ que describe los estados del sistema compuesto, es el producto tensorial de $#H_{1}$ y $#H_{2}$, es decir,  $#H=#H_{1} #pr #H_{2}$.##

En caso de que tengamos dos part#'iculas id#'enticas, $#H$ es el subespacio sim#'etrico o anti sim#'etrico de $ #H_{1} #pr #H_{2} $, seg#'un el tipo de part#'iculas con las que estemos trabajando, bosones o fermiones, respectivamente. El subespacio sim#'etrico (respectivamente el subespacio anti sim#'etrico) de $#H$ es aquel que generado por el conjunto de todos los vectores de la forma $ #psi_{1} #pr #psi_{2} + #psi_{2} #pr #psi_{1} $ (respectivamente $#psi_{1} #pr #psi_{2} - #psi_{2} #pr #psi_{1}$), con $#psi_{1},#psi_{2} #in #H_{1}$. Notemos que en este caso, necesariamente tenemos que $ #H_{1}=#H_{2} $.##

Con el objetivo de ser concretos, solo discutiremos el caso de dos part#'iculas distinguibles, no relativistas, sin spin, de masas $m_{1}$ y $m_{2}$.##

El espacio de Hilbert del sistema compuesto esta dado por $#H= L^{2}(#R^{3}) #pr L^{2}(#R^{3}) $, el cual como hemos visto anteriormente, lo podemos identificar con el espacio $L^{2}(#R^{6})$.##
De esta forma, escribimos $#x_{1}= ( x_{1,1}, x_{1,2},x_{1,3} )$ como la variable del primer espacio $L^{2}(#R^{3})$ y $#x_{2}= ( x_{2,1}, x_{2,2},x_{2,3} )$ como la variable del segundo espacio, las cuales definen a las variables del espacio $#H$. De forma similar, a las variables de la transformada de Fourier de las funciones en $#H$ (respecto a $#x_{1},#x_{2}$), las denotaremos por $#k_{1}$ y $#k_{2}$.##

El Hamiltoniano libre $#H_{0}$, es la suma de los Hamiltonianos libres de los subsistemas $ H_{0,1} #pr #1 $ y $#1 #pr #H_{0,2}$ de cada part#'icula:
$$ D(#H_{0})= #left#{ #Psi #in L^{2}(#R^{6}) #,#Big|#,#, #k_{1}^{2} #hat{#Psi}(#k_{1},#k_{2}) #in L^{2}(#R^{6}) #,#,;#,#, #k_{2}^{2} #hat{#Psi}(#k_{1},#k_{2}) #in L^{2}(#R^{6})   #right#} $$
y
#begin{equation}
( #F #H_{0} #Psi )( #k_{1},#k_{2} )= #left( #frac{#k_{1}^{2}}{2m_{1}} + #frac{#k_{2}^{2}}{2m_{2}} #right) #, #hat{#Psi}(#k_{1},#k_{2}) 
#end{equation}

Luego, como hemos visto anteriormente, el grupo de evoluci#'on libre $U_{t}= #exp( -i H_{0}t )$, se factoriza como:
#begin{equation}#label{ev-g}
U_{t}= U_{t}^{(1)} #pr U_{t}^{(2)}
#end{equation}
en donde $U_{t}^{(s)} = #exp ( -iH_{0,s}t ) $ el cual act#'ua sobre $L^{2}(#R^{3})$, $s=1,2$. La ecuaci#'on #eqref{ev-g} expresa el hecho de que las part#'iculas se mueven independientemente; si el estado inicial del sistema es un estado producto $#Psi= #psi_{1} #pr #psi_{2}$, entonces es claro que $U_{t} #Psi$ continua siendo un estado producto para todo tiempo.##

A diferencia de como hemos trabajado en las secciones anteriores, en esta ocasi#'on desarrollaremos los siguientes conceptos en el espacio de momentos de nuestro sistema de dos part#'iculas.##

Tal y como lo hicimos en el capitulo de sistemas binarios, introducimos a la masa total $M=m_{1}+m_{2}$ y a la masa reducida $m= m_{1} #cdot m_{2}/M $ (ecs. #eqref{Mt-mr}). Ahora introducimos los observables de momento total $#P_{tot}$ y de momento relativo $#P_{rel}$, dados por los siguientes operadores:
#begin{equation}
#P_{tot}= #P_{1}+#P_{2} #,#,;#,#,#,#,#, #P_{rel} = #frac{ m_{2}#P_{1} - m_{1}#P_{2} }{M}
#end{equation}

A trav#'es de la identidad:
$$ #frac{1}{2M} ( #k_{1}+#k_{2} )^{2} + #frac{1}{2m M^{2}} ( m_{2}#k_{1} - m_{1}#k_{2} )^{2}= #frac{#k_{1}^{2}}{2m_{1}} + #frac{ #k_{2}^{2} }{2m_{2}} $$
observamos que podemos escribir al Hamiltoniano del sistema compuesto $H_{0}$, en la representaci#'on de momentos, como:
#begin{equation}#label{H-cm-rel}
H_{0}= H_{0,CM} + H_{0,rel}
#end{equation}
en donde $H_{0,CM}$ y $H_{0,rel}$ est#'an dados por los operadores de multiplicaci#'on:
#begin{equation}
H_{0,CM}= #frac{ #k_{1}^{2} + #k_{2}^{2} }{2M} #,#,;#,#,#,#,#, H_{0,rel}= #frac{( m_{2}#k_{1} - m_{1}#k_{2} )^{2}}{ 2 m M^{2} } 
#end{equation}
sobre las transformadas de Fourier de las funciones de $L^{2}(#R^{6})$, ya que estamos en la representaci#'on de momentos.##

Ahora, introducimos en el espacio de momentos a las variables:
#begin{equation}#label{K-cm-rel}
#k_{CM}= #k_{1}+#k_{2} #,#,;#,#,#,#,#,#, #k_{rel}= #frac{m_{2}#k_{1}-m_{1}#k_{2}}{M}
#end{equation}
en donde nos referimos a $#k_{CM}$ y a $#k_{rel}$, como el #emph{momento del centro de masa} y el #emph{momento relativo} respectivamente, en analog#'ia con los sistemas cl#'asicos [17]. Mas aun, el jacobiano de la transformaci#'on $ #k_{1},#k_{2} #rar #k_{CM},#k_{rel} $, es uno. De esta forma, podemos escribir el producto escalar de $L^{2}(#R^{6})$ directamente en estas nuevas variables:
$$ (#psi,#phi)= ( #hat{#psi},#hat{#phi} ) =#int_{#R^{6}} #ol{#hat{#psi}}(#k_{1},#k_{2}) #, #hat{#phi}(#k_{1},#k_{2})#,#, d#k_{1}#, d#k_{2} $$
$$ = #int_{#R^{6}} #ol{#hat{#psi}}(#k_{CM},#k_{rel}) #, #hat{#phi}(#k_{CM},#k_{rel})#,#, d#k_{CM}#, d#k_{rel} $$

Lo anterior significa que podemos identificar a $H=L^{2}(#R^{6})$ con $ L^{2}(#R^{3}) #pr L^{2}(#R^{3}) $ de una forma distinta: Cuando tomemos las transformadas de Fourier de funciones en esta nueva descomposici#'on de $H$, la variable en el primer factor deber#'a de ser $#k_{CM}$ y en el segundo factor deber#'a de ser $#k_{rel}$. En el resto del desarrollo de este capitulo, todas las descomposiciones de operadores en forma  productos tensoriales, ser#'an respecto a esta ultima estructura tensorial de $H$ (es decir, estas descomposiciones ser#'an con respecto a $#k_{CM}$ y a $#k_{rel}$). En particular los Hamiltonianos libres los escribiremos como:
#begin{equation}
H_{0,CM}= H_{0,CM} #pr I #,#,#,;#,#,#,#,#, H_{0,rel}= I #pr H_{0,rel}
#end{equation}

Ahora, para determinar las variables de posici#'on en esta nueva factorizaci#'on de $H$, debemos de tomar en cuenta el cambio de variables anterior ($#k_{1},#k_{2} #rar #k_{CM},#k_{rel}$) en la transformada inversa de Fourier, ya que si realizamos este procedimiento directamente no vamos a obtener una funci#'on de las variables $#x_{1},#x_{2}$ . Sin embargo, haciendo el cambio de coordenadas $ #x_{1},#x_{2} #rar #xx , #x $ en el espacio de posiciones, en donde $#xx$ y $#x$ es la coordenada del centro de masa y la coordenada relativa, respectivamente (ec. #eqref{cm-cr}), y dado que el jacobiano de esta transformaci#'on tambi#'en es uno, es f#'acil ver que:
$$ #Psi(#x_{1},#x_{2})=#F^{-1} ( #hat{#Psi} ) (#x_{1},#x_{2}) = #frac{1}{(2 #pi)^{3}} #int_{#R^{6}} #exp #left( i #k_{1} #cdot #x_{1} + i #k_{2} #cdot #x_{2} #right) #, #hat{#Psi}( #k_{1},#k_{2} ) #,#, d#k_{1} #, d#k_{2} $$
#begin{equation}
=#frac{1}{(2 #pi)^{3}} #int_{#R^{6}} #exp #left( i #k_{CM} #cdot #xx + i #k_{rel} #cdot #x #right) #, #hat{#Psi}( #k_{CM},#k_{rel} ) #,#, d#k_{CM} #, d#k_{rel} =#F_{CM,r}^{-1}(#hat{#Psi})( #xx , #x ) =#Psi( #xx , #x )
#end{equation}
En donde $#F_{CM,r}$, denota la transformada de Fourier respecto a al sistema de coordenadas $(#xx,#x)$ y $(#k_{CM},#k_{rel})$.##

Debemos observar que la ecuaci#'on #eqref{H-cm-rel} implica que el grupo de evoluci#'on libre $U_{t}$, tambi#'en se factoriza en la nueva descomposici#'on de $#H$ como un producto tensorial:
#begin{equation}
U_{t}= U_{t,CM} #pr U_{t,rel}
#end{equation}
en donde,
#begin{equation}
U_{t,CM} = #exp (-i H_{0,CM} t) #,#,;#,#,#,#,#,#, U_{t,rel}= #exp (-i H_{0,rel} t)
#end{equation}
act#'uan en su respectivo factor de $L^{2}(#R^{3})$. Por tanto, bajo la evoluci#'on libre el movimiento del centro de masa y el movimiento relativo son independientes. Comparando esto con la ecuaci#'on #eqref{ev-g}, observamos que tambi#'en podemos considerar que nuestro sistema esta formado por dos part#'iculas ficticias, no relativistas y sin spin, de masas $M$ y $m$ respectivamente.##

Por otro lado, supongamos ahora que el grupo total de evoluci#'on $V_{t}$, se puede factorizar de la forma:
#begin{equation}#label{tot-evv}
V_{t}= U_{t,CM} #pr V_{t,rel}
#end{equation}
derivando esta ecuaci#'on respecto de $t$ y por el teorema de Stone (teorema #ref{stone}), observamos que el Hamiltoniano total $H$, debe de ser una extensi#'on del operador $ H_{0,CM} #pr I + I #pr H_{rel} $. Rec#'iprocamente, para que la ecuaci#'on #eqref{tot-evv} se cumpla, es suficiente que la diferencia $H-H_{0}$, es decir el factor de interacci#'on, actu#'e de manera no trivial solamente en el espacio de la coordenada relativa (respectivamente en el espacio del momento relativo) de nuestro sistema de dos part#'iculas ($#x$, $#k_{rel}$).## 

Si los operadores de onda existen, entonces por las relaciones #eqref{opp-rel2} tenemos que:
#begin{equation}#label{om-sep}
#O_{#pm} = #slim_{t #rar #pm #infty} ( U_{t,CM}^{*} #, U_{t,CM} #pr V_{t,rel}^{*}#, U_{t,rel} )= I #pr #O_{#pm,rel}
#end{equation}

Aqu#'i introdujimos a los operadores de onda relativos, los cuales act#'uan sobre el espacio de Hilbert relativo $H_{rel}$:
#begin{equation}#label{wav-rel}
#O_{#pm,rel}= #slim_{t #rar #pm #infty} V_{t,rel}^{*} #, U_{t,rel}
#end{equation}

as#'i tambi#'en, tenemos que:
#begin{equation}#label{s-sep}
#S= #O_{+}^{*}#, #O_{-} = I #pr #O_{+,rel}^{*}#, #O_{-,rel} = I #pr #S_{rel}
#end{equation}
Sin embargo, debemos de tomar en cuenta que los operadores #eqref{om-sep} y #eqref{s-sep}, est#'an definidos para actuar en el espacio de posiciones $(#xx , #x)$. En especial estamos interesados en el operador de dispersi#'on $#S$, por lo que hay que definir como act#'ua este en el espacio de momentos $(#k_{CM} , #k_{rel})$. Pero ya que la transformada de Fourier es una transformaci#'on unitaria en $L^{2}(#R^{6})$, tenemos que el operador de dispersi#'on en el espacio de momentos esta dado por:
$$ #hat{#S}= #F_{CM,r} #, #S #, #F_{CM,r}^{-1} = #F_{CM,r} #, (I #pr #S_{rel}) #, #F_{CM,r}^{-1} $$
#begin{equation}#label{sp-rell}
= I #pr ( #F_{r} #, #S_{rel} #, #F_{r}^{-1} ) = I #pr #hat{#S}_{rel}
#end{equation}
en donde $#F_{r}$ es la transformada de Fourier respecto a las coordenadas relativas $#x , #k_{rel}$.
#begin{equation}
#F_{r}(#psi)(#k_{rel}) =#frac{1}{(2 #pi)^{3/2}} #int_{#R^{3}} #exp( -i#k_{rel} #cdot #x )#, #psi (#x) #, #, d#x
#end{equation}

#begin{equation}
#F_{r}^{-1}( #hat{#psi} )(#x) =#frac{1}{(2 #pi)^{3/2}} #int_{#R^{3}} #exp( i#k_{rel} #cdot #x )#, #hat{#psi} (#k_{rel}) #, #, d#k_{rel}
#end{equation}

Lo anterior muestra que bajo la hip#'otesis #eqref{tot-evv}, basta con solo resolver el problema de dispersi#'on en el espacio $L^{2}(#R^{3})$ que describe el movimiento relativo de nuestro sistema de part#'iculas. Observamos que este problema de dispersi#'on, es el mismo que el de una part#'icula no relativista, sin spin, de masa $m$ (la masa reducida), con interacci#'on $V$. Si $V$ es un potencial que depende solamente de la distancia $#x_{1} - #x_{2}$ de las dos part#'iculas originales del sistema, el problema de dispersi#'on se reduce a un problema #emph{dispersi#'on potencial}, ya que el potencial en el sistema de referencia relativo claramente esta dado por $V(#x)$.## 

Mas aun, ya que hemos reducido el problema al de una sola part#'icula, por la ecuaci#'on #eqref{s-esp-des} podemos dar una representaci#'on espectral (matriz de dispersi#'on) del operador de dispersi#'on $#hat{#S}$:
#begin{equation}#label{sp-mat}
#hat{#S}= #{ #S(k_{rel}^{2}) #}
#end{equation}
en donde $k_{rel}^{2} =|#k_{rel}|^{2}$.##

La eliminaci#'on anterior del movimiento del centro de masa de las dos part#'iculas, es interpretada frecuentemente como el hacer que el momento total del sistema sea cero o como el escoger un marco de referencial el cual se mueve junto con el sistema de referencia del centro de masa. En analog#'ia con los sistemas de dispersi#'on cl#'asicos, al sistema de coordenadas $(#x_{1},#x_{2})$ se le suele conocer como el sistema de referencia del laboratorio, mientras que al sistema de coordenadas $(#xx , #x )$ se le conoce como el sistema de referencia del centro de masa.

#newpage

#chapter{Dispersi#'on a bajas energ#'ias}

#section{Cambio a unidades f#'isicas}
Ahora entramos al tema principal de este texto, el cual es el desarrollo de una expresi#'on para la pureza de un estado cu#'antico asint#'otico de un sistema dispersor formado por dos part#'iculas sin spin, de masas $m_{1}$ y $m_{2}$ respectivamente, cuando la dispersi#'on entre estas ocurre a bajas energ#'ias.##
Para hacer esto no solo en t#'erminos matem#'aticos sino tambi#'en en t#'erminos f#'isicos, recordemos que en el capitulo 5, introdujimos los cambios de variable:
#begin{equation}#label{camb1}
#x #rar #x #sqrt{#frac{#hbar}{m}} #,#,#, ; #,#,#, #P #rar #sqrt{#hbar m}#, #k
#end{equation}
con el objetivo de deshacernos de las constantes f#'isicas $m$ y $#hbar$ para simplificar la notaci#'on en los desarrollos matem#'aticos posteriores. Con el cambio de variables anterior introdujimos la ecuaci#'on general de Schr#"odinger dependiente del tiempo:
$$ i #, #frac{#partial}{#partial t}#psi = H#psi $$
en donde $H$ es el operador cu#'antico Hamiltoniano:
$$ H #psi=- #frac{1}{2} #Delta #psi + V(#x) #, #psi $$
tambi#'en con estas variables introdujimos al operador de evoluci#'on temporal:
$$ U(t) = e^{-iHt} $$
a la transformada de Fourier para funciones integrables como:
$$ #hat{#psi}(#k) = #F#psi(#k)=#frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{-i #k #cdot #x} #, #psi(#x)#, d#x $$
y a la transformada inversa de Fourier:
$$ #psi(#x) =  #F^{-1}#hat{#psi}(#x) =#frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{i #k #cdot #x} #, #hat{#psi}(#k)#, d#k $$

Ahora, para recuperar el sentido f#'isico de los conceptos expresados en las ecuaciones anteriores, es necesario hacer el cambio de variables contrario a #eqref{camb1}:
$$ #x #rar #x #sqrt{#frac{m}{#hbar}} #,#,#, ; #,#,#, #k #rar #frac{#P}{#sqrt{#hbar m}} $$

En estas variables (las cuales son las variables f#'isicas originales) la ecuaci#'on general de Schr#"odinger dependiente del tiempo esta dada por:
$$ i#hbar #, #frac{#partial}{#partial t} #psi = H #psi  $$
con el operador Hamiltoniano:
$$ H #psi = -#frac{#hbar^{2}}{2m} #Delta #psi + V(#x) #psi $$
aqu#'i debemos hacer notar que $-#frac{#hbar^{2}}{2m} #Delta #psi$ es el operador de energ#'ia cin#'etica en el espacio de posiciones. as#'i tambi#'en, los operadores:
#begin{equation}#label{mom-op}
#P_{i}#, #psi=  -i#hbar #frac{#partial }{#partial x_{i}} #, #psi #,#,;#, i=1,#ldots n
#end{equation}
son los operadores de momento en la direcci#'on de la coordenada $x_{i}$ en el espacio de posiciones.##

as#'i tambi#'en, en estas variables el operador de evoluci#'on esta dado por:
$$ U(t)= e^{-i #frac{t}{#hbar}H} $$ 
La transformada de Fourier y su inversa, para funciones integrables, est#'an dadas por:
$$ #hat{#psi}(#P) = #F#psi(#P)=#frac{1}{(2#pi #hbar)^{n/2}} #int_{#R^{n}} e^{-#frac{i}{#hbar}#,#P #cdot #x } #, #psi(#x)#, d#x $$

$$ #psi(#x) =  #F^{-1}#hat{#psi}(#x) =#frac{1}{(2#pi)^{n/2}} #int_{#R^{n}} e^{#frac{i}{#hbar}#,#P #cdot #x} #, #hat{#psi}(#P)#, d#P $$

En donde, $#F$ nos lleva del espacio de posiciones $#x$ al espacio de momentos $#P$ mientras que $#F^{-1}$ hace lo contrario.##
En el espacio de momentos el operador de posici#'on $#x_{i}$ esta dado por:
#begin{equation}#label{pos-op}
#x_{i} = i#hbar #, #frac{#partial}{#partial p_{i}} #hat{#psi} #,#,;#, i=1,#ldots n
#end{equation}
mientras que en el espacio de posiciones, este operador sigue siendo simplemente la multiplicaci#'on por la variable $#x_{i}$. Lo mismo sucede en el espacio de momentos en el que el operador $#P_{i}$ definido en #eqref{mom-op} en el espacio de posiciones, act#'ua simplemente como la multiplicaci#'on por la variable independiente $#P_{i}$.

Todo lo anterior es claramente valido para una sola part#'icula que se mueve en $n$ dimensiones. En el caso de un sistema binario, formado por part#'iculas de masas $m_{1}$ y $m_{2}$, vimos que la ecuaci#'on de Schr#"odinger esta dada por la expresi#'on #eqref{ec-2p}:
$$ i #frac{#partial}{ #partial t } #psi (#x_{1} , #x_{2},t)= - #left( #frac{1}{2m_{1}} #Delta_{1} + #frac{1}{2m_{2}} #Delta_{2} #right) #, #psi (#x_{1},#x_{2},t)  $$
la cual esta escrita en una escala en la cual $#hbar =1$. Haciendo un re escalamiento a unidades en las cuales $#hbar= 1.0546 #cdot 10^{-34} #, J #cdot s$ y por la ecuaci#'on #eqref{Ham-2p-hb} tenemos que la ecuaci#'on de Schr#"odinger para un sistema binario, en estas nuevas unidades, esta dada por:
$$ i#hbar #frac{#partial}{#partial t} #psi(#x_{1},#x_{2})= H#psi(#x_{1},#x_{2}) $$
con $H$ el operador Hamiltoniano, el cual act#'ua como:
#begin{equation}#label{ham}
H #psi =H_{0} #psi + V(#x_{1} , #x_{2}) #psi
#end{equation}
en donde $H_{0}$ es el #emph{Hamiltoniano libre}, el cual esta dado por:
#begin{equation}#label{H-lib}
H_{0}#psi=-#frac{#hbar^{2}}{2m_{1}} #Delta_{1}#psi - #frac{#hbar^{2}}{2m_{2}} #Delta_{2} #psi
#end{equation}
$#Delta_{1}$ y $#Delta_{2}$ denotan al operador Laplaciano, el primero act#'ua sobre la variable vectorial $#x_{1}$ y mientras que el segundo act#'ua sobre $#x_{2}$. De acuerdo con los conceptos de teor#'ia de dispersi#'on potencial que desarrollamos en el capitulo 14, nos referiremos a $H$ como el #emph{Hamiltoniano perturbado} ya que este si toma en cuenta al potencial $V$ el cual causa la interacci#'on entre las part#'iculas de nuestro sistema binario.##
Para estos sistemas, en estas unidades, el operador de evoluci#'on esta dado por:
$$ V(t)= e^{-i #frac{i}{#hbar} H}  $$
En el caso de un sistema binario, el operador de momento en la direcci#'on $i$ para la part#'icula $j$ ($i=1,#ldots,n$ y $j=1,2,$) en el espacio de posiciones esta dado por:
$$ #P_{i}^{j}#, #psi=  -i#hbar #frac{#partial }{#partial x_{i}^{j}} #, #psi   $$

as#'i tambi#'en, en estas nuevas unidades introducimos a la #emph{coordenada del centro de masa} $#xx$ y a la #emph{coordenada relativa de la posici#'on} $#x$, tal y como lo hicimos en #eqref{cm-cr}:
#begin{equation}#label{cm-cr-h}
#xx = #frac{ m_{1}#x_{1} + m_{2}#x_{2} }{m_{1}+m_{2}} #,#,#,;#,#,#,#,#,#,#,#,#,#, #x= #x_{1} - #x_{2}
#end{equation}
en analog#'ia con los sistemas cl#'asicos de dispersi#'on [17] y por la regla de substituci#'on.## 
Como vimos en el capitulo 10, con estas coordenadas podemos factorizar al espacio de Hilbert del sistema compuesto como:
$$ #H= #H_{cm} #pr #H_{rel} $$
y en este caso con $#H_{cm}=#H_{rel}=#R^{n}$, en donde $#H_{cm}$ y $#H_{rel}$ son los espacio de Hilbert que describen el movimiento del centro de masa y de la coordenada relativa respectivamente.##

Vamos a estudiar sistemas dispersores formados por dos part#'iculas sin spin, como los de el capitulo 16, en los que el potencial es una funci#'on de la posici#'on relativa $#x= #x_{1} - #x_{2} $. as#'i la ecuaci#'on de Schr#"odinger en la que centraremos nuestra discusi#'on es de la forma:
$$ i#hbar#, #frac{#partial}{#partial t} #psi= H#psi =H_{0}#psi + V(#x_{1} - #x_{2}) #psi $$
#begin{equation}#label{shc-x12}
=-#frac{#hbar^{2}}{2m_{1}} #Delta_{1}#psi - #frac{#hbar^{2}}{2m_{2}} #Delta_{2} #psi + V(#x_{1} - #x_{2}) #psi
#end{equation}
o haciendo el cambio al sistema de coordenadas $(#xx,#x)$,
#begin{equation}#label{sch-2p-h}
i#hbar #, #frac{#partial}{#partial t} #psi = -#frac{#hbar^{2}}{2M} #Delta_{#xx} #psi - #frac{#hbar^{2}}{2 m} #Delta_{#x} #psi + V(#x) #psi
#end{equation}
cuyas soluciones pueden ser de la forma,
#begin{equation}#label{sol-h-2}
#psi(#xx,#x,t) = #Psi(#xx , t) #, #psi_{rel}(#x,t)
#end{equation}
en donde:
$$ i#hbar #, #frac{#partial}{#partial t} #Psi = -#frac{#hbar^{2}}{2M} #Delta_{#xx} #Psi $$
y
#begin{equation}#label{h-rel}
i#hbar #, #frac{#partial}{#partial t} #psi_{rel}= - #frac{#hbar^{2}}{2 m} #Delta_{#x} #psi_{rel} + V(#x) #psi_{rel}
#end{equation}
o por la linealidad de la ecuaci#'on #eqref{sch-2p-h} la soluci#'on puede ser cualquier combinaci#'on lineal arbitraria de soluciones de la forma #eqref{sol-h-2}.##
De la ecuaci#'on #eqref{h-rel}, extraemos las definiciones del Hamiltoniano relativo libre $H_{0,rel}$ y  del Hamiltoniano relativo perturbado $H_{rel}$:
$$ H_{0,rel} = - #frac{#hbar^{2}}{2 m} #Delta_{#x}  $$ 
$$ H_{rel} = - #frac{#hbar^{2}}{2 m} #Delta_{#x} + V(#x)  $$
de esta forma, podemos referirnos a la ecuaci#'on #eqref{h-rel} como:
$$ i#hbar #, #frac{#partial}{#partial t} #psi_{rel}= H_{rel} #psi_{rel}= H_{0,rel} #psi_{rel}+ V(#x) #psi_{rel} $$

En las unidades en las que $#hbar= 1.0546 #cdot 10^{-34} #, J #cdot s$ para un sistema binario en el que las part#'iculas que lo forman se encuentran en $#R^{n}$, la transformada de Fourier y su inversa en el sistema de coordenadas $(#x_{1} , #x_{2} )$ est#'an dadas por:

#begin{equation}#label{trans-uf}
#hat{#psi}(#P_{1},#P_{2}) =#F #psi(#P_{1},#P_{2})= #frac{1}{(2#pi #hbar)^{n}} #int_{#R^{2n}} e^{-#frac{i}{#hbar} ( #P_{1}#cdot #x_{1} + #P_{2}#cdot #x_{2} ) } #psi(#x_{1},#x_{2}) #, d#x_{1}#, d#x_{2}
#end{equation}

#begin{equation}#label{transi-uf}
#psi(#x_{1},#x_{2}) =#F^{-1} #hat{#psi}(#x_{1},#x_{2})= #frac{1}{(2#pi #hbar)^{n}} #int_{#R^{2n}} e^{#frac{i}{#hbar} ( #P_{1}#cdot #x_{1} + #P_{2}#cdot #x_{2} ) } #hat{#psi}(#P_{1},#P_{2}) #, d#P_{1}#, d#P_{2}
#end{equation}

#n Tomando la interpretaci#'on usual de la transformada de Fourier de una funci#'on de onda (como una transformaci#'on entre el espacio de posiciones y de momentos), podemos observar de las expresiones anteriores que las variables en el espacio de momentos las denotaremos ahora como $#P_{1},#P_{1}$ en lugar de $#k_{1},#k_{2}$.##
En el caso de un sistema binario, el operador de posici#'on en la direcci#'on $i$ para la part#'icula $j$ ($i=1,#ldots,n$ y $j=1,2,$) en el espacio de momentos esta dado por:
$$ x_{i}^{j}#, #hat{#psi}=  i#hbar #frac{#partial }{#partial p_{i}^{j}} #, #hat{#psi}   $$
 
Tal y como lo hicimos en #eqref{K-cm-rel}, introducimos en el espacio de momentos $#P_{1},#P_{2}$ a las variables:
#begin{equation}#label{pr-pcm}
#P= #P_{1}+#P_{2} #,#,#,;#,#,#, #P_{cm}= #frac{ m_{2}#P_{1} - m_{1}#P_{2} }{M}
#end{equation}
en donde $M=m_{1}+m_{2}$, es la masa total del sistema. as#'i como lo hicimos con las expresiones #eqref{K-cm-rel} nos referiremos a la variable $#P_{cm}$ como el #emph{momento del centro de masa} y a la variable $#P$ como el momento relativo del sistema. Es f#'acil ver que esta definici#'on de las variables $#P_{cm}$ y $#P$ es consistente con el hecho de que al tomar la transformada de Fourier de una funci#'on de onda de la forma $#Psi (#xx,#x)$ lo que se obtiene es una funci#'on de la forma $#hat{#psi}(#P_{cm},#P)$ y viceversa. Esto se debe al hecho de que el jacobiano de las transformaciones de coordenadas $ (#x_{1} , #x_{2}) #rar (#xx , #x) $ y $ (#P_{1} , #P_{2}) #rar (#P_{cm} , #P) $ es uno.## 
Lo anterior quiere decir que las expresiones:
$$ #hat{#psi}(#P_{cm},#P) =#F #psi (#P_{cm},#P)= #frac{1}{(2#pi #hbar)^{n}} #int_{#R^{2n}} e^{-#frac{i}{#hbar} ( #P_{cm}#cdot #xx + #P#cdot #x ) } #psi(#xx,#x) #, d#xx #, d#x $$

$$ #psi(#xx,#x) =#F^{-1} #hat{#psi}(#xx,#x)= #frac{1}{(2#pi #hbar)^{n}} #int_{#R^{2n}} e^{#frac{i}{#hbar} ( #P_{cm}#cdot #xx + #P #cdot #x ) } #hat{#psi}(#P_{cm},#P) #, d#P_{cm}#, d#P $$
tienen sentido, en el contexto de los cambios de variables #eqref{cm-cr-h} y #eqref{pr-pcm}. De esta forma, tenemos que el espacio de Hilbert de los estados del sistema binario en la representaci#'on de momentos se factoriza de la forma:
#begin{equation}#label{H-em}
#hat{#H}= #hat{#H}_{cm} #pr #hat{#H}_{rel}
#end{equation}
con:
$$ #hat{#H}_{cm}= #F_{cm}#H_{cm} #,#,#,,#,#,#, #hat{#H}_{rel}= #F_{rel}#H_{rel} $$
en donde introdujimos a la transformada de Fourier respecto a la coordenada del centro de masa $#F_{cm}$ y a la transformada de Fourier respecto a la coordenada relativa $#F_{rel}$, las cuales est#'an dadas por:
$$ #F_{cm}#Psi = #frac{1}{(2#pi #hbar)^{n/2}} #int_{#R} e^{-#frac{i}{#hbar} (#P_{cm} #cdot #xx) } #Psi(#xx) #, d#xx $$
$$ #F_{rel}#psi_{rel} = #frac{1}{(2#pi #hbar)^{n/2}} #int_{#R} e^{-#frac{i}{#hbar} (#P #cdot #x) } #psi_{rel}(#x) #, d#x $$
en donde $ #Psi #in #H_{cm} $ y $#psi_{rel} #in #H_{rel}$. Respectivamente las transformaciones inversas est#'an dadas por:
$$ #F_{cm}^{-1}#hat{#Psi} = #frac{1}{(2#pi #hbar)^{n/2}} #int_{#R} e^{#frac{i}{#hbar} (#P_{cm} #cdot #xx) } #hat{#Psi}(#P_{cm}) #, d#P_{cm} $$
$$ #F_{rel}^{-1}#hat{#psi}_{rel} = #frac{1}{(2#pi #hbar)^{n/2}} #int_{#R} e^{#frac{i}{#hbar} (#P #cdot #x) } #hat{#psi}_{rel}(#P) #, d#P $$
en donde $ #hat{#Psi} #in #hat{#H}_{cm} $ y $#hat{#psi}_{rel} #in #hat{#H}_{rel}$.##

#section{Espacio de configuraci#'on, operadores de onda y el operador de dispersi#'on}
Consideremos un sistema de dispersi#'on formado por dos part#'iculas sin spin de masas $m_{1}$ y $m_{2}$ respectivamente, las cuales est#'an en el espacio $#R^{3}$. Para describir a este sistema utilizaremos el formalismo de la teor#'ia de dispersi#'on dependiente del tiempo, que desarrollamos en las secciones 14 y 15.##

En este caso, el espacio de Hilbert de los estados cu#'anticos del sistema binario en el espacio de posiciones $(#x_{1} , #x_{2})$ es $#H= L^{2}(#R^{6})$, el cual por lo visto en la secci#'on 3.1 y por la expresi#'on #eqref{sep-estt}, podemos descomponer como $#H= L^{2}(#R^{3}) #pr L^{2}(#R^{3}) $. Esta descomposici#'on es natural ya que las dos part#'iculas que forman el sistema son libres de moverse en todo el espacio $#R^{3}$. En este caso, la ecuaci#'on de Schr#"odinger esta dada por #eqref{shc-x12} y en donde consideraremos que el potencial de interacci#'on entre las part#'iculas $V(#x_{1}-#x_{2})=V(#x)$ es una funci#'on real definida en todo el espacio $#R^{3}$, la cual no suponemos que tenga ning#'un tipo de simetr#'ia, por ejemplo simetr#'ia del tipo esf#'erica (en cuyo caso $V(#x)=V(|#x|)$) como suele suceder en muchos casos de estudio de sistemas de dispersi#'on potencial.##

Haciendo el cambio de coordenadas $ (#x_{1},#x_{2}) #rar ( #xx , #x) $ dado por #eqref{cm-cr-h} es claro que $#H_{cm}= #H_{rel}= L^{2}(#R^{3})$ y por tanto seguimos teniendo que $#H= #H_{cm} #pr #H_{rel}= L^{2}(#R^{6}) $. Mas aun, ya que la transformada de Fourier es una transformaci#'on unitaria de $L^{2}(#R^{3})$ en si mismo, tenemos por la expresi#'on #eqref{H-em} que: $$#hat{#H}_{cm}=#hat{#H}_{rel}=L^{2}(#R^{3})= L^{2}(#R^{3}) $$
y por tanto, $ #hat{#H}= #hat{#H}_{cm} #pr #hat{#H}_{rel}=L^{2}(#R^{6}) $.##

Supongamos ahora que el Hamiltoniano del sistema es un operador auto adjunto, tal que nuestra teor#'ia es asint#'oticamente completa, en el sentido del desarrollo del capitulo 14. De esta forma, la evoluci#'on temporal de todo estado de dispersi#'on $#Psi$ del Hamiltoniano perturbado $H$, puede ser descrita asint#'oticamente por estados $#varphi_{-}$ y $#varphi_{+}$ del Hamiltoniano libre $H_{0}$ en el pasado remoto y el futuro distante respectivamente, en el sentido de la expresi#'on #eqref{cas-1}. El estado asint#'otico entrante $#varphi_{-}$ es la condici#'on de Cauchy al tiempo $t=0$ de la soluci#'on de la ecuaci#'on de Schr#"odinger libre entrante, mientras que el estado asint#'otico de salida $#varphi_{+}$ es la condici#'on de Cauchy al tiempo $t=0$ de la soluci#'on de la ecuaci#'on libre de Schr#"odinger de salida.##

Los operadores de onda est#'an dados por:
$$ #O_{#pm}= #slim_{t #rar #pm#infty} e^{i#frac{t}{#hbar}H }#, e^{-i #frac{t}{#hbar}H_{0}} $$
Cuando la separaci#'on entre las part#'iculas de nuestro sistema de dispersi#'on es grande y la interacci#'on entre estas es d#'ebil, por la hip#'otesis asint#'otica tenemos que la din#'amica del sistema se puede aproximar a tiempos en el pasado remoto por:
$$ e^{-i#frac{t}{#hbar}H_{0} } #varphi_{-} $$
despu#'es, cuando la separaci#'on de las part#'iculas disminuye de tal forma que la interacci#'on entre estas no puede ser despreciada, la din#'amica del sistema esta dada por la soluci#'on a la ecuaci#'on de Schr#"odinger $e^{-i #frac{t}{#hbar}H} #Psi$, con el con el Hamiltoniano perturbado $H$, la cual, como vimos en el capitulo 14, se puede expresar en t#'erminos del operador de onda $#O_{-}$ y de la soluci#'on libre de entrada $#varphi_{-}$ como:
#begin{equation}#label{Opsi-}
e^{-i #frac{t}{#hbar}H} #O_{-} #varphi_{-}
#end{equation}
la cual se aproxima asint#'oticamente la soluci#'on libre entrante cuando $t #rar -#infty$, es decir:
$$ #lim_{t #rar -#infty} #| e^{-i#frac{t}{#hbar} H_{0}} #varphi_{-} - e^{-i#frac{t}{#hbar} H} #O_{-} #varphi_{-} #|=0 $$

Luego de la interacci#'on y la dispersi#'on de las part#'iculas, a tiempos muy grandes en el futuro remoto, la din#'amica de sistema puede ser aproximada por la soluci#'on libre de salida $e^{-t#frac{t}{#hbar}H_{0}} #varphi_{+}$, la cual tambi#'en se puede expresar en t#'erminos de los operadores de onda $#O_{#pm}$ y de la soluci#'on libre entrante:
$$ e^{ -i#frac{t}{#hbar}H_{0}} #, #O_{+}^{*} O_{-} #varphi_{-} $$
la cual tambi#'en tiene un comportamiento asint#'otico con la ecuaci#'on #eqref{Opsi-} cuando $t#rar #infty$:
$$ #lim_{t #rar #infty} #| e^{-i#frac{t}{#hbar}H} #O_{-} #varphi_{-} - e^{-i #frac{t}{#hbar}H_{0} } #O_{+}^{*} O_{-} #varphi_{-} #| = #lim_{t #rar #infty} #| e^{-i#frac{t}{#hbar}H} #O_{-} #varphi_{-} - e^{-i #frac{t}{#hbar}H_{0} } #varphi_{+} #|=0 $$

El operador de dispersi#'on esta dado tal y como se defini#'o en el capitulo 14:
$$ #S= #O_{+}^{*}#, #O_{-} $$

Para asegurar la existencia de los operadores de onda, del operador de dispersi#'on, que el Hamiltoniano perturbado $H$ sea un operador auto adjunto y que nuestra teor#'ia sea asint#'oticamente completa, debemos hacer suposiciones mas concretas acerca del potencial de interacci#'on $V(#x)$ que las que hemos dado hasta ahora. Nuestro objetivo es trabajar en el limite de bajas energ#'ias, esto es en el limite en el que $|#P / #hbar| #rar 0$, para una variedad de potenciales #emph{bastante general} mas all#'a de los que presentan simetr#'ia esf#'erica como suele suceder en muchos casos. A continuaci#'on damos la suposici#'on sobre el potencial de interacci#'on $V(#x)$ que usaremos a partir de este momento en el desarrollo del texto.

#paragraph{Suposici#'on 1:## Condici#'on sobre el potencial de interacci#'on $V(#x)$:}

Para alguna $#beta >0$, $(1+|#x|)^{#beta} #, V(#x)$ es un operador compacto del espacio de Sobolev $H^{1}$ al espacio de Sobolev $H^{-1}$.##
#rightline{$#dag$}
#vspace{3 mm}

Los espacios de Sobolev $H^{1}$ y $H^{-1}$ est#'an dados por:
$$ H^{1}= H^{1,2}(#R^{n}) = #left#{ f #in L^{2}(#R^{n}) #,|#, #F^{-1} (1+ |#xi|^{2})^{1/2} #,#F f #in L^{2}(#R^{n}) #right#}$$
$$ H^{-1}= H^{-1,2}(#R^{n}) = #left#{ f #in L^{2}(#R^{n}) #,|#, #F^{-1} (1+ |#xi|^{2})^{-1/2} #,#F f #in L^{2}(#R^{n}) #right#}$$

En general, los espacios de Sobolev son espacios de Banach bajo una norma definida adecuadamente. En particular los espacios $H^{1}$ y $H^{-1}$ resultan ser espacios de Hilbert bajo las normas:
$$ #| f #|_{H^{1}} = #| #F^{-1} (1+|#xi|^{2} )^{1/2} #, #F f #|_{L^{2}} $$
$$ #| f #|_{H^{-1}} = #| #F^{-1} (1+|#xi|^{2} )^{-1/2} #, #F f #|_{L^{2}} $$
respectivamente.##

Para una definici#'on y un estudio formal de las propiedades de los espacios de Sobolev nos referimos a [20]. ##

Para las condiciones que un cierto potencial debe de cumplir para satisfacer la suposici#'on 1 nos referimos a [21] ya que aqu#'i no trabajaremos con un potencial en particular. Sin embargo podemos decir que el potencial $V(x)$ cumple con la suposici#'on 1, si existen dos constantes $R, C > 0 $ tales que:
$$ #int_{|#x| #leq R} |V(#x)|^{2} #, d#x < #infty $$
y
$$ |V(#x)| #leq C#, (1+|#x|^{#beta'}) $$
para $|#x| > R$ y para alguna $#beta' >#beta$, en donde $#beta$ es la misma constante que en la suposici#'on 1.##

Bajo la condici#'on anterior, el Hamiltoniano $H$ es un operador auto adjunto [18] y queda definido como la suma de los operadores $H_{0}$ y $V$. as#'i tambi#'en los operadores de onda $#O_{#pm}$ existen y sus rangos coinciden con el subespacio absolutamente continuo de $H$ (ver [10] y [18] para una definici#'on de este espacio), mas aun por la proposici#'on #ref{prop-s-u} el operador de dispersi#'on $#S$ es unitario.##

Por la ecuaci#'on #eqref{s-sep} tenemos que en la factorizaci#'on del espacio de Hilbert de posiciones $#H= #H_{cm} #pr #H_{rel}$, el operador de dispersi#'on $#S$ se factoriza como:
$$ #S = I_{cm} #pr #S_{rel} $$
en donde $I_{cm}$ es el operador identidad del espacio de Hilbert $#H_{cm}= L^{2}(#R^{n})$ y $#S_{rel}$ es el operador de dispersi#'on respecto a la coordenada relativa $#x$, el cual esta dado por:
$$#S_{rel} = #O_{+,rel}^{*} #O_{-,rel} $$
los operadores de onda relativos, $#O_{#pm,rel}$ est#'an dados como en la ecuaci#'on #eqref{wav-rel}:
$$ #O_{#pm,rel}= #slim_{t #rar #pm #infty} V_{t,rel}^{*} #, U_{t,rel}=#slim_{t #rar #pm #infty} e^{i#frac{t}{#hbar}H_{rel} }#, e^{-i#frac{t}{#hbar}H_{0,rel} } $$

De forma similar en la factorizaci#'on $#hat{#H}= #hat{#H}_{cm} #pr #hat{#H}_{rel}$ del espacio de momentos, tenemos que el operador de dispersi#'on $#hat{#S}$, se factoriza como:
$$ #hat{#S}= I_{cm} #pr #hat{#S}_{rel} $$
en donde $#hat{#S}_{rel}$ es el operador de dispersi#'on respecto al momento relativo $#P$ el cual esta dado por la ecuaci#'on #eqref{sp-rell}. Mas aun, por la teor#'ia que desarrollamos en los capitulos 15 y 16, podemos dar una representaci#'on espectral de este operador para obtener entonces la matriz de dispersi#'on, tal y como lo hicimos en la expresi#'on #eqref{sp-mat} en la unidades en las que $#hbar=1$, pero ya que este es un factor constante, podemos escribir a la matriz de dispersi#'on en el espacio de momento relativo como:
#begin{equation}#label{sp-mat}
#hat{#S}_{rel} =#{ #S( #P^{2}/2m ) #}
#end{equation}
en donde $#S(E)$ es un operador unitario en el espacio de Hilbert $L^{2}(S^{2})$ para cada $E #in (0 , #infty )$ y $m$ es la masa reducida del sistema.##

Como hemos mencionado, nuestro objetivo es dar una expresi#'on para la pureza de un sistema dispersor en el limite de bajas energ#'ias. Esto lo haremos bajo las condiciones del siguiente teorema, el cual da una expansi#'on de la matriz de dispersi#'on #eqref{sp-mat} en el limite cuando $|#P/#hbar| #rar 0$. Este teorema fue probado por Kato y Jensen en el articulo [18] para el caso particular en el que $#hbar=1$ y $m=1/2$, pero el caso general se obtiene f#'acilmente a trav#'es de un argumento que desarrollamos en el ap#'endice de este texto.##

Antes de enunciar el teorema definimos a la #emph{longitud de dispersi#'on} como:
#begin{equation}#label{scat-l}
c_{0}= #frac{1}{4#pi} #left(#frac{2m}{#hbar^{2}}V #left( 1+G_{0}#frac{2m}{#hbar^{2}}V #right)^{-1}#, 1 , 1   #right)
#end{equation}
en donde $( #cdot , #cdot )$ denota (a partir de este momento a menos que se indique lo contrario) al producto escalar del espacio de Hilbert $L^{2}(#R^{3})$, $1$ denota a la funci#'on id#'enticamente uno y $G_{0}$ es el operador integral cuyo n#'ucleo esta dado por la funci#'on de Green a energ#'ia cero:
$$ G_{0}(#x , #y )= #frac{1}{ 4#pi |#x - #y| } #,#,,#,#,#, #x ,#y #in #R^{3} $$

as#'i tambi#'en definimos:
#begin{equation}
Y_{0}(#nu) = #frac{1}{ #sqrt{4 #pi} } #,#,,#, #nu #in S^{2}
#end{equation}
y 
#begin{equation}
Y_{1}(#nu)= #frac{1}{4 #pi^{3/2}} #left(#frac{2m}{#hbar^{2}} V #, #left(1+G_{0}#frac{2m}{#hbar^{2}}#, V  #right)^{-1}#, 1, #x #cdot #nu #right) #,#,,#, #nu #in S^{2}
#end{equation}
como siempre denotamos por $#B(L^{2}(S^{2}))$ al espacio de Banach de todos los operadores lineales en el espacio de Hilbert $L^{2}(S^{2})$.


#paragraph{Teorema 1: Expansi#'on de la matriz de dispersi#'on en el limite de bajas energ#'ias (Kato y Jensen):} Sea $V(#x)$ un potencial que cumple con la suposici#'on 1. Supongamos que en cero el Hamiltoniano $H_{rel}$ no tiene #emph{resonancias} o #emph{valores propios}. Entonces si $#beta >5$, en la norma de $#B(L^{2}(S^{2}))$ para $|#P/#hbar| #rar 0$, tenemos la expansi#'on: 
#begin{equation}#label{KJ-ex}
#S(#P^{2}/2 m)= I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} + o(|#P/#hbar^{2}|)
#end{equation}
en donde $I$ denota al operador identidad de $L^{2}(S^{2})$. Los factores $#Sigma_{1}^{0}$ y $ #Sigma_{2}^{0} $, son operadores de rango finito y est#'an dados por:
#begin{equation}
#Sigma_{1}^{0}= -2c_{0} #, ( #cdot , Y_{0} ) Y_{0}
#end{equation}
y 
#begin{equation}
#Sigma_{2}^{0}= 2c_{0}^{2} (#cdot , Y_{0}) Y_{0} + ( #cdot,Y_{1})Y_{0} - ( #cdot,Y_{0}) Y_{1}
#end{equation}
mas aun, si $#beta >7$, $o(|#P/#hbar|^{2})$ se puede remplazar por $O(|#P/#hbar|^{3})$.##
#rightline{$#dag$}
#vspace{3 mm}

Una resonancia a energ#'ia cero del Hamiltoniano $H_{rel}$ es una soluci#'on de la ecuaci#'on $H_{rel} #psi=0$, que decae cuando su argumento tiende a infinito, pero que no pertenece a $L^{2}(#R^{3})$. Para una definici#'on precisa nos referimos a [18]. Por lo anterior y suponiendo que cero no es un valor propio de este mismo Hamiltoniano, que la inversa del operador $#left( 1+G_{0}#frac{2m}{#hbar^{2}} #right)$ existe sin ninguna ambiguedad en la definici#'on #eqref{scat-l} de la longitud de dispersi#'on $c_{0}$.##

Con la expansi#'on de la matriz de dispersi#'on que da el teorema de Kato - Jensen, para un estado inicial de un sistema dispersor de dos part#'iculas, el cual satisface la ecuaci#'on libre de Schr#"odinger, obtendremos el estado libre final (en el sentido del capitulo 14) al cual le calcularemos la pureza (en el sentido del capitulo 12). Todo esto lo haremos en el espacio de momentos y mas precisamente en las variables del momento del centro de masa $#P_{cm}$ y del momento relativo $#P$.##

Si $#varphi(#P_{1},#P_{2}) #in L^{2}(#R^{6})$ es un estado puro de un sistema de dos part#'iculas en la representaci#'on del espacio de momentos, su pureza $P(#varphi)$ esta dada por la expresi#'on #eqref{pur-im}, a saber:
$$ P(#varphi)= #int_{#R^{12}} #varphi(#P_{1},#P_{2}) #, #varphi(#P_{1}',#P_{2}') #, #ol{#varphi(#P_{1}',#P_{2})} #, #ol{#varphi(#P_{1},#P_{2}')}#, d#P_{1}#, d#P_{2}#, d#P_{1}' #, d#P_{2}' $$
ya que el Jacobiano de la transformaci#'on $( #P_{1},#P_{2} ) #rar (#P_{cm},#P) $ es uno, si expresamos al estado del sistema de dos part#'iculas como $#varphi(#P_{cm},#P)$, el calculo de la pureza de este ultimo en t#'erminos de estas coordenadas tiene la misma forma que la expresi#'on anterior:
$$ P(#varphi)= #int_{#R^{12}} #varphi(#P_{cm},#P) #, #varphi(#P_{cm}',#P') #, #ol{#varphi(#P_{cm}',#P)} #, #ol{#varphi(#P_{cm},#P')}#, d#P_{cm}#,d#P#,d#P_{cm}' #,d#P' $$
como vimos en el capitulo 12, la pureza $P(#varphi)$ de un estado cu#'antico normalizado $#varphi$, es una medida del entrelazamiento que existe en este estado. Sabemos que para un estado no entrelazado (o separable) la pureza es siempre igual a 1, mientras que para estados no separables es un numero en el intervalo $(0,1)$.##

#newpage

#part{Entrelazamiento cu#'antico en procesos de dispersi#'on a bajas energ#'ias}

#chapter{Creaci#'on de entrelazamiento}

Consideremos un estado asint#'otico incidente el cual es el producto de dos gausianas normalizadas en el sistema de referencia del laboratorio:

#begin{equation}#label{estin}
#vp_{in,#P_{0} }(#P_{1},#P_{2}) = #vp_{#P_{0}}(#P_{1})#, #vp_{-#P_{0}}(#P_{2})
#end{equation}

En donde

#begin{equation}#label{estin1}
#vp_{#P_{0}}(#P_{1})= #frac{1}{( #sigma^{2} #pi )^{3/4}}#,
e^{-| #P_{1} - #P_{0} |^{2}/2 #sigma^{2}} #,#,,#,#,#,#, #P_{1} #in #R^{3}
#end{equation}

#begin{equation}#label{estin2}
#vp_{-#P_{0}}(#P_{2})= #frac{1}{( #sigma^{2} #pi )^{3/4}}#,
e^{-| #P_{1} + #P_{0} |^{2}/2 #sigma^{2}} #,#,,#,#,#,#, #P_{2} #in #R^{3}
#end{equation}

En el estado #eqref{estin} las part#'iculas uno y dos tienen momento promedio $#P_{0}$ y $-#P_{0}$ respectivamente, siendo $#sigma$ la varianza de la distribuci#'on de momento de ambas.##
Asumimos que la disperci#'on de las part#'iculas tiene lugar en el origen del sistema de referencia del laboratorio al tiempo cero, por esta raz#'on en el estado #eqref{estin} la posici#'on promedio de ambas part#'iculas es cero. Para observar esto #'ultimo escribamos:
$$ #P_{0}=( p_{0,1},p_{0,2},p_{0,3} ) #,#,;#,#, #P_{1}=( p_{1,1},p_{1,2},p_{1,3} ) #,#,;#,#, #P_{2}=( p_{2,1},p_{2,2},p_{2,3} ) $$
y sean
$$ #v{x}_{1}= ( x_{1,1},x_{1,2},x_{1,3} ) $$
$$ #v{x}_{2}= ( x_{2,1},x_{2,2},x_{2,3} ) $$
Los vectores de posici#'on de las part#'iculas uno y dos respectivamente.
Luego la posici#'on promedio de la primera part#'icula en la coordenada $x_{1,#,j}$  $(j=1,2,3)$ esta dada por:

$$ #langle x_{1,#,j} #rangle= #left(#vp_{in,#P_{0} }#, , i#hbar#, #frac{#partial #vp_{in,#P_{0} } }{#partial p_{1,j}}   #right)_{ L^{2}(#R^{6}) }= i #hbar #int_{#R^{3}} #int_{#R^{3}} #vp_{in,#P_{0} } #cdot #frac{#partial #vp_{in,#P_{0} } }{#partial p_{1,j}} #, d#P_{1}#, d#P_{2} $$

$$ =#frac{i #hbar}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #exp #left( #frac{ -|#P_{1}-#P_{0}|^{2}-|#P_{2}+#P_{0}|^{2} }{#sigma^{2}} #right)#, #frac{#partial}{#partial p_{1,j}} #left( -#frac{|#P_{1}-#P_{0}|^{2}}{2 #sigma^{2}} #right) #, d#P_{1}#, d#P_{2}  $$

$$ =-#frac{i #hbar}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #exp #left( #frac{ -|#P_{1}-#P_{0}|^{2}-|#P_{2}+#P_{0}|^{2} }{#sigma^{2}} #right)#, #frac{#partial}{#partial p_{1,j}} #left( #frac{ ( p_{1,j}-p_{0,j} )^{2} }{2 #sigma^{2}} #right) #, d#P_{1}#, d#P_{2}  $$

$$ =-#frac{i #hbar}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #exp #left( #frac{ -|#P_{1}-#P_{0}|^{2}-|#P_{2}+#P_{0}|^{2} }{#sigma^{2}} #right)#,  #left( #frac{ p_{1,j}-p_{0,j} }{#sigma^{2}} #right) #, d#P_{1}#, d#P_{2}  $$

$$ =-#frac{i #hbar}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #exp #left( -#frac{|#P_{2}+#P_{0}|^{2}}{#sigma^{2}} #right)  d#P_{2} #cdot #int_{#R} #exp #left( -#frac{( p_{1,i}-p_{0,i} )^{2}}{#sigma^{2}} #right) dp_{1,i} #cdot #int_{#R} #exp #left( -#frac{( p_{1,k}-p_{0,k} )^{2}}{#sigma^{2}} #right) dp_{1,k}  $$
$$ #cdot #int_{#R} #exp #left( -#frac{( p_{1,j}-p_{0,j} )^{2}}{#sigma^{2}} #right)  #left( #frac{ p_{1,j}-p_{0,j} }{#sigma^{2}} #right) dp_{1,j} $$

Donde $i,k #neq j$.##

Haciendo el cambio de variable $ p= p_{1,j}-p_{0,j} $ en la integral del extremo derecho de la ultima ecuaci#'on se tiene que:

$$ #int_{-#infty}^{#infty} #exp #left( -#frac{( p_{1,j}-p_{0,j} )^{2}}{#sigma^{2}} #right)  #left( #frac{ p_{1,j}-p_{0,j} }{#sigma^{2}} #right) dp_{1,j} =#int_{-#infty}^{#infty} #exp #left( -#frac{p^{2}}{#sigma^{2}} #right)  #left( #frac{ p }{#sigma^{2}} #right) dp=0 $$
ya que, $#exp #left( -#frac{p^{2}}{#sigma^{2}} #right)  #left( #frac{ p }{#sigma^{2}} #right)$ es una funci#'on impar. Por tanto, $#langle x_{i,#, j} #rangle=0 $ para $j=1,2,3$ y podemos concluir que, $ #langle #v{x}_{1} #rangle=0 $ .##
De forma similar obtenemos que la posici#'on promedio de la segunda part#'icula en la coordenada $x_{2,#,j}$ $j=1,2,3$ est#'a dada por:

$$ #langle x_{2,#,j} #rangle= #left(#vp_{in,#P_{0} }#, , i#hbar#, #frac{#partial #vp_{in,#P_{0} } }{#partial p_{2,j}}   #right)_{ L^{2}(#R^{6}) } $$

$$ =-#frac{i #hbar}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #exp #left( -#frac{|#P_{1}-#P_{0}|^{2}}{#sigma^{2}} #right)  d#P_{1} #cdot #int_{#R} #exp #left( -#frac{( p_{2,i}+p_{0,i} )^{2}}{#sigma^{2}} #right) dp_{2,i} #cdot #int_{#R} #exp #left( -#frac{( p_{2,k}+p_{0,k} )^{2}}{#sigma^{2}} #right) dp_{2,k}  $$
$$ #cdot #int_{#R} #exp #left( -#frac{( p_{2,j}+p_{0,j} )^{2}}{#sigma^{2}} #right)  #left( #frac{ p_{2,j}+p_{0,j} }{#sigma^{2}} #right) dp_{2,j} $$

Haciendo el cambio de variable $p=p_{2,j}+p_{0,j}$ en la integral del extremo derecho de la ultima ecuaci#'on se tiene que:
$$  #int_{-#infty}^{#infty} #exp #left( -#frac{( p_{2,j}+p_{0,j} )^{2}}{#sigma^{2}} #right)  #left( #frac{ p_{2,j}+p_{0,j} }{#sigma^{2}} #right) dp_{2,j} =#int_{-#infty}^{#infty} #exp #left( -#frac{p^{2}}{#sigma^{2}} #right)  #left( #frac{ p }{#sigma^{2}} #right) dp=0 $$
Por tanto $#langle x_{2,#,j} #rangle =0 $ para $j=1,2,3$ y podemos concluir que $#langle #v{x}_{2} #rangle=0 $ .##

Otra cantidad de inter#'es que se puede obtener del estado #eqref{estin} es el promedio del momento relativo $#P=#mu_{2}#P_{1} - #mu_{1}#P_{2} $; asi el valor promedio de la coordenada $p_{j}= #mu_{2}p_{1,j}-#mu_{1}p_{2,j} $ ($j=1,2,3$) del momento relativo esta dado por,
$$ #langle p_{j} #rangle = #left(#vp_{in,#P_{0} }#, , p_{j}#, #vp_{in,#P_{0} }  #right)_{ L^{2}(#R^{6}) }
=#frac{1}{( #sigma^{2} #pi )^{3}} #int_{#mathbb{R}^{3}} #int_{#mathbb{R}^{3}} p_{j} #cdot #exp #left( #frac{-| #P_{1}-#P_{0} |^{2}-|#P_{2}+#P_{0}|^{2}}{ #sigma^{2} } #right)  d#P_{1}#, d#P_{2}  $$

$$= #frac{1}{( #sigma^{2} #pi )^{3}} #int_{#R^{3}}
#exp #left( #frac{-| #P_{2}+#P_{0} |^{2}}{#sigma^{2}} #right) #, d#P_{2}
#cdot #int_{#R^{3}} #exp #left( #frac{-| #P_{1}-#P_{0} |^{2}}{#sigma^{2}} #right) #cdot (#mu_{2} #, p_{1, j}) #, d#P_{1}  $$
$$- #frac{1}{( #sigma^{2} #pi )^{3}} #int_{#R^{3}}
#exp #left( #frac{-| #P_{1}-#P_{0} |^{2}}{#sigma^{2}} #right) #, d#P_{1}
#cdot #int_{#R^{3}} #exp #left( #frac{-| #P_{2}+#P_{0} |^{2}}{#sigma^{2}} #right) #cdot (#mu_{1} #, p_{2, j}) #, d#P_{2} $$

$$= #frac{1}{( #sigma^{2} #pi )^{3}} ( ( #sigma^{3}#pi^{3/2} )^{2} #cdot #mu_{2}p_{0,j} ) +   #frac{1}{( #sigma^{2} #pi )^{3}} ( ( #sigma^{3}#pi^{3/2} )^{2} #cdot #mu_{1}p_{0,j} ) = ( #mu_{2}+#mu_{1} ) p_{0,j}= p_{0,j} $$

Por tanto, podemos concluir que $#langle #P #rangle= #P_{0}$.##

Hay que notar que para trabajar en el r#'egimen de bajas energ#'ias, es necesario que el promedio del momento relativo, $#langle #P #rangle= #P_{0}$ y la varianza de la distribuci#'on $#sigma$, tengan un valor peque#~no, ya que si $#sigma$ tiene un valor grande el estado asint#'otico inicial $ #vp_{in,#P_{0} } $ va a tener una gran probabilidad de tener un momento muy grande, no importando que el promedio del momento relativo sea muy peque#~no.##

Ya que $#vp_{in,#P_{0}}$ es un estado producto, su pureza es uno,
#begin{equation}#label{p:estin}
P ( #vp_{in,#P_{0}} )= 1
#end{equation}

Una vez que el proceso de disperci#'on termina, las part#'iculas est#'an en el estado asint#'otico de salida $#vp_{out,#P_{0}}$ dado por,
#begin{equation}#label{estout}
#vp_{out,#P_{0}}(#P_{1},#P_{2})= ( #mathcal{S}(#P^{2}/2m) #vp_{in,#P_{0}} ) (#P_{1},#P_{2})
#end{equation}

Ya que el momento relativo $#P$ depende tanto de $#P_{1}$ como de $#P_{2}$, $#vp_{out,#P_{0}}$ ya no es un estado producto y por tanto tiene una pureza menor que uno, lo cual indica que las part#'iculas se han entrelazado cu#'anticamente debido al proceso de disperci#'on.##

A continuaci#'on introducimos alguna de la notaci#'on que usaremos en el c#'alculo del pureza del estado $#vp_{out,#P_{0}}$.## 
#n Denotamos por $#vp_{in}$ al estado asint#'otico inicial con momento relativo promedio cero,
#begin{equation}#label{estinp0}
#vp_{in}(#P_{1},#P_{2})= #vp(#P_{1})#,#vp(#P_{2})
#end{equation}

En donde,

#begin{equation}#label{estinp01}
#vp(#P)= #frac{1}{(#sigma^{2} #pi)^{3/4}} e^{-#P^{2}/2#sigma^{2} } #,#,,#,#,#,#, #P #in #R^{3}
#end{equation}

y denotaremos por $#vp_{out}$ al estado asint#'otico de salida con estado asint#'otico de entrada $#vp_{in}$,

#begin{equation}#label{estoutp0}
#vp_{out}(#P_{1},#P_{2})= ( #mathcal{S}(#P^{2}/2m) #vp_{in} ) (#P_{1},#P_{2})
#end{equation}

Tambi#'en definimos,
#begin{equation}
#psi_{#v{q}_{0}}(#v{q})=#frac{1}{(#pi)^{3/4}}e^{-|#v{q}-#v{q}_{0}|^{2}/2}#,#,,#,#,#,#, #v{q} #in #R^{3}
#end{equation}
#begin{equation}#label{psi}
#psi(#v{q})=#frac{1}{(#pi)^{3/4}}e^{-#v{q}^{2}/2}#,#,,#,#,#,#, #v{q} #in #R^{3}
#end{equation}

#begin{equation}
#psi_{in,#v{q}_{0}}(#v{q}_{1},#v{q}_{2})=#psi_{#v{q}_{0}}(#v{q}_{1})#,#psi_{-#v{q}_{0}}(#v{q}_{2})
#end{equation}
#begin{equation}#label{psi-est}
#psi_{in}(#v{q}_{1},#v{q}_{2})=#psi(#v{q}_{1})#,#psi(#v{q}_{2})
#end{equation}
Notamos que,
#begin{equation}
#left#| #psi_{#v{q}_{0}} #right#|_{L^{2}(#R^{3})}^{2}= #frac{1}{(#pi)^{3/2}} #int_{#R^{3}} e^{-|#v{q}-#v{q}_{0}|^{2}} d #v{q}=1
#end{equation}
#begin{equation}
#left#| #psi #right#|_{L^{2}(#R^{3})}^{2}= #frac{1}{(#pi)^{3/2}} #int_{#R^{3}} e^{-#v{q}^{2}} d #v{q}=1
#end{equation}
y por tanto,
#begin{subequations}#label{p1-p2-p-pcm}
#begin{align}#label{norm-psi}
#| #psi_{in,#v{q}_{0}} #|=1##
#| #psi_{in} #|=1
#end{align}
#end{subequations}


#newpage
En las siguientes proposici#'ones establecemos un par de resultados que utilizaremos en el c#'alculo del pureza del estado $#vp_{out,#P_{0}}$.

#begin{proposition}
#label{p1}

#begin{equation}#label{p1.1}
#| #vp_{in,#P_{0} } -#vp_{in}  #| #leq C #min #left#{ #frac{|#P_{0}|}{#sigma} , 1  #right#}
#end{equation}

#begin{equation}#label{p1.2}
#| #P (#vp_{in,#P_{0}} -#vp_{in}) #| #leq C #, |#P_{0}|
#end{equation}

#end{proposition}

#n {#bf Demostraci#'on}##
Denotamos por $#v{q}_{i}= #P_{i}/#sigma$, $i=0,1,2$. Luego observamos que,
$$ #| #vp_{in,#P_{0}}-#vp_{in} #|^{2} = #frac{1}{(#sigma^{2}#pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #left[ #exp#left(#frac{-|#P_{1}-#P_{0}|^{2}-|#P_{2}+#P_{0}|^{2} }{2#sigma^{2}} #right)-#exp #left(#frac{-|#P_{1}|^{2}-|#P_{2}|^{2}}{2#sigma^{2}} #right) #right]^{2} d#P_{1}#,d#P_{2} $$
$$=#frac{1}{(#sigma^{2}#pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #left[ #exp#left(#frac{-|#v{q}_{1}-#v{q}_{0}|^{2}-|#v{q}_{2}+#v{q}_{0}|^{2} }{2} #right)-#exp #left(#frac{-|#v{q}_{1}|^{2}-|#v{q}_{2}|^{2}}{2} #right) #right]^{2} #sigma^{3} d#v{q}_{1}#, #sigma^{3} d#v{q}_{2} $$
$$=#frac{1}{#pi^{3}} #int_{#R^{3}} #int_{#R^{3}} #left[ #exp#left(#frac{-|#v{q}_{1}-#v{q}_{0}|^{2}-|#v{q}_{2}+#v{q}_{0}|^{2} }{2} #right)-#exp #left(#frac{-|#v{q}_{1}|^{2}-|#v{q}_{2}|^{2}}{2} #right) #right]^{2} d#v{q}_{1}#, d#v{q}_{2} = #| #psi_{in,#v{q}_{0}}-#psi_{in} #|^{2}$$

Por tanto,
#begin{equation}#label{p1.3}
#| #vp_{in,#P_{0}}-#vp_{in} #|=#| #psi_{in,#v{q}_{0}}-#psi_{in} #|
#end{equation}
Primero para $|#v{q}_{0}|#leq 1$ se tiene,
#begin{equation}#label{p1.4}
#psi_{in,#v{q}_{0}}-#psi_{in}= #frac{1}{#pi^{3/2}} #, e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})/2} #left[ e^{-|#v{q}_{0}|^{2}+(#v{q}_{1}-#v{q}_{2})#cdot #v{q}_{0} }-1 #right]
#end{equation}
textcolor{red}{Schwarz}
 Schwars y la desigualdad de Minkowski tenemos que,
$$ -|#v{q}_{0}|^{2}+( #v{q}_{1}-#v{q}_{2} )#cdot #v{q}_{0} #leq |#v{q}_{0}|^{2} +|#v{q}_{1}-#v{q}_{2}|#, |#v{q}_{0}|$$
$$ #leq |#v{q}_{0}|^{2} + (|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}| $$

As#'i podemos escribir,
#begin{equation}#label{p1.5}
#left| e^{-|#v{q}_{0}|^{2}+( #v{q}_{1}-#v{q}_{2} )#cdot #v{q}_{0}} -1 #right|=#left| #int_{0}^{-|#v{q}_{0}|^{2}+( #v{q}_{1}-#v{q}_{2} )#cdot #v{q}_{0}} e^{s}#, ds #right|
#end{equation}
$$#leq #left| #int_{0}^{|#v{q}_{0}|^{2} + (|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}|} e^{s}#, ds #right|  $$
#begin{equation}#label{p1.6}
#leq e^{|#v{q}_{0}|^{2} + (|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}|} ( |#v{q}_{0}|^{2} + (|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}| )
#end{equation}
Por las ecuaciones #eqref{p1.4},#eqref{p1.5} y #eqref{p1.6} se tiene que
$$#| #psi_{in,#v{q}_{0}}-#psi_{in} #|^{2}= #frac{1}{#pi^{3}}#int_{#R^{3}} #int_{#R^{3}} e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})} #left[ e^{-|#v{q}_{0}|^{2}+(#v{q}_{1}-#v{q}_{2})#cdot #v{q}_{0} }-1 #right]^{2} d#v{q}_{1}#, d#v{q}_{2} $$
$$ #leq #frac{1}{#pi^{3}}#int_{#R^{3}} #int_{#R^{3}} e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})} e^{2|#v{q}_{0}|^{2} + 2(|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}|} ( |#v{q}_{0}|^{2} + (|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}| )^{2}  #, d#v{q}_{1}#, d#v{q}_{2} $$
$$ =|#v{q}_{0}|^{2} #cdot #left[ #frac{1}{#pi^{3}}#int_{#R^{3}} #int_{#R^{3}} e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})} e^{2|#v{q}_{0}|^{2} + 2(|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}|} ( |#v{q}_{0}| + (|#v{q}_{1}|+|#v{q}_{2}|) )^{2}  #, d#v{q}_{1}#, d#v{q}_{2} #right] $$

Se observa que el miembro derecho de la ultima igualdad es una constante real y por tanto,

$$ #| #psi_{in,#v{q}_{0}}-#psi_{in} #| #leq C#, |#v{q}_{0}| $$

Por #eqref{p1.3} y la definici#'on de $#v{q}_{0}$ concluimos que para $|#P_{0}|#leq 1$,
$$ #| #vp_{in,#P_{0}}-#vp_{in} #| #leq C #, #frac{|#P_{0}|}{#sigma} $$

Ahora supongamos $1 #leq |#v{q}_{0}|$. Por la desigualdad de Schwars se tiene que,
$$ #| #psi_{in,#v{q}_{0}}-#psi_{in} #| #leq #| #psi_{in,#v{q}_{0}} #| + #| #psi_{in} #|= 2 #leq C #cdot 1 #leq C #, |#v{q}_{0}| $$
Por lo anterior concluimos que #eqref{p1.1} se cumple.##

Denotamos por $#v{q}=#P/#sigma$ y notamos,
#begin{equation}#label{p1.7.1}
|#v{q}|= | #mu_{2}#v{q}_{1} - #mu_{1}#v{q}_{2} | #leq | #mu_{2}#v{q}_{1} |+ | #mu_{1}#v{q}_{2} |#leq |#v{q}_{1}|+|#v{q}_{2}|
#end{equation}

Luego se tiene que,
$$ #| #P (#vp_{in,#P_{0}}-#vp_{in}) #|^{2} =$$
$$#frac{1}{(#sigma^{2}#pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} |#P|^{2} #left[ #exp#left(#frac{-|#P_{1}-#P_{0}|^{2}-|#P_{2}+#P_{0}|^{2} }{2#sigma^{2}} #right)-#exp #left(#frac{-|#P_{1}|^{2}-|#P_{2}|^{2}}{2#sigma^{2}} #right) #right]^{2} d#P_{1}#,d#P_{2} $$
$$=#frac{1}{(#sigma^{2}#pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #sigma^{2}|#v{q}|^{2} #left[ #exp#left(#frac{-|#v{q}_{1}-#v{q}_{0}|^{2}-|#v{q}_{2}+#v{q}_{0}|^{2} }{2} #right)-#exp #left(#frac{-|#v{q}_{1}|^{2}-|#v{q}_{2}|^{2}}{2} #right) #right]^{2} #sigma^{3} d#v{q}_{1}#, #sigma^{3} d#v{q}_{2} $$
$$=  #frac{#sigma^{2}}{#pi^{3}} #int_{#R^{3}} #int_{#R^{3}} |#v{q}|^{2} #left[ #exp#left(#frac{-|#v{q}_{1}-#v{q}_{0}|^{2}-|#v{q}_{2}+#v{q}_{0}|^{2} }{2} #right)-#exp #left(#frac{-|#v{q}_{1}|^{2}-|#v{q}_{2}|^{2}}{2} #right) #right]^{2} d#v{q}_{1}#, d#v{q}_{2} $$
$$= #sigma^{2} #|#v{q} (#psi_{in,#v{q}_{0}}-#psi_{in}) #|^{2}$$
por tanto,
#begin{equation}#label{p1.7}
#| #P#, (#vp_{in,#P_{0}}-#vp_{in}) #|= #sigma #, #|#v{q}#, (#psi_{in,#v{q}_{0}}-#psi_{in}) #|
#end{equation}

Primero para $|#v{q}_{0}|#leq 1$ tenemos,

#begin{equation}#label{p1.8}
|#v{q}|(#psi_{in,#v{q}_{0}}-#psi_{in})=|#v{q}|#cdot #frac{1}{#pi^{3/2}} #, e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})/2} #left[ e^{-|#v{q}_{0}|^{2}+(#v{q}_{1}-#v{q}_{2})#cdot #v{q}_{0} }-1 #right]
#end{equation}
Luego por las ecuaciones #eqref{p1.5}, #eqref{p1.6},#eqref{p1.7.1} y #eqref{p1.8},
$$ #| #v{q}(#psi_{in,#v{q}_{0}}-#psi_{in}) #|^{2}= #frac{1}{#pi^{3}}#int_{#R^{3}} #int_{#R^{3}} |#v{q}|^{2}#, e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})} #left[ e^{-|#v{q}_{0}|^{2}+(#v{q}_{1}-#v{q}_{2})#cdot #v{q}_{0} }-1 #right]^{2} d#v{q}_{1}#, d#v{q}_{2} $$
$$#leq #frac{1}{#pi^{3}}#int_{#R^{3}} #int_{#R^{3}}(|#v{q}_{1}|+|#v{q}_{2}|)^{2} #, e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})} e^{2|#v{q}_{0}|^{2} + 2(|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}|} ( |#v{q}_{0}|^{2} + (|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}| )^{2}  #, d#v{q}_{1}#, d#v{q}_{2} $$
$$= |#v{q}_{0}|^{2} #left[ #frac{1}{#pi^{3}}#int_{#R^{3}} #int_{#R^{3}}(|#v{q}_{1}|+|#v{q}_{2}|)^{2} #, e^{-(|#v{q}_{1}|^{2}+|#v{q}_{2}|^{2})} e^{2|#v{q}_{0}|^{2} + 2(|#v{q}_{1}|+|#v{q}_{2}|)#, |#v{q}_{0}|} ( |#v{q}_{0}| + (|#v{q}_{1}|+|#v{q}_{2}|) )^{2}  #, d#v{q}_{1}#, d#v{q}_{2} #right] $$

De nuevo, el miembro derecho de la ultima igualdad es una constante real y por tanto,
$$ #| #v{q}(#psi_{in,#v{q}_{0}}-#psi_{in}) #| #leq C#, |#v{q}_{0}| $$
Por #eqref{p1.7} y la definici#'on de $#v{q}_{0}$ concluimos que para $|#P_{0}|#leq 1$,
$$ #| #P#, (#vp_{in,#P_{0}}-#vp_{in}) #| #leq C#, |#P_{0}|. $$

Ahora supongamos que $ 1 #leq |#v{q}_{0}|$ y observamos que esto implica, $#sigma #leq |#P_{0}| $. Por #eqref{p1.7.1} y la definici#'on de $#v{q}$ es claro que,
#begin{equation}#label{p1.7.2}
|#P|#leq |#P_{1}|+|#P_{2}|
#end{equation}
As#'i por #eqref{p1.7.2} y la desigualdad de Minkowski tenemos:
#begin{equation}#label{p1.7.3}
#| #P ( #vp_{in,#P_{0}} - #vp_{in} ) #| #leq #| #P_{1} ( #vp_{in,#P_{0}} - #vp_{in} ) #| + #| #P_{2} ( #vp_{in,#P_{0}} - #vp_{in} ) #|
#end{equation}

Luego para $#| #P_{1} ( #vp_{in,#P_{0}} - #vp_{in} ) #|$ tenemos que:
$$ #| #P_{1} ( #vp_{in,#P_{0}} - #vp_{in} ) #|= #| (#P_{1} + #P_{0}-#P_{0})  ( #vp_{in,#P_{0}} - #vp_{in} ) #| #leq #| (#P_{1} + #P_{0}-#P_{0}) #vp_{in,#P_{0}}  #|+ #| #P_{1} #vp_{in}  #| $$
#begin{equation}#label{p1.9}
#leq |#P_{0}| #cdot #| #vp_{in,#P_{0}} #| +#| (#P_{1}-#P_{0}) #vp_{in,#P_{0}}  #|+ #| #P_{1} #vp_{in}  #|= |#P_{0}|  +#| (#P_{1}-#P_{0}) #vp_{in,#P_{0}}  #|+ #| #P_{1} #vp_{in}  #|
#end{equation}

Ahora observamos que:
$$ #| (#P_{1}-#P_{0}) #vp_{in,#P_{0}}  #|^{2}= $$

$$#frac{1}{(#sigma^{2}#pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} (|#P_{1}|-|#P_{0}|)^{2} #left[ #exp#left(#frac{-|#P_{1}-#P_{0}|^{2}-|#P_{2}+#P_{0}|^{2} }{2#sigma^{2}} #right) #right]^{2} d#P_{1}#,d#P_{2} $$

$$=#frac{1}{(#sigma^{2}#pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #sigma^{2} (|#v{q}_{1}|-|#v{q}_{0}|)^{2} #left[ #exp#left(#frac{-|#v{q}_{1}-#v{q}_{0}|^{2}-|#v{q}_{2}+#v{q}_{0}|^{2} }{2} #right) #right]^{2} #sigma^{3} d#v{q}_{1}#, #sigma^{3} d#v{q}_{2} $$

$$=  #frac{#sigma^{2}}{#pi^{3}} #int_{#R^{3}} #int_{#R^{3}} (|#v{q}_{1}|-|#v{q}_{0}|)^{2} #left[ #exp#left(#frac{-|#v{q}_{1}-#v{q}_{0}|^{2}-|#v{q}_{2}+#v{q}_{0}|^{2} }{2} #right) #right]^{2} d#v{q}_{1}#, d#v{q}_{2} $$
$$= #sigma^{2} #| (#v{q}_{1}-#v{q}_{0}) (#psi_{in,#v{q}_{0}}-#psi_{in}) #|^{2}$$

Por lo anterior y ya que $#sigma #leq |#P_{0}|$ tenemos que,
#begin{equation}#label{p1.10}
#| (#P_{1}-#P_{0}) #vp_{in,#P_{0}}  #|=#sigma #| (#v{q}_{1}-#v{q}_{0}) (#psi_{in,#v{q}_{0}}-#psi_{in}) #| #leq |#P_{0}|#cdot #| (#v{q}_{1}-#v{q}_{0}) (#psi_{in,#v{q}_{0}}-#psi_{in}) #|
#end{equation}
#begin{equation}#label{p1.11}
#| #P_{1} #vp_{in,#P_{0}}  #|=#sigma #| #v{q}_{1} (#psi_{in,#v{q}_{0}}-#psi_{in}) #| #leq |#P_{0}|#cdot #| #v{q}_{1} (#psi_{in,#v{q}_{0}}-#psi_{in}) #|
#end{equation}
En donde #eqref{p1.11} se obtuvo de manera similar a #eqref{p1.10}.##
Luego por #eqref{p1.10} y #eqref{p1.11},
$$ #| #P_{1} ( #vp_{in,#P_{0}} - #vp_{in} ) #| #leq C#, |#P_{0}| $$
y de manera similar se tiene que,
$$ #| #P_{2} ( #vp_{in,#P_{0}} - #vp_{in} ) #| #leq C#, |#P_{0}| $$
As#'i por #eqref{p1.7.3} concluimos que para $1 #leq |#v{q}_{0}|$,
$$ #| #P#, (#vp_{in,#P_{0}}-#vp_{in}) #| #leq C#, |#P_{0}| $$
y por tanto #eqref{p1.2} se cumple. #qed

#begin{proposition}
#label{p2}
Sea:
#begin{equation}#label{tau}
#T ( #P^{2}/2m )= #S (#P^{2}/2m) - I
#end{equation}
Donde $I$ denota al operador identidad en $L^{2}(#mathbb{S}^{2})$. Entonces en la norma de $ #B(L^{2}(#mathbb{S}^{2})) $ se tiene que,
#begin{equation}#label{taucot1}
#| #T(#P^{2}/2m) #|_{#B(L^{2}(#mathbb{S}^{2}))} #leq C#, #frac{ |#P/#hbar| }{1+|#P/#hbar|}
#end{equation}

#end{proposition}
#n {#bf Demostraci#'on}##
A lo largo de esta demostraci#'on denotaremos por $#| #cdot #|$ a la norma de $#B(L^{2}(#mathbb{S}^{2}))$.##
Tomando el hecho de que $#| #S(#P^{2}/2m) #|= 1$, observamos que para todo valor de $#P^{2}$ se tiene:
#begin{equation}#label{p2.1}
#| #T(#P^{2}/2m) #|=#| #S(#P^{2}/2m)-I  #|#leq #|#S(#P^{2}/2m) #|+ #| I #|= 2 .
#end{equation}

#n Por otra parte, el teorema de $Kato-Jensen$ nos dice que en el l#'imite cuando $#left| #frac{#P}{#hbar} #right| #rightarrow 0 $ podemos escribir:
$$ #| #T ( #P^{2}/2m ) #|= #left#|  i #left| #frac{#P}{#hbar} #right| #Sigma_{1}^{0} - #left| #frac{#P}{#hbar} #right|^{2} #Sigma_{2}^{0}  #, #right#|$$
$$ #leq #left| #frac{#P}{#hbar} #right|#cdot #left#|  #Sigma_{1}^{0} #right#|+ #left| #frac{#P}{#hbar} #right|^{2} #left#|   #Sigma_{2}^{0}  #, #right#|  $$

#n Observamos que la derivada respecto a $|#P/#hbar|$ del #'ultimo miembro de la expresi#'on anterior es,
$$ #left#|  #Sigma_{1}^{0} #right#|+ 2 #, #left| #frac{#P}{#hbar} #right| #left#|   #Sigma_{2}^{0}  #, #right#| $$
Cuyo valor en $|#P|=0$ es $#left#|  #Sigma_{1}^{0} #right#|$ ; Por tanto si $C_{1}$ es una constante real tal que $#left#|  #Sigma_{1}^{0} #right#| < C_{1}$ y ya que estamos en el l#'imite $#left| #frac{#P}{#hbar} #right| #rightarrow 0 $, concluimos que:

#begin{equation}
#| #T ( #P^{2}/2m ) #| #leq C_{1}#, #left| #frac{#P}{#hbar} #right| #,#,#,;#,#,#, #left| #frac{#P}{#hbar} #right| #rightarrow 0
#end{equation}
Por otro lado supongamos que $1 #leq #left| #frac{#P}{#hbar} #right|$; Entonces en este caso se tiene:
$$ #frac{1}{2} #leq #frac{ |#P/#hbar| }{1+|#P/#hbar|} < 1 $$
Luego por #eqref{p2.1} y escogiendo una constante real $C_{2}$ lo suficientemente grande, a saber $4<C_{2}$, tenemos que:
$$ #| #T(#P^{2}/2m) #| #leq C_{2}#, #frac{ |#P/#hbar| }{1+|#P/#hbar|}  $$
Luego, tomando una constante real $C$ tal que, $C > #max #{ C_{1},C_{2} #}$, concluimos que #eqref{taucot1} se cumple.#qed ##

Ahora, como una consecuencia de la proposici#'on anterior, calcularemos una cota superior para $#| #T(#P^{2}/2m) #vp_{in}  #|$.##

#n Ya que $ #T(#P^{2}/2m) $ es un operador de $L^{2}(#S^{2})_{rel}$ es conveniente hacer el cambio de variable $ ( #P_{1},#P_{2} ) #rightarrow ( #P_{cm},#P ) $, as#'i tenemos que:

$$ #| #T(#P^{2}/2m) #vp_{in}  #|^{2} = #int_{#R^{3}} #int_{#R^{3}} #left|#T(#P^{2}/2m) #vp_{in}( #P_{cm},#P ) #right|^{2} d#P#, d#P_{cm}$$
$$ = #int_{#R^{3}} #int_{0}^{#infty} #int_{#S^{2}} #left|#T(p^{2}/2m) #vp_{in} #right|^{2} p^{2} #sen (#theta) #, d#theta #, d #phi #, dp #, d#P_{cm} $$
En donde en la #'ultima igualdad se realizo el cambio a coordenadas esf#'ericas, $ #P #rightarrow (p,#theta , #phi ) $. Se observa que:

$$ #int_{#S^{2}} #left|#T(p^{2}/2m) #vp_{in} #right|^{2} #sen (#theta) #, d#theta #, d #phi = #left#| #T(p^{2}/2m) #vp_{in}  #right#|_{L^{2}(#S^{2})}^{2} $$
Luego por #eqref{taucot1} tenemos que,

$$ #| #T(#P^{2}/2m) #vp_{in}  #|^{2} = #int_{#R^{3}} #int_{0}^{#infty}  #left#| #T(p^{2}/2m) #vp_{in}  #right#|_{L^{2}(#S^{2})}^{2}#, p^{2} #, dp #, d#P_{cm} $$
$$#leq C #int_{#R^{3}} #int_{0}^{#infty} #left| #frac{ |p/#hbar| }{1+|p/#hbar|} #right|^{2}#left#| #vp_{in} #right#|_{L^{2}(#S^{2})}^{2}#, p^{2} #, dp #, d#P_{cm}
#leq C #int_{#R^{3}} #int_{0}^{#infty} #left| #frac{p}{#hbar} #right|^{2}#left#| #vp_{in} #right#|_{L^{2}(#S^{2})}^{2}#, p^{2} #, dp #, d#P_{cm} $$
$$= C #int_{#R^{3}} #int_{0}^{#infty} #left#| #left| p/#hbar #right| #vp_{in} #right#|_{L^{2}(#S^{2})}^{2}#, p^{2} #, dp #, d#P_{cm} = C #, #frac{1}{#hbar^{2}}#, #| #P #, #vp_{in}  #|^{2} $$
Ahora desarrollemos $#| #P #, #vp_{in}  #|^{2}$ en las variables $#P_{1}#,,#, #P_{2}$ para despu#'es hacer el cambio, $ #P/#sigma #rightarrow #v{q} $ y $ #P_{1,2}/#sigma #rightarrow #v{q}_{1,2} $ ,

$$ #| #P#, #vp_{in}  #|^{2} = #frac{1}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #P^{2}#, #exp #left( #frac{-#P_{1}^{2} -#P_{2}^{2} }{#sigma^{2}} #right) #, d#P_{1} #, d#P_{2} $$
$$ = #frac{1}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #sigma^{2} #v{q}^{2}#, #exp #left( -#v{q}_{1}^{2} -#v{q}_{2}^{2}  #right) #, #sigma^{3} #, d#v{q}_{1} #, #sigma^{3} #, d#v{q}_{2} = #sigma^{2} #cdot #frac{1}{#pi^{3}} #int_{#R^{3}} #int_{#R^{3}}  #v{q}^{2}#, #exp #left( -#v{q}_{1}^{2} -#v{q}_{2}^{2}  #right) #, d#v{q}_{1} #, d#v{q}_{2} $$
$$ = #sigma^{2}#, #|#v{q}#, #psi_{in}  #|^{2} $$

#n De esta forma, $#| #T(#P^{2}/2m) #vp_{in}  #|$ esta acotada superiormente de la siguiente manera,

#begin{equation}#label{tau-phi-leq}
#| #T(#P^{2}/2m) #vp_{in}  #| #leq C #, #frac{#sigma}{#hbar} #, #|#v{q}#, #psi_{in}  #|
#end{equation}

Denotamos por,
#begin{equation}#label{lphi}
#L ( #phi_{1},#phi_{2},#phi_{3},#phi_{4} ) = #int_{#R^{12}} #phi_{1}(#P_{1},#P_{2}) #, #ol{ #phi_{2}(#P_{3},#P_{2}) } #, #phi_{3}(#P_{3},#P_{4}) #, #ol{ #phi_{4}(#P_{1},#P_{4}) } #, d#P_{1}#, d#P_{2}#, d#P_{3} #, d#P_{4}
#end{equation}
Notamos que cuando $#phi_{1}= #phi_{2}=#phi_{3}=#phi_{4}=#phi$ , tenemos que:
$$ #L(#phi,#phi,#phi,#phi)= P(#phi) $$
Por la desigualdad de Schwarz en $L^{2}(#R^{12})$ se tiene que,
#begin{equation}#label{lshw}
#left| #L ( #phi_{1},#phi_{2},#phi_{3},#phi_{4} ) #right|=#left| #left(#phi_{2} #phi_{4},#phi_{1} #phi_{3} #right)_{L^{2}(#R^{12})} #right| #leq
#left#| #phi_{2} #phi_{4} #right#|_{L^{2}(#R^{12})} #cdot #left#| #phi_{1} #phi_{3} #right#|_{L^{2}(#R^{12})} = #prod_{i=1}^{4} #| #phi_{i} #|_{L^{2}(#R^{6})}
#end{equation}

#newpage

El siguiente teorema es nuestra primera estimaci#'on de la pureza para bajas energ#'ias.

#begin{theorem}
Supongamos que la suposici#'on sobre el potencial $V$ se cumple y que en cero $H_{rel}$ no tiene una resonancia o un valor propio. Entonces,
#begin{equation}#label{est0}
P (#vp_{out,#P_{0}})= P (#vp_{out})+O(|#P_{0}/#hbar|)
#end{equation}
cuando, $|#P_{0}/#hbar| #rightarrow 0$ .
#end{theorem}

#n {#bf Demostraci#'on}##
Primero notamos que, $#vp_{out,#P_{0}}$ se puede escribir como:
$$ #vp_{out,#P_{0}} = #S(#P^{2}/2m) #vp_{in,#P_{0}} = #vp_{in,#P_{0}} +( #S(#P^{2}/2m) - I ) #vp_{in,#P_{0}} = #vp_{in,#P_{0}} + #T(#P^{2}/2m) #vp_{in,#P_{0}} $$
As#'i,
$$ P(#vp_{out,#P_{0}})= P #left( #vp_{in,#P_{0}} + #T(#P^{2}/2m) #vp_{in,#P_{0}} #right) $$
Desarrollando el t#'ermino derecho de la expresi#'on anterior tenemos por la ecuaci#'on #eqref{p:estin},
$$ P #left(#vp_{in,#P_{0}} + #T(#P^{2}/2m) #vp_{in,#P_{0}} #right) $$
$$ = #int_{#R^{12}} #vp_{in,#P_{0}}(#P_{1},#P_{2}) #, #ol{ #vp_{in,#P_{0}}(#P_{3},#P_{2}) } #, #vp_{in,#P_{0}}(#P_{3},#P_{4}) #, #ol{ #vp_{in,#P_{0}}(#P_{1},#P_{4}) } #, d#P_{1} ... d#P_{4} + #sum_{i=1}^{15} #L_{i}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) $$
$$= P(#vp_{in,#P_{0}}) + #sum_{i=1}^{15} #L_{i}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) = 1 + #sum_{i=1}^{15} #L_{i}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) $$
Aqu#'i introdujimos $#L_{i}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})$ que se define en funci#'on de #eqref{lphi} de la siguiente manera,
#begin{equation}#label{li_p0}
#L_{i}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) := #L(#psi_{1},#psi_{2},#psi_{3},#psi_{4})
#end{equation}
en donde $k$, ($ 1 #leq k #leq 4 $) de las $#psi_{i}$ son iguales a $#T(#P^{2}/2m) #vp_{in,#P_{0}} $ y las $4-k$ restantes son iguales a $#vp_{in,#P_{0}}$ . Definiendo,
#begin{equation}#label{R_p0}
#Rr (#P_{0}) :=#sum_{i=1}^{15} #L_{i}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})
#end{equation}
Tenemos que,
#begin{equation}#label{p_phiout_p0}
P(#vp_{out,#P_{0}})= 1 + #Rr (#P_{0})
#end{equation}

#n Por otro lado, ya que $#vp_{out}$ es un caso particular de $#vp_{out,#P_{0}}$ cuando $#P_{0}=0$, es claro que la pureza de este estado est#'a dada por,
#begin{equation}#label{p_phiout_0}
P(#vp_{out})= 1 + #Rr (0)
#end{equation}
En donde,
#begin{equation}#label{R0}
#Rr (0) :=#sum_{i=1}^{15} #L_{i}(0, #psi_{1},#psi_{2},#psi_{3},#psi_{4})
#end{equation}

Ahora, sin p#'erdida de generalidad podemos suponer que,

#begin{equation}
#L_{1}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})=
#L(#T (#P^{2}/2m) #vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}} )
#end{equation}

#newpage

Haciendo, $#T (#P^{2}/2m) #vp_{in,#P_{0}}= #T (#P^{2}/2m) #vp_{in}+ #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} )  $ en la ecuaci#'on anterior y por #eqref{lphi},
$$ #L_{1}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})=  $$
#begin{equation}#label{l12}
#L(#T (#P^{2}/2m) #vp_{in},#vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}} )+
#L( #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) ,#vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}} )
#end{equation}

#n Ahora prestemos atenci#'on al segundo t#'erminos del lado derecho de la ecuaci#'on anterior. Por #eqref{lshw} tenemos que,
$$ #left|#L( #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) ,#vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}} ) #right| #leq $$
$$ #| #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) #| #cdot #| #vp_{in,#P_{0}} #|^{3} = #| #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) #| $$

#n Luego por las ecuaciones #eqref{p1.2} , #eqref{taucot1} y tomando el hecho de que $ #left| #frac{#P}{#hbar} #right| #rightarrow 0 $, podemos acotar a $#| #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) #|$ de la siguiente forma,
$$ #| #T(#P^{2}/2m) ( #vp_{in,#P_{0}} - #vp_{in} ) #|^{2}= #int_{#R^{3}} #int_{0}^{#infty} #left#| #T(p^{2}/2m) ( #vp_{in,#P_{0}} - #vp_{in} ) #right#|_{L^{2}(#S^{2})}^{2} p#, dp #, d #P_{cm} $$
$$ #leq C #int_{#R^{3}} #int_{0}^{#infty} #left| #frac{p}{#hbar} #right|^{2} #cdot #left#| #vp_{in,#P_{0}} - #vp_{in}  #right#|_{L^{2}(#S^{2})}^{2} p#, dp #, d #P_{cm} = C#, #frac{1}{#hbar^{2}} #int_{#R^{3}} #int_{0}^{#infty} #left#|p#, (#vp_{in,#P_{0}} - #vp_{in})  #right#|_{L^{2}(#S^{2})}^{2} p#, dp #, d #P_{cm}  $$
$$ =C #, #frac{1}{#hbar^{2}} #left#| #P(#vp_{in,#P_{0}} - #vp_{in}) #right#|^{2} #leq C #, #left|#frac{#P_{0}}{#hbar} #right|^{2} $$
As#'i tenemos que, $ #| #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) #| #leq C#, |#P_{0}/#hbar| $. Luego,
$$ #left|#L( #T (#P^{2}/2m)( #vp_{in,#P_{0}} - #vp_{in} ) ,#vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}} ) #right| #leq C #, #left|#frac{#P_{0}}{#hbar} #right| $$

#n Por lo anterior, en el l#'imite cuando $#left| #frac{#P}{#hbar} #right| #rightarrow 0$, podemos estimar a $#L_{1}$ de la siguiente forma,
#begin{equation} #label{l101}
#L_{1}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})=
#L(#T (#P^{2}/2m) #vp_{in},#vp_{in,#P_{0}},#vp_{in,#P_{0}},#vp_{in,#P_{0}} )+ O #left( #left|#P_{0}/#hbar #right| #right)
#end{equation}
Haciendo $ #vp_{in,#P_{0}}= ( #vp_{in,#P_{0}} - #vp_{in} )+ #vp_{in} $ en la segunda entrada del lado derecho de la ecuaci#'on #eqref{l101} tenemos por #eqref{lphi},
$$#L_{1}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})= #L(#T (#P^{2}/2m) #vp_{in},( #vp_{in,#P_{0}} - #vp_{in} ),#vp_{in,#P_{0}},#vp_{in,#P_{0}} ) $$
#begin{equation}#label{ap1}
+ #L(#T (#P^{2}/2m) #vp_{in},#vp_{in},#vp_{in,#P_{0}},#vp_{in,#P_{0}} )+ O #left( #left|#P_{0}/#hbar #right| #right)
#end{equation}

#n Ahora prestemos atenci#'on al termino $ #L(#T (#P^{2}/2m) #vp_{in},( #vp_{in,#P_{0}} - #vp_{in} ),#vp_{in,#P_{0}},#vp_{in,#P_{0}} ) $ de la ecuaci#'on anterior. Por las ecuaciones #eqref{p1.1} , #eqref{tau-phi-leq} y #eqref{lshw} tenemos que,
$$ #left| #L(#T (#P^{2}/2m) #vp_{in},( #vp_{in,#P_{0}} - #vp_{in} ),#vp_{in,#P_{0}},#vp_{in,#P_{0}} ) #right| #leq #| #T (#P^{2}/2m) #vp_{in} #| #cdot #| #vp_{in,#P_{0}} - #vp_{in} #| #cdot #| #vp_{in,#P_{0}} #|^{2} $$
$$ = #| #T (#P^{2}/2m) #vp_{in} #| #cdot #| #vp_{in,#P_{0}} - #vp_{in} #| #leq C #, #frac{#sigma}{#hbar}#, #| #v{q} #, #psi_{in} #| #cdot #left|#frac{#P_{0}}{#sigma} #right| #leq C #, #left|#frac{#P_{0}}{#hbar} #right| $$
As#'i, en el l#'imite cuando $#left| #frac{#P}{#hbar} #right| #rightarrow 0$, podemos estimar a $#L_{1}$ de la siguiente forma,
#begin{equation}#label{ap2}
#L_{1}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})=
#L(#T (#P^{2}/2m) #vp_{in},#vp_{in},#vp_{in,#P_{0}},#vp_{in,#P_{0}} )+ O #left( #left|#P_{0}/#hbar #right| #right)
#end{equation}
Repitiendo este proceso dos veces m#'as, concluimos que para $#left| #frac{#P}{#hbar} #right| #rightarrow 0$, podemos estimar a $#L_{1}$ como:
$$#L_{1}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})=
#L(#T (#P^{2}/2m) #vp_{in},#vp_{in},#vp_{in},#vp_{in} )+ O #left( #left|#P_{0}/#hbar #right| #right) $$
#begin{equation}#label{ap3}
= #L_{1}(0, #psi_{1},#psi_{2},#psi_{3},#psi_{4})+O #left( #left|#P_{0}/#hbar #right| #right)
#end{equation}
Para $#L_{j}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4})$ arbitrario, podemos estimar su valor repitiendo los argumentos anteriores las veces que sean necesarias, para obtener:
#begin{equation}#label{liest}
#L_{j}(#P_{0}, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) = #L_{j}(0, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) + O(|#P_{0}/#hbar|)
#end{equation}

As#'i, por las ecuaciones #eqref{R_p0},#eqref{p_phiout_p0},#eqref{p_phiout_0},#eqref{R0} y #eqref{liest}, concluimos que en el l#'imite cuando $#left| #frac{#P}{#hbar} #right| #rightarrow 0$, la pureza del estado $#vp_{out,#P_{0}}$ se puede estimar como:
$$#P (#vp_{out,#P_{0}})=
1+#sum_{i=1}^{15} #L_{i}(0, #psi_{1},#psi_{2},#psi_{3},#psi_{4})+ O(|#P_{0}/#hbar|) $$
#begin{equation}
= #P (#vp_{out})+O(|#P_{0}/#hbar|)
#end{equation}
#qed
#vspace{3 mm}

#n Denotamos por,
#begin{equation}#label{tao1}
#T_{1}( #P^{2}/2m ) := #S ( #P^{2}/2m ) - I -i |#P/#hbar| #Sigma_{1}^{0} + |#P/#hbar|^{2} #Sigma_{2}^{0}
#end{equation}

#n Es claro que por el teorema de Kato - Jensen,
#begin{equation}#label{tao1nor}
#left#| #T_{1}( #P^{2}/2m )  #right#|_{#B(L^{2}(#S^{2}))} #leq #begin{cases} #begin{matrix} |#P/#hbar|^{2} o(1) & si#; #beta >5 ## |#P/#hbar|^{2} O(|#P/#hbar|) & si#; #beta > 7 #end{matrix} #end{cases}
#end{equation}
En donde $o(1)$ y $O(|#P/#hbar|)$ son funciones acotadas de $|#P/#hbar|$ tales que, $#lim_{|#P/#hbar| #rightarrow 0} o(1)=0$ y ##
$ O(|#P/#hbar|) #leq C #, |#P/#hbar|  $ para $|#P/#hbar| #leq 1 $.

#n Ahora calcularemos a primer orden la pureza del estado $#vp_{out}$.
#begin{theorem}
Supongamos que la suposici#'on sobre el potencial $V$ se cumple y que en cero $H_{rel}$ no tiene una resonancia o un valor propio. Entonces en el l#'imite cuando $#sigma/#hbar #rightarrow 0 $,
$$ #P( #vp_{out} )= #P #left( #left[ I+i|#P/#hbar|#Sigma_{1}^{0} -|#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right) $$
#begin{equation}#label{1.4.0}
+#begin{cases} #begin{matrix} o(|#sigma/#hbar|^{2}) & si#; #beta >5 ## O(|#sigma/#hbar|^{3}) & si#; #beta > 7 #end{matrix} #end{cases}
#end{equation}

#end{theorem}
#n {#bf Demostraci#'on}##
Primero escribimos a $#vp_{out}$ de la siguiente manera,
$$ #vp_{out}= #vp_{out,1} + #T_{1} (#P^{2}/2m) #vp_{in} $$
En donde,
#begin{equation}
#vp_{out,1}= #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in}
#end{equation}
As#'i, usando esta descomposici#'on tenemos que la pureza de $#vp_{out}$ est#'a dada por,
$$ #P (#vp_{out})= #P #left(#vp_{out,1} + #T_{1} (#P^{2}/2m) #vp_{in} #right) $$
$$= #int_{#R^{12}} #vp_{out,1}(#P_{1},#P_{2}) #, #ol{ #vp_{out,1}(#P_{2},#P_{3}) } #, #vp_{out,1}(#P_{3},#P_{4}) #, #ol{ #vp_{out,1}(#P_{1},#P_{4}) } #, d#P_{1} ... d#P_{4} + #sum_{i=1}^{15} #L_{i}(#sigma, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) $$
$$= #P(#vp_{out,1}) + #sum_{i=1}^{15} #L_{i}(#sigma, #psi_{1},#psi_{2},#psi_{3},#psi_{4}) $$
Aqu#'i introdujimos $#L_{i}(#sigma, #psi_{1},#psi_{2},#psi_{3},#psi_{4})$ que se define en funci#'on de #eqref{lphi} de la siguiente manera,
$$ #L_{i}(#sigma, #psi_{1},#psi_{2},#psi_{3},#psi_{4})=#L( #psi_{1},#psi_{2},#psi_{3},#psi_{4}) $$
En donde $k$ ($ 1 #leq k #leq 4 $) de las $#psi_{j}$ son iguales a $#T_{1}(#P^{2}/2m) #vp_{in} $ y las $4-k$ restantes son iguales a $#vp_{out,1}$ . Definiendo,
#begin{equation}#label{1.4.4}
#Rr(#sigma):= #sum_{i=1}^{15} #L_{i}(#sigma, #psi_{1},#psi_{2},#psi_{3},#psi_{4})
#end{equation}
Tenemos que,
#begin{equation}#label{1.4.5}
#P (#vp_{out})= #P(#vp_{out,1})+#Rr (#sigma)
#end{equation}
Ahora tomemos $#beta > 7$ y sin p#'erdida de generalidad tomemos tambi#'en,
#begin{equation}#label{1.4.1}
#L_{15}(#sigma,#psi_{1},#psi_{2},#psi_{3},#psi_{4})=#L #left(#vp_{out,1},#vp_{out,1},#vp_{out,1},#T_{1}(#P^{2}/2m)#vp_{in} #right)
#end{equation}
Luego por #eqref{lshw} tenemos que,
$$ #left| #L #left(#vp_{out,1},#vp_{out,1},#vp_{out,1},#T_{1}(#P^{2}/2m)#vp_{in} #right) #right| $$
#begin{equation}#label{1.4.2}
#leq #| #vp_{out,1} #|^{3}#cdot #left#| #T_{1}(#P^{2}/2m)#vp_{in} #right#| #leq C #, #left#| #T_{1}(#P^{2}/2m)#vp_{in} #right#|
#end{equation}
Para alguna constante real $C$. Ahora por la ecuaci#'on #eqref{tao1nor} y tomando el hecho de que $O(|#P/#hbar|)#leq C#, |#P/#hbar| $ en el l#'imite cuando $ #left| #frac{#P}{#hbar} #right| #rightarrow 0 $ tenemos que,
$$ #left#| #T_{1}(#P^{2}/2m)#vp_{in} #right#|^{2} = #int_{#R^{3}} #int_{0}^{#infty} #left#| #T_{1}(p^{2}/2m)#vp_{in} #right#|_{L^{2}(#S^{2})}^{2} p#, dp#, d#P_{cm} $$
$$#leq #int_{#R^{3}} #int_{0}^{#infty} #left#| #T_{1}(p^{2}/2m) #right#|^{2} #left#|#vp_{in} #right#|_{L^{2}(#S^{2})}^{2} p#, dp#, d#P_{cm}
#leq C #int_{#R^{3}} #int_{0}^{#infty} #left| #frac{#P^{3}}{#hbar^{3}} #right|^{2} #left#|#vp_{in} #right#|_{L^{2}(#S^{2})}^{2} p#, dp#, d#P_{cm} $$
$$= C#,#left|#frac{1}{#hbar^{3}} #right|^{2} #int_{#R^{3}} #int_{0}^{#infty}  #left#| #P^{3}  #vp_{in} #right#|_{L^{2}(#S^{2})}^{2} p#, dp#, d#P_{cm} = C#, #left|#frac{1}{#hbar^{3}} #right|^{2} #, #left#| #P^{3}#, #vp_{in} #right#|^{2} $$

#n Ahora desarrollemos $#| #P^{3} #, #vp_{in}  #|^{2}$ en las variables $#P_{1}#,,#, #P_{2}$ para despu#'es hacer el cambio, $ #P/#sigma #rightarrow #v{q} $ y $ #P_{1,2}/#sigma #rightarrow #v{q}_{1,2} $ ,

$$ #| #P^{3}#, #vp_{in}  #|^{2} = #frac{1}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #P^{6}#, #exp #left( #frac{-#P_{1}^{2} -#P_{2}^{2} }{#sigma^{2}} #right) #, d#P_{1} #, d#P_{2} $$
$$ = #frac{1}{(#sigma^{2} #pi)^{3}} #int_{#R^{3}} #int_{#R^{3}} #sigma^{6} #v{q}^{6}#, #exp #left( -#v{q}_{1}^{2} -#v{q}_{2}^{2}  #right) #, #sigma^{3} #, d#v{q}_{1} #, #sigma^{3} #, d#v{q}_{2} = #sigma^{6} #cdot #frac{1}{#pi^{3}} #int_{#R^{3}} #int_{#R^{3}}  #v{q}^{6}#, #exp #left( -#v{q}_{1}^{2} -#v{q}_{2}^{2}  #right) #, d#v{q}_{1} #, d#v{q}_{2} $$
$$ = #sigma^{6}#, #|#v{q}#, #psi_{in}  #|^{2} $$
As#'i para alguna constante $C >0 $, se tiene que:
#begin{equation}#label{1.4.3}
#left#| #T_{1}(#P^{2}/2m)#vp_{in} #right#| #leq C #,#left|#frac{#sigma^{3}}{#hbar^{3}} #right|
#end{equation}
Luego por las ecuaciones #eqref{1.4.1}, #eqref{1.4.2} y #eqref{1.4.3} concluimos que,
#begin{equation}#label{1.4.6}
#left| #L_{15}(#sigma,#psi_{1},#psi_{2},#psi_{3},#psi_{4}) #right| #leq C #,#left|#frac{#sigma^{3}}{#hbar^{3}} #right|
#end{equation}
Notemos que por la ecuaci#'on #eqref{lshw} el argumento anterior es v#'alido para toda $  #L_{i}(#sigma,#psi_{1},#psi_{2},#psi_{3},#psi_{4}) $, en donde una de las $#psi_{j}$ es igual a $#T_{1}(#P^{2}/2m)#vp_{in}$ y las otras tres son iguales a $#vp_{out,1}$.##

#n En el caso cuando $#L_{i}(#sigma,#psi_{1},#psi_{2},#psi_{3},#psi_{4})$ tenga $l$ ($2#leq l #leq 4$) de las $#psi_{j}$ iguales a $#T_{1}(#P^{2}/2m)#vp_{in}$ y las $4-l$ restantes iguales a $#vp_{out,1}$ se tiene que, por las ecuaciones #eqref{lshw}, #eqref{1.4.3} y an#'alogamente a como se estimo el modulo de $#L_{15}$,
$$ #left| #L_{i}(#sigma,#psi_{1},#psi_{2},#psi_{3},#psi_{4}) #right| #leq C #,#left|#frac{#sigma^{3}}{#hbar^{3}} #right|^{l} #,#,#,,#, 2 #leq l #leq 4 $$
Pero, ya que estamos en el l#'imite cuando $#sigma / #hbar #rightarrow 0$ estos t#'erminos de orden $l$ quedan dominados por los t#'erminos de orden uno. As#'i, por el argumento anterior y por las ecuaciones #eqref{1.4.4}, #eqref{1.4.5} y #eqref{1.4.6} concluimos que para $#beta > 7$,
$$ #P (#vp_{out})= #P(#vp_{out,1})+O(|#sigma^{3}/#hbar^{3}|) $$
$$ = #P #left( #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right) + O(|#sigma^{3}/#hbar^{3}|) $$
De manera an#'aloga para el caso en el que $#beta >5$ concluimos que,
$$ #P (#vp_{out})= #P(#vp_{out,1})+o(|#sigma^{2}/#hbar^{2}|) $$
$$ = #P #left( #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right) + o(|#sigma^{2}/#hbar^{2}|) $$
Por tanto la estimaci#'on #eqref{1.4.0} se cumple. 
#qed
#vspace{5 mm}

Por la definici#'on del momento relativo $#P$ y del momento del centro de masa $#P_{cm}$ tememos que,
#begin{subequations}#label{p1-p2-p-pcm}
#begin{align}
#P_{1}= #mu_{1} #P_{cm} + #P##
#P_{2}= #mu_{2} #P_{cm} - #P
#end{align}
#end{subequations}
Es claro que,
$$ #P_{1}^{2} = #P_{1} #cdot #P_{1}=(#mu_{1} #P_{cm} + #P) #cdot (#mu_{1} #P_{cm} + #P)= #P^{2}+2#, #mu_{1} #P_{cm} #cdot #P + #mu_{1}^{2}#, #P_{cm}^{2}  $$
$$ #P_{2}^{2} = #P_{2} #cdot #P_{2}=(#mu_{2} #P_{cm} - #P) #cdot (#mu_{2} #P_{cm} - #P)= #P^{2}-2#, #mu_{2} #P_{cm} #cdot #P + #mu_{2}^{2}#, #P_{cm}^{2} $$
Haciendo este cambio en la ecuaci#'on #eqref{estinp0} obtenemos,
#begin{equation}#label{estinpcm}
#vp_{in}(#P,#P_{cm})= #frac{1}{(#sigma^{2} #pi)^{3/2}} #, e^{-#frac{#P^{2}+(#mu_{1}-#mu_{2}) #P_{cm}#cdot #, #P}{#sigma^{2}} } #, e^{ -#frac{(#mu_{1}^{2}+#mu_{2}^{2} ) }{2 #sigma^{2}}#,#P_{cm}^{2} }
#end{equation}
De forma similar, haciendo el cambio de variable
#begin{subequations}#label{q1-q2-q-qcm}
#begin{align}
#v{q}_{1}= #mu_{1} #v{q}_{cm} + #v{q}##
#v{q}_{2}= #mu_{2} #v{q}_{cm} - #v{q}
#end{align}
#end{subequations}
En la ecuaci#'on #eqref{psi-est} obtenemos,
#begin{equation}#label{estinqcm}
#psi_{in}(#q,#q_{cm})= #frac{1}{#pi^{3/2}} #, e^{-#v{q}^{2}-(#mu_{1}-#mu_{2}) #v{q}_{cm}#cdot#, #v{q} } #, e^{ -#frac{(#mu_{1}^{2}+#mu_{2}^{2} )}{2} #,#v{q}_{cm}^{2}}
#end{equation}

Sea $#vp #in L^{2}(#R^{6})$ un estado cu#'antico normalizado. Denotamos por $#I (#vp)$ al integrando de la pureza $#P(#vp)$,
#begin{equation}#label{intp}
#I(#vp) := #vp(#P_{1},#P_{2}) #, #ol{#vp(#P_{3},#P_{2})}#, #vp(#P_{3},#P_{4})#, #ol{#vp(#P_{1},#P_{4})}
#end{equation}

#n As#'i, haciendo el cambio de variable $#v{q}_{i}= #P_{i}/#sigma$ tenemos que,
$$ #P #left( #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right)= #int_{#R^{12}} #I #left( #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right) #, d#P_{1} #, d#P_{2}#, d#P_{3}#, d#P_{4} $$
$$= #int_{#R^{12}} #I #left( #left[I + i#, #sigma #,|#v{q}/#hbar| #Sigma_{1}^{0} -#sigma^{2}#, |#v{q}/#hbar|^{2} #Sigma_{2}^{0} #right]#frac{1}{#sigma^{3}}#, #psi_{in} #right) #, #sigma^{12}#, d#v{q}_{1} #, d#v{q}_{2}#, d#v{q}_{3}#, d#v{q}_{4} $$
#begin{equation}#label{Ppsi}
= #int_{#R^{12}} #I #left( #left[I + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0} - (#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0} #right] #psi_{in} #right) #, d#v{q}_{1} #, d#v{q}_{2}#, d#v{q}_{3}#, d#v{q}_{4}
#end{equation}

#n Ahora, ya que estamos en el l#'imite cuando $ |#P/#hbar|#rightarrow 0 $ y $|#sigma/#hbar|#rightarrow 0$ podemos desarrollar el integrando##
$#I #left( #left[I + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0} - (#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0} #right] #psi_{in} #right)$ hasta t#'erminos de orden $ |#sigma/#hbar|^{2} $ de la siguiente forma,
#begin{equation}#label{expp}
#I #left( #left[I + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0} - (#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0} #right] #psi_{in} #right)=#A_{0} + (#sigma/#hbar)#, #A_{1} + (#sigma/#hbar)^{2}#, #A_{2}+ O(|#sigma/#hbar|^{3})
#end{equation}
Con $#A_{0},#A_{1},#A_{2}$ funciones de $#P_{1},#P_{2},#P_{3},#P_{4}$. De esta forma, integrando #eqref{expp} obtendremos una expansi#'on hasta t#'erminos de orden $(#sigma/#hbar)^{2}$ de la pureza #eqref{Ppsi},
$$#P #left( #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right) $$
$$=#int_{#R^{12}} #A_{0}#,#, d#v{q}_{1}#,...#,d#v{q}_{4} + #frac{#sigma}{#hbar} #int_{#R^{12}} #A_{1} #, d#v{q}_{1}#,...#,d#v{q}_{4}+
#left(#frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}} #A_{2} #, d#v{q}_{1}#,...#,d#v{q}_{4}+O(|#sigma/#hbar|^{3}) $$
#begin{equation}#label{pb}
= B_{0}  + #frac{#sigma}{#hbar}#, B_{1} +
#left(#frac{#sigma}{#hbar} #right)^{2} B_{2} + O(|#sigma/#hbar|^{3})
#end{equation}
En donde $B_{0},B_{1},B_{2}$ son n#'umeros reales.##
#n Para esto primero denotamos por,
#begin{subequations}#label{p1-p2-p-pcm}
#begin{align}
a_{1}+b_{1}+c_{1}= #psi_{in}(#v{q}_{1},#v{q}_{2}) + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0}#psi_{in}(#v{q}_{1},#v{q}_{2}) -(#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0}#psi_{in}(#v{q}_{1},#v{q}_{2}) ##
a_{2}+b_{2}+c_{2}= #psi_{in}(#v{q}_{3},#v{q}_{2}) + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0}#psi_{in}(#v{q}_{3},#v{q}_{2}) -(#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #,  #Sigma_{2}^{0}#psi_{in}(#v{q}_{3},#v{q}_{2}) ##
a_{3}+b_{3}+c_{3}= #psi_{in}(#v{q}_{3},#v{q}_{4}) + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0}#psi_{in}(#v{q}_{3},#v{q}_{4}) -(#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0}#psi_{in}(#v{q}_{3},#v{q}_{4})##
a_{4}+b_{4}+c_{4}= #psi_{in}(#v{q}_{1},#v{q}_{4}) + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0}#psi_{in}(#v{q}_{1},#v{q}_{4}) -(#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #,  #Sigma_{2}^{0}#psi_{in}(#v{q}_{1},#v{q}_{4})
#end{align}
#end{subequations}
En donde,
$$ (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0}#psi_{in}(#v{q}_{i},#v{q}_{j})= (#sigma/#hbar)#, |#mu_{2}#v{q}_{i}-#mu_{1}#v{q}_{j}|#, #Sigma_{1}^{0}#psi_{in}(#v{q}_{i},#v{q}_{j}) $$
$$ (#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #,  #Sigma_{2}^{0}#psi_{in}(#v{q}_{i},#v{q}_{j})=(#sigma^{2}/#hbar^{2})#, |#mu_{2}#v{q}_{i}-#mu_{1}#v{q}_{j}|^{2} #,  #Sigma_{2}^{0}#psi_{in}(#v{q}_{i},#v{q}_{j}) $$

#n con la correspondencia,
$$ a_{i} #longleftrightarrow #psi_{in} $$
$$ b_{i} #longleftrightarrow i#,(#sigma/#hbar)#,|#v{q}|#,#Sigma_{1}^{0}#psi_{in} $$
$$ c_{i} #longleftrightarrow -(#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #,  #Sigma_{2}^{0}#psi_{in} $$

#n As#'i, el integrando $#I #left( #left[I + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0} - (#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0} #right] #psi_{in} #right)$ se puede escribir como,
$$#I #left( #left[I + i#, (#sigma/#hbar)#, |#v{q}|#, #Sigma_{1}^{0} - (#sigma^{2}/#hbar^{2})#, |#v{q}|^{2} #, #Sigma_{2}^{0} #right] #psi_{in} #right) $$
#begin{equation}#label{p-abc}
= (a_{1}+b_{1}+c_{1}) #ol{(a_{2}+b_{2}+c_{2})} (a_{3}+b_{3}+c_{3}) #ol{(a_{4}+b_{4}+c_{4})}
#end{equation}
De las ecuaciones #eqref{p1-p2-p-pcm} y #eqref{p-abc} observamos que el termino de orden cero $#A_{0}$ del integrando de la pureza es,
#begin{equation}#label{A0}
#A_{0}= a_{1}#, #ol{a_{2}}#, a_{3}#, #ol{a_{4}}= #psi_{in}(#q_{1},#q_{2})#, #psi_{in}(#q_{3},#q_{2}) #, #psi_{in}(#q_{3},#q_{4})#, #psi_{in}(#q_{1},#q_{4}) = #I(#psi_{in})
#end{equation}
Luego por las ecuaciones #eqref{psi-est},#eqref{intp}, #eqref{pb} y #eqref{A0} tenemos que el termino de orden cero $B_{0}$ de la pureza esta dado por,
#begin{equation}#label{B0}
B_{0}= #int_{#R^{12}} #A_{0} #,d#v{q}_{1}#,...#,d#v{q}_{4}=#int_{#R^{12}} I(#psi_{in}) #,d#v{q}_{1}#,...#,d#v{q}_{4}= #P(#psi_{in})=1
#end{equation}

Ahora obtengamos el termino de orden uno $#A_{1} |#sigma/#hbar|$ del integrando de la pureza. De las ecuaciones #eqref{p1-p2-p-pcm} y #eqref{p-abc} observamos los t#'erminos de orden $|#sigma/#hbar|$ del integrando de la pureza son,

$$ b_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}} #,#,,#,#, a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{a_{4}} $$
$$ a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{a_{4}} #,#,,#,#,  a_{1}#, #overline{a_{2}} #, a_{3}#, #overline{b_{4}} $$
Es claro que por la ecuaci#'on #eqref{expp},
#begin{equation}#label{A1}
#A_{1} |#sigma/#hbar|= b_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}}+ a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{a_{4}}+a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{a_{4}} +  a_{1}#, #overline{a_{2}} #, a_{3}#, #overline{b_{4}}
#end{equation}
y que por la ecuaci#'on #eqref{pb} el termino de primer orden $B_{1}|#sigma/#hbar|$ de la pureza esta dado por
#begin{equation}#label{A1int}
B_{1}|#sigma/#hbar|= #int_{#R^{12}} b_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}}+ a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{a_{4}}+a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{a_{4}} +  a_{1}#, #overline{a_{2}} #, a_{3}#, #overline{b_{4}} #, d#q_{1},...,d#q_{4}
#end{equation}

#n Pero ya que en el c#'alculo de la pureza las variables $#v{q}_{1},...,#v{q}_{4}$ son mudas tenemos que,
$$#int_{#R^{12}} b_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}} #,d#v{q}_{1}#, ...#, d#v{q}_{4}= #int_{#R^{12}} a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{a_{4}} #,d#v{q}_{1}#, ... #, d#v{q}_{4} $$
$$#int_{#R^{12}} a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{a_{4}} #,d#v{q}_{1}#, ... #, d#v{q}_{4}=#int_{#R^{12}} a_{1}#, #overline{a_{2}} #, a_{3}#, #overline{b_{4}} #,d#v{q}_{1}#, ...#, d#v{q}_{4} $$
Luego observamos que,
$$#int_{#R^{12}} b_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}} #,d#v{q}_{1}#,...#,d#v{q}_{4} + #int_{#R^{12}} a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{a_{4}} #,d#v{q}_{1}#,...#,d#v{q}_{4} $$
#begin{equation}#label{p11}
=2 i #int_{#R^{12}} #frac{#sigma}{#hbar}#,|#v{q}| #, #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{2})#, #psi_{in}(#q_{3},#q_{2}) #, #psi_{in}(#q_{3},#q_{4})#, #psi_{in}(#q_{1},#q_{4})    #,d#v{q}_{1}...d#v{q}_{4}
#end{equation}

$$#int_{#R^{12}} a_{1}#,#overline{b_{2}}#,a_{3}#,#overline{a_{4}} #,d#v{q}_{1}...d#v{q}_{4} + #int_{#R^{12}} a_{1}#, #overline{a_{2}}#, a_{3}#, #overline{b_{4}} #,d#v{q}_{1}...d#v{q}_{4} $$
#begin{equation}#label{p12}
=-2i #int_{#R^{12}} #frac{#sigma}{#hbar}#,|#v{q}| #, #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{2})#, #psi_{in}(#q_{3},#q_{2}) #, #psi_{in}(#q_{3},#q_{4})#, #psi_{in}(#q_{1},#q_{4})    #,d#v{q}_{1}...d#v{q}_{4}
#end{equation}

As#'i, por las ecuaciones #eqref{A1} y #eqref{A1int}, el termino de orden uno $B_{1}|#sigma/#hbar|$ de la pureza, esta dado por la suma de las ecuaciones #eqref{p11} y #eqref{p12} cuya suma es claramente cero. Por tanto,
#begin{equation}#label{B1}
B_{1}|#sigma/#hbar|= 0
#end{equation}

#newpage

Ahora obtengamos el termino de orden dos $B_{2}|#sigma/#hbar|^{2}$ de la pureza. Para esto, de las ecuaciones #eqref{p1-p2-p-pcm} y #eqref{p-abc} observamos que los t#'erminos de orden $|#sigma/#hbar|^{2}$, del integrando de la pureza, se pueden dividir en dos grupos. El primero es el que consiste de t#'erminos que son el producto de dos coeficientes $a_{i}$ y dos $b_{i}$. As#'i el primer grupo es,
$$ b_{1}#,#overline{b_{2}}#,a_{3}#,#overline{a_{4}}  #,#,,#,#,  a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{b_{4}}$$
$$ b_{1}#, #overline{a_{2}} #, a_{3} #, #overline{b_{4}} #,#,,#,#, a_{1}#, #overline{b_{2}} #, b_{3}#, #overline{a_{4}} $$
$$a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{b_{4}} #,#,,#,#, b_{1}#, #overline{a_{2}} #, b_{3}#, #overline{a_{4}} $$
M#'as aun, ya que en el c#'alculo de la pureza las variables $#P_{1},...,#P_{4}$ son mudas, tenemos que,
#begin{equation}#label{g1a}
#int_{#R^{12}} b_{1}#,#overline{b_{2}}#,a_{3}#,#overline{a_{4}} #,d#P_{1}...d#P_{4} = #int_{#R^{12}} a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{b_{4}} #,d#P_{1}...d#P_{4}
#end{equation}
#begin{equation}#label{g1b}
#int_{#R^{12}} b_{1}#, #overline{a_{2}} #, a_{3} #, #overline{b_{4}} #,d#P_{1}...d#P_{4} = #int_{#R^{12}} a_{1}#, #overline{b_{2}} #, b_{3}#, #overline{a_{4}} #,d#P_{1}...d#P_{4}
#end{equation}
#begin{equation}#label{g1c}
#int_{#R^{12}} a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{b_{4}} #,d#P_{1}...d#P_{4} = #int_{#R^{12}} b_{1}#, #overline{a_{2}} #, b_{3}#, #overline{a_{4}} #,d#P_{1}...d#P_{4}
#end{equation}
Desarrollando el lado izquierdo de la ecuaci#'on #eqref{g1a} tenemos que por las ecuaciones #eqref{psi} y #eqref{psi-est},
$$#int_{#R^{12}} b_{1}#,#overline{b_{2}}#,a_{3}#,#overline{a_{4}} #,d#P_{1}...d#P_{4} $$
$$= #left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}} |#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{3}-#mu_{1}#q_{2}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{3},#q_{2})#cdot #psi_{in}(#q_{3},#q_{4})#cdot #psi_{in}(#q_{1},#q_{4}) #,d#v{q}_{1}#,...#,d#v{q}_{4} $$
$$= #left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{3}-#mu_{1}#q_{2}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{3},#q_{2})#cdot #psi(#q_{1}) #cdot #psi(#q_{3}) #cdot #psi^{2}(#q_{4})#,d#v{q}_{1}...d#v{q}_{4} $$
$$=#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{9}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{3}-#mu_{1}#q_{2}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{3},#q_{2})#cdot #psi_{in}(#q_{1},#q_{3})#,d#v{q}_{1}...d#v{q}_{3}#cdot #int_{#R^{3}}#psi^{2}(#q_{4}) d#q_{4} $$
$$=#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{9}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{3}-#mu_{1}#q_{2}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{3},#q_{2})#cdot #psi_{in}(#q_{1},#q_{3})#,d#v{q}_{1}...d#v{q}_{3} $$
Luego, definimos a $P_{1,1}(#psi_{in})$ de la siguiente forma,
$$ -#left( #frac{#sigma}{#hbar} #right)^{2} P_{1,1}(#psi_{in}):= $$
$$ #int_{#R^{12}} b_{1}#,#overline{b_{2}}#,a_{3}#,#overline{a_{4}} #,d#P_{1}...d#P_{4} + #int_{#R^{12}} a_{1}#, #overline{a_{2}}#, b_{3}#, #overline{b_{4}} #,d#P_{1}...d#P_{4} $$
#begin{equation}#label{p1_1}
=2 #left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{9}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{3}-#mu_{1}#q_{2}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{3},#q_{2})#cdot #psi_{in}(#q_{1},#q_{3})#,d#v{q}_{1}...d#v{q}_{3}
#end{equation}

Desarrollando el lado izquierdo de la ecuaci#'on #eqref{g1b}, tenemos que por las ecuaciones #eqref{psi} y #eqref{psi-est},
$$#int_{#R^{12}} b_{1}#,#overline{a_{2}}#,a_{3}#,#overline{b_{4}} #,d#P_{1}...d#P_{4} $$
$$= #left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}} |#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{4}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{4})#cdot #psi_{in}(#q_{3},#q_{2})#cdot #psi_{in}(#q_{3},#q_{4}) #,d#v{q}_{1}#,...#,d#v{q}_{4} $$
$$= #left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{4}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{4})#cdot #psi(#q_{2}) #cdot #psi(#q_{4}) #cdot #psi^{2}(#q_{3})#,d#v{q}_{1}...d#v{q}_{4} $$
$$=#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{9}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{4}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{4})#cdot #psi_{in}(#q_{2},#q_{4})#,d#v{q}_{1}#,d#v{q}_{2}#,d#v{q}_{4} #cdot #int_{#R^{3}}#psi^{2}(#q_{3}) d#q_{3} $$
$$=#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{9}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{4}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{4})#cdot #psi_{in}(#q_{2},#q_{4})#, d#v{q}_{1}#,d#v{q}_{2}#,d#v{q}_{4} $$
Y definimos a $P_{1,2}(#psi_{in})$ de la siguiente forma,
$$ -#left( #frac{#sigma}{#hbar} #right)^{2} P_{1,2}(#psi_{in}):= $$
$$#int_{#R^{12}} b_{1}#, #overline{a_{2}} #, a_{3} #, #overline{b_{4}} #,d#P_{1}...d#P_{4} + #int_{#R^{12}} a_{1}#, #overline{b_{2}} #, b_{3}#, #overline{a_{4}} #,d#P_{1}...d#P_{4} $$
#begin{equation}#label{p1_2}
=2#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{9}}|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{3}|#cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #Sigma_{1}^{0}#psi_{in}(#q_{1},#q_{3})#cdot #psi_{in}(#q_{2},#q_{3})#, d#v{q}_{1}#,d#v{q}_{2}#,d#v{q}_{3}
#end{equation}

#vspace{3 mm}

Desarrollando el lado izquierdo de la ecuaci#'on #eqref{g1c}, tenemos que por las ecuaciones #eqref{psi} y #eqref{psi-est},
$$#int_{#R^{12}} a_{1}#,#overline{b_{2}}#,a_{3}#,#overline{b_{4}} #,d#P_{1}...d#P_{4} $$
$$= - #left(#frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}} |#mu_{2} #q_{3}-#mu_{1}#q_{2}| #cdot #Sigma_{1}^{0} #psi_{in}(#q_{3},#q_{2}) #cdot #psi_{in}(#q_{3},#q_{2}) #cdot |#mu_{2} #q_{1}-#mu_{1}#q_{4}| #cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{4}) #cdot #psi_{in}(#q_{1},#q_{4})#,d#P_{1}...d#P_{4} $$
$$= - #left(#frac{#sigma}{#hbar} #right)^{2} #left[ #int_{#R^{6}} |#mu_{2} #q_{1}-#mu_{1}#q_{2}| #cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #psi_{in}(#q_{1},#q_{2}) #, d#q_{1}#,d#q_{2}  #right]^{2} $$
Y definimos a $P_{1,3}(#psi_{in})$ de la siguiente forma,
$$- #left(#frac{#sigma}{#hbar} #right)^{2} P_{1,3}(#psi_{in}) :=$$
$$#int_{#R^{12}} a_{1}#, #overline{b_{2}} #, a_{3} #, #overline{b_{4}} #,d#P_{1}...d#P_{4} + #int_{#R^{12}} b_{1}#, #overline{a_{2}} #, b_{3}#, #overline{a_{4}} #,d#P_{1}...d#P_{4} $$
#begin{equation}#label{p1_3}
=-2 #left(#frac{#sigma}{#hbar} #right)^{2} #left[ #int_{#R^{6}} |#mu_{2} #q_{1}-#mu_{1}#q_{2}| #cdot #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2}) #cdot #psi_{in}(#q_{1},#q_{2}) #, d#q_{1}#,d#q_{2}  #right]^{2}
#end{equation}

Ahora, el segundo grupo de t#'erminos de orden $|#sigma/#hbar|^{2}$ del integrando de la pureza es el que est#'a formado por t#'erminos que son el producto de tres coeficientes $a_{i}$ y un coeficiente $c_{i}$. As#'i el segundo grupo es,
$$ c_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}}  #,#,,#,#,  a_{1}#, #overline{c_{2}}#, a_{3}#, #overline{a_{4}}$$
$$ a_{1}#,#overline{a_{2}}#,c_{3}#,#overline{a_{4}}  #,#,,#,#,  a_{1}#, #overline{a_{2}}#, a_{3}#, #overline{c_{4}}$$
De nuevo, ya que en el c#'alculo de la pureza las variables $#q_{1},...,#q_{4}$ son mudas y $c_{i}$ es un coeficiente real, tenemos que:
$$ #int_{#R^{12}} c_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}} #,d#q_{1},...,d#q_{4}=#int_{#R^{12}} a_{1}#, #overline{c_{2}}#, a_{3}#, #overline{a_{4}} #,d#q_{1},...,d#q_{4}$$
#begin{equation}#label{cint}
= #int_{#R^{12}} a_{1}#,#overline{a_{2}}#,c_{3}#,#overline{a_{4}} #,d#q_{1},...,d#q_{4}
=#int_{#R^{12}} a_{1}#,#overline{a_{2}}#,a_{3}#,#overline{c_{4}} #,d#q_{1},...,d#q_{4}
#end{equation}
Desarrollando el extremo izquierdo de #eqref{cint} tenemos que por las ecuaciones #eqref{psi} y #eqref{psi-est},
$$#int_{#R^{12}} c_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}} #,d#q_{1},...,d#q_{4} $$
$$= -#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}}  |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #Sigma_{2}^{0} #psi_{in}(#q_{1},#q_{2})#,#psi_{in}(#q_{3},#q_{2})#,#psi_{in}(#q_{3},#q_{4})#, #psi_{in}(#q_{1},#q_{4})#, d#q_{1},...,d#q_{4} $$
$$= -#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}}  |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #Sigma_{2}^{0} #psi_{in}(#q_{1},#q_{2})#,#psi_{in}(#q_{1},#q_{2})#, #psi^{2}(#q_{3}) #, #psi^{2}(#q_{4})  d#q_{1},...,d#q_{4} $$
$$= -#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}}  |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #Sigma_{2}^{0} #psi_{in}(#q_{1},#q_{2})#,#psi_{in}(#q_{1},#q_{2})  d#q_{1}#, d#q_{2} #cdot #left[#int_{#R^{3}} #psi^{2}(#q) #, d#q #right]^{2} $$
$$= -#left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}}  |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #Sigma_{2}^{0} #psi_{in}(#q_{1},#q_{2})#,#psi_{in}(#q_{1},#q_{2})  d#q_{1}#, d#q_{2} $$
Y definimos a $P_{2}(#psi_{in})$ de la siguiente forma,
$$- #left( #frac{#sigma}{#hbar} #right)^{2} P_{2}(#psi_{in}):= $$
$$ #int_{#R^{12}} c_{1}#,#overline{a_{2}}#,a_{3}#,#overline{a_{4}} #,d#q_{1},...,d#q_{4}+#int_{#R^{12}} a_{1}#, #overline{c_{2}}#, a_{3}#, #overline{a_{4}} #,d#q_{1},...,d#q_{4}$$
$$+ #int_{#R^{12}} a_{1}#,#overline{a_{2}}#,c_{3}#,#overline{a_{4}} #,d#q_{1},...,d#q_{4}
+#int_{#R^{12}} a_{1}#,#overline{a_{2}}#,a_{3}#,#overline{c_{4}} #,d#q_{1},...,d#q_{4}$$
#begin{equation}#label{p2}
= -4 #left( #frac{#sigma}{#hbar} #right)^{2} #int_{#R^{12}} |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2}#, #Sigma_{2}^{0} #psi_{in}(#q_{1},#q_{2})#,#psi_{in}(#q_{1},#q_{2}) d#q_{1}#, d#q_{2}
#end{equation}
Luego, por las ecuaciones #eqref{A1} y #eqref{A1int}, el termino de orden dos $B_{2}|#sigma/#hbar|$ de la pureza, esta dado por la suma de las ecuaciones #eqref{p1_1}, #eqref{p1_2}, #eqref{p1_3} y #eqref{p2},
#begin{equation}#label{B2}
B_{2}|#sigma/#hbar|= - #left( #frac{#sigma}{#hbar} #right)^{2} #left[ P_{1,1}(#psi_{in})+P_{1,2}(#psi_{in})+P_{1,3}(#psi_{in})+P_{2}(#psi_{in}) #right]
#end{equation}
As#'i, por las ecuaciones #eqref{pb}, #eqref{B0},#eqref{B1} y #eqref{B2} tenemos la expansi#'on hasta t#'erminos de orden $(#sigma/#hbar)^{2}$ de la pureza #eqref{Ppsi} en el l#'imite cuando $|#P/#hbar|#rightarrow 0$ y $|#sigma/#hbar|#rightarrow 0$,
#begin{equation}#label{exppur}
#P #left( #left[I + i|#P/#hbar| #Sigma_{1}^{0} - |#P/#hbar|^{2} #Sigma_{2}^{0} #right] #vp_{in} #right) = 1-#left( #frac{#sigma}{#hbar} #right)^{2} #left[ #sum_{k=1}^{3} P_{1,k}(#psi_{in})+P_{2}(#psi_{in}) #right]+O(|#sigma/#hbar|^{3})
#end{equation}

Ahora desarrollemos las integrales de $P_{1,1}(#psi_{in})$, $P_{1,2}(#psi_{in})$ y $P_{1,1}(#psi_{in})$. Primero calculemos expl#'icitamente a $#Sigma_{1}^{0} #psi_{in}$. Ya que $#Sigma_{1}^{0}= -#frac{2#, c_{0}}{#sqrt{4#pi}} #left#la #cdot , #frac{1}{#sqrt{4#pi}} #right#ra $ en donde $#la #cdot , #cdot #ra $ es el producto escalar de $L^{2}(#S^{2})$ respecto a la coordenada relativa $#q$, tenemos haciendo el cambio a coordenadas esf#'ericas $#q #longrightarrow (q,#theta,#phi)$ y por la ecuaci#'on #eqref{estinqcm}:
$$ #Sigma_{1}^{0} #psi_{in} = -#frac{2#, c_{0}}{#sqrt{4#pi}} #left#la #psi_{in} , #frac{1}{#sqrt{4#pi}} #right#ra $$
$$= -#frac{2#, c_{0}}{4#pi^{5/2}} #int_{#S^{2}} #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, #q_{cm}^{2} - q^{2} #right]#, #exp #left[ -(#mu_{1} - #mu_{2}) |#q_{cm}|#, q #, #cos (#theta) #right] #, #sen(#theta)#, d#theta #, d#phi $$

$$ = -#frac{2#, c_{0}}{4#pi^{5/2}}#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, #q_{cm}^{2} - q^{2} #right] #int_{#S^{2}} #exp #left[ -(#mu_{1} - #mu_{2}) |#q_{cm}|#, q #, #cos (#theta) #right] #, #sen(#theta)#, d#theta #, d#phi $$

$$= -#frac{c_{0}}{#pi^{3/2}}#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, #q_{cm}^{2} - q^{2} #right] #int_{0}^{#pi} #exp #left[ -(#mu_{1} - #mu_{2}) |#q_{cm}|#, q #, #cos (#theta) #right] #, #sen(#theta)#, d#theta $$

$$ = -#frac{2#,c_{0}}{#pi^{3/2}}#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, #q_{cm}^{2} - q^{2} #right]#, #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{cm}|#, q #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#, q} $$
Por tanto, en las coordenadas $#q_{1}$, $#q_{2}$ tenemos que,
$$#Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2})= -2#,c_{0} #la #psi_{in},Y_{0} #ra #, Y_{0}= $$
#begin{equation}#label{Spsi}
-#frac{2#,c_{0}}{#pi^{3/2}}#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, |#q_{1}+#q_{2}|^{2} - |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #right]#, #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#end{equation}
Por la ecuaci#'on #eqref{p1_1} tenemos que, $P_{1,1}(#psi_{in})$ se puede escribir como:
#begin{equation}#label{p1_1_2}
P_{1,1}(#psi_{in})= -2 #int_{#R^{3}} #left[ #int_{#R^{3}} |#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2})#cdot #psi (#q_{1})#, d#q_{1} #right]^{2} #, d#q_{2}
#end{equation}
As#'i, por las ecuaciones #eqref{psi}, #eqref{Spsi} y #eqref{p1_1_2} tenemos que:
$$P_{1,1}(#psi_{in})= $$
#begin{align}#label{p11exp}
-#frac{8#,c_{0}^{2}}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}}
|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, |#q_{1}+#q_{2}|^{2} - |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2}-#frac{|#q_{1}|^{2}}{2}#, #right]
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{2}}}
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#,d#q_{1} #right]^{2} d#q_{2}
#end{align}

Por la ecuaci#'on #eqref{p1_2} tenemos que, $P_{1,2}$ se puede escribir como:
$$P_{1,2}(#psi_{in})= -2 #int_{#R^{3}} #left[ #int_{#R^{3}} |#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #Sigma_{1}^{0} #psi_{in}(#q_{1},#q_{2})#cdot #psi (#q_{2})#, d#q_{2} #right]^{2} #, d#q_{1} $$
#begin{equation}#label{p1_2_2}
=-2 #int_{#R^{3}} #left[ #int_{#R^{3}} |#mu_{2}#q_{2}-#mu_{1}#q_{1}|#, #Sigma_{1}^{0} #psi_{in}(#q_{2},#q_{1})#cdot #psi (#q_{1})#, d#q_{1} #right]^{2} #, d#q_{2}
#end{equation}
As#'i, por las ecuaciones #eqref{psi}, #eqref{Spsi} y #eqref{p1_2_2} tenemos que:
$$P_{1,2}(#psi_{in})= $$
#begin{align}#label{p12exp}
-#frac{8#,c_{0}^{2}}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}}
|#mu_{2}#q_{2}-#mu_{1}#q_{1}|#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, |#q_{1}+#q_{2}|^{2} - |#mu_{2}#q_{2}-#mu_{1}#q_{1}|^{2}-#frac{|#q_{1}|^{2}}{2}#, #right]
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{2}}}
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{2}-#mu_{1}#q_{1}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{2}-#mu_{1}#q_{1}| }
#,d#q_{1} #right]^{2} d#q_{2}
#end{align}

As#'i tambi#'en por las ecuaciones #eqref{Spsi} y #eqref{p1_3} tenemos que,
#begin{align}#label{p13exp}
P_{1,3}(#psi_{in})=
-#frac{8#,c_{0}^{2}}{#pi^{6}} #left[ #int_{#R^{6}}
|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #exp #left[ (-#mu_{1}^{2}-#mu_{2}^{2})#, |#q_{1}+#q_{2}|^{2} - 2#,|#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #right]
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{6}}}
#cdot #exp[ -(#mu_{1}-#mu_{2})(#q_{1}+#q_{2})#cdot(#mu_{2}#q_{1}-#mu_{1}#q_{2}) ]
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#,d#q_{1}#, d#q_{2} #right]^{2}
#end{align}

#newpage

Denotando por,
$$J(#mu_{1},#mu_{2})= $$
#begin{align}#label{J}
#frac{1}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}}
|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, |#q_{1}+#q_{2}|^{2} - |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2}-#frac{|#q_{1}|^{2}}{2}#, #right]
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{2}}}
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#,d#q_{1} #right]^{2} d#q_{2}
#end{align}
Y por,
#begin{align}#label{L}
L(#mu_{1},#mu_{2})=
#frac{1}{#pi^{3}} #int_{#R^{6}}
|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #exp #left[ (-#mu_{1}^{2}-#mu_{2}^{2})#, |#q_{1}+#q_{2}|^{2} - 2#,|#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2} #right]
#nonumber##
#qquad #vphantom{#int_{#R^{6}}}
#cdot #exp[ -(#mu_{1}-#mu_{2})(#q_{1}+#q_{2})#cdot(#mu_{2}#q_{1}-#mu_{1}#q_{2}) ]
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#,d#q_{1}#, d#q_{2}
#end{align}
Tenemos por las ecuaciones #eqref{p11exp}, #eqref{p12exp}, #eqref{p13exp},#eqref{J},#eqref{L} y utilizando el hecho de que $#mu_{2}=1-#mu_{1}$,

#begin{equation}#label{p1-1j}
P_{1,1}(#psi_{in})= -8#, c_{0}^{2}#, J(#mu_{1},1-#mu_{1})
#end{equation}

#begin{equation}#label{p1-2j}
P_{1,2}(#psi_{in})= -8#, c_{0}^{2}#, J(1-#mu_{1},#mu_{1})
#end{equation}

#begin{equation}#label{p1-3l}
P_{1,3}(#psi_{in})= 8#, c_{0}^{2}#, [ L(#mu_{1},1-#mu_{1}) ]^{2}
#end{equation}

Ahora desarrollemos la integral de $P_{2}(#psi_{in})$. Para esto primero denotamos por,
#begin{subequations}#label{q-qcm-n}
#begin{align*}
q &:= |#q|##
q_{cm} &:= |#q_{cm}|
#end{align*}
#end{subequations}

Y por,
#begin{equation}#label{I_1}
I_{0}(#q_{cm},q) := #int_{#S^{2}} #psi_{in}(#q_{cm},q#, #w)#cdot Y_{1}(#w) #, sen(#t) #, d#t #, d#phi
#end{equation}
En donde $#w #in #S^{2}$ y $#q= q#, #w$. De la definici#'on de $Y_{1}(#w)$, notamos que esta es una funci#'on impar, es decir $Y_{1}(-#w)=-Y_{1}(#w)$.##
#n Ya que un vector en $#S^{2}$ se puede representar en coordenadas esf#'ericas por medio de sus #'angulos acimutal $#phi$ ($0#leq #phi < 2#pi$) y de colatitud $#t$ ($0#leq #t #leq #pi$) tenemos que, si $#w #in #S^{2}$ est#'a representado por los #'angulos $(#t , #phi)$ en un sistema de coordenadas fijo, entonces el vector $-#w$ va a estar representado por los #'angulos $ (#pi - #t , #phi + #pi) $ en ese mismo sistema de coordenadas. As#'i, $Y_{1}$ puede ser expresado como funci#'on de $(#t,#phi)$, es decir, $ Y_{1}= Y_{1}(#theta, #phi) $ y de esta forma, ya que es una funci#'on impar, tenemos que:
#begin{equation}#label{y1im}
Y_{1}(#pi - #t , #phi + #pi)= Y_{1}(-#w)= -Y_{1}(#w)= -Y_{1}(#t , #phi)
#end{equation}
Sean $#q_{cm} #in #R^{3}$ y $q #geq 0$ fijos. Ahora, sin p#'erdida de generalidad, escojamos un sistema de coordenadas en el que el vector $#q_{cm}$ sea paralelo al eje $z$ y este representado en coordenadas cartesianas por, $#q_{cm}=(0,0,q_{cm})$. De la ecuaci#'on #eqref{estinqcm} observamos que $#psi_{in}$ es una funci#'on que depende solamente de $q$, $q_{cm}$ y del #'angulo entre los vectores $#q#, $ y $#, #q_{cm}$, pero en el sistema de referencia escogido, el #'angulo entre estos dos vectores no es m#'as que el #'angulo de colatitud, $#t$ del vector $q#, #w$, es decir,##
$#psi_{in}=#psi_{in}(q_{cm},q,#t) $ . Por lo anterior y por la ecuaci#'on #eqref{I_1} tenemos que para este sistema de referencia y para este vector $#q_{cm}$:
#begin{equation}#label{I_2}
I_{0}(#q_{cm},q) = #int_{#S^{2}} #psi_{in}(q_{cm},q,#t)#cdot Y_{1}(#t,#phi) #, sen(#t) #, d#t #, d#phi
#end{equation}
Ahora consideremos el caso en el que tenemos $-#q_{cm}$. Es claro que, $-#q_{cm}=(0,0,-q_{cm})$ y que ahora el #'angulo entre $#q$ y $-#q_{cm}$ esta dado por la diferencia $#pi - #t$, en donde $#t$ es el #'angulo de colatitud del vector $q#, #w$ en el sistema de referencia escogido anteriormente. Por lo anterior tenemos que para $-#q_{cm}$:

$$ I_{0}(-#q_{cm},q)= #int_{0}^{2#pi} #int_{0}^{#pi} #psi_{in}(q_{cm},q,#pi-#t)#cdot Y_{1}(#t,#phi) #, sen(#t) #, d#t #, d#phi $$

#n Ahora hagamos el cambio de variable $ #hat{#t}= #pi - #t $ , $ #hat{#phi}= #phi - #pi $ en la integral de la ecuaci#'on anterior, entonces tenemos por las ecuaciones #eqref{y1im} y #eqref{I_2}:
$$ I_{0}(-#q_{cm},q)= - #int_{-#pi}^{#pi} #int_{#pi}^{0} #psi_{in}(q_{cm},q,#hat{#t})#cdot Y_{1}(#pi-#hat{#t},#hat{#phi}+#pi) #, sen(#pi-#hat{#t}) #, d#hat{#t} #, d#hat{#phi}$$
$$ = #int_{-#pi}^{#pi} #int_{0}^{#pi} #psi_{in}(q_{cm},q,#hat{#t})#cdot Y_{1}(#pi-#hat{#t},#hat{#phi}+#pi) #, sen(#hat{#t}) #, d#hat{#t} #, d#hat{#phi}
=- #int_{-#pi}^{#pi} #int_{0}^{#pi} #psi_{in}(q_{cm},q,#hat{#t})#cdot Y_{1}(#hat{#t},#hat{#phi}) #, sen(#hat{#t}) #, d#hat{#t} #, d#hat{#phi}$$
$$=- #int_{#S^{2}} #psi_{in}(q_{cm},q,#hat{#t})#cdot Y_{1}(#hat{#t},#hat{#phi}) #, sen(#hat{#t}) #, d#hat{#t} #, d#hat{#phi} = - I_{0}(#q_{cm},q) $$
Y por tanto,
#begin{equation}#label{I-imp}
I_{0}(-#q_{cm},q)=- I_{0}(#q_{cm},q)
#end{equation}
De la ecuaci#'on #eqref{Spsi} es f#'acil ver que,$#la #psi_{in},Y_{0} #ra$ es una funci#'on que solo depende de $q$ y $q_{cm}$ dada por:

#begin{equation}#label{psi-y0}
#la #psi_{in},Y_{0} #ra =
#frac{2}{#pi}#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, q_{cm}^{2} -q^{2} #right]#, #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, q_{cm} #cdot q #, #right]}{ (#mu_{1}-#mu_{2})#, q_{cm} #cdot q }
#end{equation}

#n Por la definici#'on de $#Sigma_{2}^{0}$ y por la ecuaci#'on #eqref{p2} tenemos que,
$$ P_{2}(#psi_{in})= 4 #int_{#R^{6}} |#mu_{2}#q_{1} -#mu_{1}#q_{2}|^{2}#, #psi_{in}(#q_{1},#q_{2}) #Big[ 2c_{0}^{2}#, #la #psi_{in},Y_{0} #ra #, Y_{0} + #la #psi_{in},Y_{1} #ra #, Y_{0} - #la #psi_{in},Y_{0} #ra #, Y_{1} #Big] #, d#q_{1}#, d#q_{2} $$
#begin{equation}#label{p2_2}
= 8#,c_{0}^{2} #int_{#R^{6}} |#mu_{2}#q_{1} -#mu_{1}#q_{2}|^{2}#, #psi_{in}(#q_{1},#q_{2})#, #la #psi_{in},Y_{0} #ra #, Y_{0} #, d#q_{1}#, d#q_{2}#, + I_{1}(#psi_{in}) + I_{2}(#psi_{in})
#end{equation}
En donde $ #la #cdot , #cdot #ra $ es el producto escalar de $L^{2}(#S^{2})$ respecto a la coordenada relativa $#q$ e introdujimos $I_{1}(#psi_{in})$, $I_{2}(#psi_{in})$ dados por,
$$ I_{1}(#psi_{in})= 4 #int_{#R^{6}} |#mu_{2}#q_{1} -#mu_{1}#q_{2}|^{2}#, #psi_{in}(#q_{1},#q_{2})#, #la #psi_{in},Y_{1} #ra #, Y_{0} #, d#q_{1}#, d#q_{2} $$
$$ I_{2}(#psi_{in})= -4 #int_{#R^{6}} |#mu_{2}#q_{1} -#mu_{1}#q_{2}|^{2}#, #psi_{in}(#q_{1},#q_{2})#, #la #psi_{in},Y_{0} #ra #, Y_{1} #, d#q_{1}#, d#q_{2} $$

#newpage

Desarrollemos a $I_{1}(#psi_{in})$ en las coordenadas $#q$ y $#q_{cm}$. Por las ecuaciones #eqref{I_1} y #eqref{I-imp} tenemos que,
$$I_{1}(#psi_{in})= 4 #int_{#R^{6}} |#q|^{2}#, #psi_{in}(#q,#q_{cm})#, #la #psi_{in},Y_{1} #ra #, Y_{0} #, d#q#, d#q_{cm}$$
$$= 4 #int_{#R^{6}} |#q|^{2}#, #psi_{in}(#q,#q_{cm})#, #left[ #int_{#S^{2}} #psi_{in}(#q_{cm},q#,#w)#cdot Y_{1}(#w) #sen(#t) #, d#t #, d#phi #, #right] #, Y_{0} #, d#q#, d#q_{cm} $$
$$ = 4 #int_{#R^{3}} #left[ #int_{#R^{3}} |#q|^{2}#, #psi_{in}(#q,#q_{cm})#, I_{0}(#q_{cm},q) Y_{0} #, d#q #, #right] #, d#q_{cm}= 0 $$
Por tanto,
#begin{equation}#label{I10}
I_{1}(#psi_{in})= 0
#end{equation}
Ahora desarrollemos a $I_{2}(#psi_{in})$ en las coordenadas $#q$ y $#q_{cm}$ para despu#'es hacer el cambio a coordenadas esf#'ericas $ #q #rightarrow (q,#t,#phi) $ y $ #q_{cm} #rightarrow (q_{cm},#hat{#t},#hat{#phi}) $. Por las ecuaciones #eqref{I_1}, #eqref{I-imp} y #eqref{psi-y0} tenemos que,
$$ I_{2}(#psi_{in})= -4 #int_{#R^{6}} |#q|^{2}#, #psi_{in}(#q,#q_{cm}) #la #psi_{in},Y_{0} #ra #, Y_{1} #, d#q #, d#q_{cm}$$
$$= -4 #int_{#R^{3}} #int_{0}^{#infty} q^{4}#,#la #psi_{in},Y_{0} #ra #left[ #int_{#S^{2}} #psi_{in}(#q_{cm},q#,#w) #cdot Y_{1}(#w) #, #sen(#t) #, d#t #, d#phi #right] #, dq #, d#q_{cm} $$
$$= -4 #int_{0}^{#infty} #int_{#R^{3}} q^{4}#,#la #psi_{in},Y_{0} #ra #, I_{0}(#q_{cm},q) #, d#q_{cm}#, dq $$
$$= -4 #int_{0}^{#infty} q^{4} #int_{0}^{#infty} q_{cm}^{2} #, #la #psi_{in} , Y_{0} #ra #left[ #int_{#S^{2}} I_{0}(q_{cm}#,#w ' ,q) #, #sen(#hat{#t}) #, d#hat{#t}#, d#hat{#phi} #right] #, dq_{cm}#, dq =0 $$
En donde $#w ' #in #S^{2}$ y $#q_{cm}= q_{cm} #, #w '$. Por tanto,
#begin{equation}#label{I20}
I_{2}(#psi_{in})= 0
#end{equation}
As#'i, por las ecuaciones #eqref{psi-y0}, #eqref{p2_2}, #eqref{I10} y #eqref{I20},
$$ P_{2}(#psi_{in}) = 8#,c_{0}^{2} #int_{#R^{6}} |#mu_{2}#q_{1} -#mu_{1}#q_{2}|^{2}#, #psi_{in}(#q_{1},#q_{2})#, #la #psi_{in},Y_{0} #ra #, Y_{0} #, d#q_{1}#, d#q_{2} $$
$$= #frac{8#, c_{0}^{2}}{#pi^{3}} #int_{#R^{6}} #q^{2} #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#q_{cm}^{2} -2#q^{2} - (#mu_{1}-#mu_{2}) #q_{cm} #cdot #q #right] #frac{ #senh #left[ (#mu_{1}-#mu_{2}) |#q_{cm}|#cdot |#q|#right]}{(#mu_{1}-#mu_{2}) |#q_{cm}|#cdot |#q|}#, d#q #, d#q_{cm} $$
Definiendo,
$$ N(#mu_{1},#mu_{2})= $$
#begin{equation}#label{N}
#frac{1}{#pi^{3}} #int_{#R^{6}} #q^{2} #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#q_{cm}^{2} -2#q^{2} - (#mu_{1}-#mu_{2}) #q_{cm} #cdot #q #right] #frac{ #senh #left[ (#mu_{1}-#mu_{2}) |#q_{cm}|#cdot |#q|#right]}{(#mu_{1}-#mu_{2}) |#q_{cm}|#cdot |#q|}#, d#q #, d#q_{cm}
#end{equation}
Y utilizando el hecho de que $#mu_{2} = 1- #mu_{1}$,
#begin{equation}#label{p2-2}
P_{2}(#psi_{in}) = 8#,c_{0}^{2}#, N(#mu_{1},1-#mu_{1})
#end{equation}

#newpage

Luego por las ecuaciones #eqref{1.4.0},#eqref{exppur}, #eqref{J}, #eqref{L},#eqref{p1-1j}-#eqref{p1-3l} y #eqref{p2-2} podemos escribir,
$$ #P (#vp_{out})= 1 - 8#, #frac{c_{0}^{2}#, #sigma^{2}}{#hbar^{2}} #Big[ #,[L(#mu_{1},1-#mu_{1})]^{2} + N(#mu_{1},1-#mu_{1}) - J(#mu_{1},1-#mu_{1}) - J(1-#mu_{1},#mu_{1})#, #Big] $$
#begin{equation}#label{1.110}
+#begin{cases} #begin{matrix} o(|#sigma/#hbar|^{2}) & si#; #beta >5 ## O(|#sigma/#hbar|^{3}) & si#; #beta > 7 #end{matrix} #end{cases}
#end{equation}

M#'as aun, la soluci#'on de las integrales en las ecuaciones #eqref{L} y #eqref{N} est#'a dada por,

#begin{equation}#label{Lexp}
L(#mu_{1},1-#mu_{1})= #sqrt{#frac{2}{#pi}} [ 1+(2#mu_{1}-1)^{2} ]^{-1/2}
#end{equation}
#begin{equation}#label{Nexp}
N(#mu_{1},1-#mu_{1})= #frac{1}{2(2#mu_{1}-1)^{2}} #frac{1}{#sqrt{1+(2#mu_{1}-1)^{2}}}#cdot #left[ [1+(2#mu_{1}-1)^{2}]^{3/2} -1 #right]
#end{equation}
En el ap#'endice mostramos estos resultados a trav#'es de un c#'alculo explicito.##

Denotamos al coeficiente de entrelazamiento como $#E (#mu_{1})$, dado por,
#begin{equation}#label{Eps}
#E(#mu_{1}) = 8 #, #Big[ [L(#mu_{1},1-#mu_{1})]^{2} + N(#mu_{1},1-#mu_{1}) - J(#mu_{1},1-#mu_{1}) - J(1-#mu_{1},#mu_{1}) #Big]
#end{equation}
Con un valor $#E(1/2)= 0.4770 $.##
Luego por las ecuaciones #eqref{Lexp} y #eqref{Nexp},
$$ #E(#mu_{1})= #frac{16}{#pi [1+(2#mu_{1}-1)^{2}]} + #frac{4}{(2#mu_{1}-1)^{2}} #cdot #frac{[1+(2#mu_{1}-1)^{2}]^{3/2} - 1}{#sqrt{1+(2#mu_{1}-1)^{2}}} $$
#begin{equation}#label{eps-2}
- 8#, J(#mu_{1},1-#mu_{1}) - 8 #, J(1-#mu_{1},#mu_{1})
#end{equation}
Por lo anterior, la demostraci#'on del siguiente teorema es inmediata.

#begin{theorem}
Supongamos que la suposici#'on sobre el potencial $V$ se cumple y que en cero $H_{rel}$ no tiene una resonancia o un valor propio. Entonces en el l#'imite cuando $#sigma/#hbar #rightarrow 0 $
#begin{equation}#label{p-eps}
#P(#vp_{out})= 1 - #left(#frac{c_{0} #, #sigma}{#hbar} #right)^{2}#E(#mu_{1}) +#begin{cases} #begin{matrix} o(|#sigma/#hbar|^{2}) & si#; #beta >5 ## O(|#sigma/#hbar|^{3}) & si#; #beta > 7 #end{matrix} #end{cases}
#end{equation}

#end{theorem}
#n {#bf Demostraci#'on}##
La ecuaci#'on #eqref{p-eps} es consecuencia directa de las ecuaciones #eqref{1.110} y #eqref{eps-2}.##
#qed

#vspace{5 mm}

Es en este punto en el que podemos recapitular lo que hemos hecho hasta ahora.## Comenzamos con un estado inicial asint#'otico $#vp_{in,#P_{0}}$ dado por el producto de dos estados Gaussianos normalizados, con la misma varianza $#sigma$ y con momento promedio opuesto (ecs. #eqref{estin} - #eqref{estin2}), de pureza 1 ya que se trata de un estado producto (ec. #eqref{p:estin}). Recordemos que este estado libre inicial es al cual el estado del sistema dispersor tiende asint#'oticamente en la norma de $L^{2}(#R^{6})$ para tiempos en el pasado remoto, esto es, para tiempos en el limite $t #rar -#infty$ la din#'amica de nuestro sistema dispersor puede ser bien aproximada por $ e^{-itH_{0}} #vp_{in,#P_{0}}$.##

Despu#'es, con la matriz de dispersi#'on (o mejor dicho, con la expansi#'on de esta en el limite de bajas energ#'ias que da el teorema de Kato-Jensen) obtuvimos al estado asint#'otico final libre $#vp_{out,#P_{0}}$, al cual el estado del sistema dispersor tiende asint#'oticamente en la norma de $L^{2}(#R^{6})$ para tiempos en el futuro lejano, esto es, para tiempos en el limite $t #rar #infty$, la din#'amica de nuestro sistema dispersor puede ser bien aproximada por $e^{-itH_{0}} #vp_{out,#P_{0}}$. Sin embargo, debemos notar que solo estamos en posici#'on de afirmar que el verdadero estado del sistema dispersor es a lo menos #emph{cercano} (en la norma de $L^{2}(#R^{6})$) al estado $ e^{-itH_{0}} #vp_{out,#P_{0}}$ para tiempos $t>0$. Lo anterior toma en cuenta que el estado del sistema compuesto nunca este dado exactamente por la funci#'on de onda $e^{-itH_{0}} #vp_{out,#P_{0}}$, si no por estados #emph{cercanos} a esta. A pesar de lo anterior en el teorema 1.5 obtuvimos una expresi#'on para la pureza de $ #vp_{out,#P_{0}}$, que no es otra cosa mas que el estado $e^{-itH_{0}} #vp_{out,#P_{0}}$ al tiempo $t=0$. Lo natural ahora seria estudiar la pureza de este ultimo estado para $t>0$.

#vspace{5 mm}

#begin{proposition}
La Pureza de un estado libre de un sistema de dos part#'iculas $#vp (#P_{1},#P_{2})$ ($#P_{i} #in #R^{3}$), es constante en el tiempo.
#end{proposition}
#n {#bf Demostraci#'on}##
Por la ecuaci#'on (12.24), tenemos que la pureza del estado $#vp$ esta dada por:##
$$ P(#vp)= #int_{#R^{12}} #vp(#P_{1},#P_{2})#, #vp(#P_{1}',#P_{2}')#, #ol{#vp (#P_{1}',#P_{2})}#, #ol{#vp (#P_{1},#P_{2}')} #,#,d#P_{1}#,#P_{2}#,#P_{1}'#,#P_{2}' $$

Cuando el estado $#vp$ evoluciona, al tiempo $t#neq 0$, esta dado por $#vp(t)=e^{-itH_{0}} #vp$, en donde como siempre:
$$ e^{-itH_{0}}= #exp #left[-it #left( #frac{#P_{1}^{2}}{2m_{1}}
 + #frac{#P_{2}^{2}}{2m_{2}} #right) #right] =U(t,#P_{1},#P_{2}) $$

Notemos que debido a que nos encontramos en la representaci#'on de momentos y el potencial es cero, el operador de evoluci#'on es simplemente la multiplicaci#'on por un factor de fase complejo.##

Luego, la pureza del estado $#vp$ al tiempo $t #neq 0$ esta dada por:
$$ P(#vp(t))= $$
$$ #int_{#R^{12}} U(t,#P_{1},#P_{2}) #vp(#P_{1},#P_{2})#, U(t,#P_{1}',#P_{2}') #vp(#P_{1}',#P_{2}')#, #ol{U(t,#P'_{1},#P_{2})#vp (#P_{1}',#P_{2})}#, #ol{U(t,#P_{1},#P_{2}')#vp (#P_{1},#P_{2}')} #,#,d#P_{1}#,#P_{2}#,#P_{1}'#,#P_{2}' $$
$$ = #int_{#R^{12}} U(t,#P_{1},#P_{2})#, U(t,#P_{1}',#P_{2}')#, #ol{U(t,#P_{1}',#P_{2})}#, #ol{U(t,#P_{1},#P_{2}')} #,#vp(#P_{1},#P_{2})#, #vp(#P_{1}',#P_{2}')#, #ol{#vp (#P_{1}',#P_{2})}#, #ol{#vp (#P_{1},#P_{2}')} #,#,d#P_{1}#,#P_{2}#,#P_{1}'#,#P_{2}' $$
$$ = #int_{#R^{12}} #vp(#P_{1},#P_{2})#, #vp(#P_{1}',#P_{2}')#, #ol{#vp (#P_{1}',#P_{2})}#, #ol{#vp (#P_{1},#P_{2}')} #,#,d#P_{1}#,#P_{2}#,#P_{1}'#,#P_{2}'=P(#vp) $$

En donde usamos:
$$ U(t,#P_{1},#P_{2})#, U(t,#P_{1}',#P_{2}')#, #ol{U(t,#P_{1}',#P_{2})}#, #ol{U(t,#P_{1},#P_{2}')}$$ 
$$ = #exp #left[-it #left( #frac{#P_{1}^{2}}{2m_{1}}
 + #frac{#P_{2}^{2}}{2m_{2}} #right) -it #left( #frac{(#P_{1}')^{2}}{2m_{1}} +
 #frac{(#P_{2}')^{2}}{2m_{2}} #right) + it #left( #frac{(#P_{1}')^{2}}{2m_{1}}
 + #frac{#P_{2}^{2}}{2m_{2}} #right) + it #left( #frac{#P_{1}^{2}}{2m_{1}} +
 #frac{(#P_{2}')^{2}}{2m_{2}} #right) #right] $$
$$= #exp[0]=1 $$

Por tanto, $P(#vp(t))=P(#vp)$ $#forall t$. #qed

#vspace{5 mm}


Ya que $#vp_{out,#P_{0}}$ es un estado libre de un sistema de dos part#'iculas, por el resultado anterior tenemos que:
$$ P(#vp_{out,#P_{0}})= P #left( e^{-itH_{0}} #vp_{out,#P_{0}} #right) #,#,,#, #forall t$$

Ahora, definimos:
$$
#tilde{#vp}_{in, #P_0}= #mathcal F^{-1} #vp_{in, #P_0},
$$
$$
#tilde{#vp}_{out, #P_0}= #mathcal F^{-1} #vp_{out, #P_0},
$$
$$
#tilde{#psi}= #Omega_-#, #tilde{#vp}_{in, #P_0}.
$$
Al tiempo $t$
$$
#tilde{#psi}(t)= e^{-i tH} #, #tilde{#psi}
$$
En el espacio de momentos,
$$
#psi:= #mathcal F #tilde{#psi}
$$
$$
#psi(t)= #mathcal F #tilde{#psi(t)}= #mathcal F #,e^{-it H} #mathcal F^{-1}#, #psi.
$$
Entonces las funciones
$$ #vp_{out,#P_{0}}^{1}(t)= (#vp_{out,#P_{0}}(t))(#P_{1},#P_{2}) #cdot (#vp_{out,#P_{0}}(t))(#P_{1}',#P_{2}'),
 $$
$$ 
#vp_{out,#P_{0}}^{2}(t)= (#vp_{out,#P_{0}}(t))(#P_{1}',#P_{2}) #cdot (#vp_{out,#P_{0}}(t))(#P_{1},#P_{2}'), 
$$

#begin{equation}#label{psi-1}
#psi^{1}(t)= (#psi(t))(#P_{1},#P_{2}) #cdot (#psi(t))(#P_{1}',#P_{2}')
#end{equation}
y
#begin{equation}#label{psi-2}
#psi^{2}(t)= (#psi(t))(#P_{1}',#P_{2}) #cdot (#psi(t))(#P_{1},#P_{2}')
#end{equation}
est#'an en $L^{2}(#R^{12})$. Recordemos  que $#vp_{out,#P_0}(t)=e^{-itH_{0}} #vp_{out, #P_0}$, with
$ e^{-itH_{0}}= #exp #left[-it #left( #frac{#P_{1}^{2}}{2m_{1}}+
 #frac{#P_{2}^{2}}{2m_{2}} #right) #right]  $.
Como  estamos en la representaci#'on de momentos y el potencial es cero, el operador de evoluci#'on libre es simplemente la multiplicaci#'on por un factor de fase complejo.##


Luego observamos que la pureza del estado $#psi(t)$ se pude ver como el producto interior de las funciones $#psi^{1}(t)$ y $#psi^{2}(t)$ en el espacio $L^{2}(#R^{12})$:
#begin{equation}#label{pur-z}
P(#psi(t))= (#psi^{1}(t),#psi^{2}(t))_{L^{2}(#R^{12})}
#end{equation}
en donde $(#cdot,#cdot )_{L^{2}(#R^{12})}$ es el producto interior del espacio $L^{2}(#R^{12})$.## 
Notamos tambi#'en que $#| #psi^{1} #|_{L^{2}(#R^{12})}=#| #psi^{2} #|_{L^{2}(#R^{12})}$= 1.##

Ahora, comparemos la pureza del estado $#psi(t)$ con la del estado libre asint#'otico $#vp_{out,#P_{0}}^{1}$. Por la proposici#'on anterior, la expresi#'on #eqref{pur-z} y la continuidad del producto interno $(#cdot , #cdot)_{ L^{2}(#R^{12}) }$, tenemos:
$$ #left|P(#vp_{out,#P_{0}}) - P(#psi(t)) #right| = #left| P(#vp_{out,#P_{0}}(t)) - P(#psi(t)) #right| = #left| (#vp_{out,#P_{0}}^{1}(t) , #vp_{out,#P_{0}}^{2}(t)) - (#psi^{1}(t),#psi^{2}(t)) #right| $$
$$ #leq #left#| #vp_{out,#P_{0}}^{1}(t) - #psi^{1}(t) #right#|_{L^{2}(#R^{12})} + #left#| #vp_{out,#P_{0}}^{2}(t) -  #psi^{2}(t) #right#|_{L^{2}(#R^{12})} #rar 0 $$
cuando $t #rar #infty$, ya que $#tilde{#psi}(t)$ y $#tilde{#vp_{out,#P_{0}}}(t)$ cumplen con la condici#'on asint#'otica y la transformada de Fourier es unitaria.
De esta forma, la pureza del estado del sistema dispersor $#psi(t)$ puede ser bien aproximada por $P(#vp_{out,#P_{0}})$ para tiempos en el futuro remoto.##

#newpage

En el ap#'endice calculamos de forma explicita los valores $ J(1/2,1/2)=$0.663497 y $ J(1,0)=$ 0.32627. De esta forma por la ecuaci#'on #eqref{eps-2} cuando las masas de las part#'iculas son iguales, el coeficiente de entrelazamiento esta dado por:
$$ #E(1/2)= 0.477 $$

Para $#mu_{1} #in [0,1] #setminus #{ 1/2,1 #}$, calculamos el valor de $J(#mu_{1},1-#mu_{1})$ num#'ericamente usando cuadraturas gaussianas [18].##

En la siguiente tabla y en la grafica 1, damos los valores de $#E(#mu_{1})$ para 0.5$#leq #mu_{1} #leq 1 $.##

#vspace{3 mm}

%#includegraphics[scale=0.99]{tabla1.PNG}

#newpage

%#includegraphics[scale=0.99]{figura2.PNG}

Comparando los valores de la grafica anterior con la ecuaci#'on #eqref{p-eps} observamos que la pureza del sistema alcanza su m#'aximo cuando las masas de las part#'iculas son iguales ($#mu_{1}=1/2$), por tanto es en este punto cuando el entrelazamiento del sistema es m#'inimo.##

Debemos notar que en la ecuaci#'on #eqref{p-eps} no hay ning#'un termino de orden $#sigma/#hbar$ ya que estos se cancelan debido a los complejos conjugados que aparecen en la formula de la pureza y por el factor $i$ que aparece en la expansi#'on de la matriz de dispersi#'on del teorema de Kato - Jensen, el cual esta ah#'i debido a que esta matriz es un operador unitario. Lo anterior muestra que en el limite de bajas energ#'ias el entrelazamiento es un efecto de segundo orden.##

Notemos tambi#'en que $#E(#mu_{1})=#E(1-#mu_{1})$, lo cual implica que el termino mayor orden de la ecuaci#'on #eqref{p-eps} es invariante ante el cambio $#mu_{1} #leftrightarrow 1-#mu_{1}$ como debe de ser, ya que como vimos en el capitulo 12 de la introducci#'on $P(#vp_{out})$ tiene el mismo valor para las dos part#'iculas del sistema, lo cual tambi#'en se puede interpretar como la invariancia de la pureza ante el intercambio de las part#'iculas.##

Como hemos mencionado, a bajas energ#'ias el entrelazamiento es un efecto de segundo orden. En la expansi#'on de la matriz de dispersi#'on del teorema de Kato - Jensen, se puede observar que para bajas energ#'ias el fen#'omeno de dispersi#'on es isotr#'opico a primer orden y esta determinado por la longitud de dispersi#'on $c_{0}$ ( $#S(#P^{2}/2#mu) #approx I + i |#P/#hbar| #, #Sigma_{1}^{0}$ ). Sin embargo, los efectos anisotropicos del potencial aparecen a segundo orden. Es de sorprenderse que estos efectos no contribuyan en la evaluaci#'on de $N(#mu_{1},#mu_{2})$. Por lo anterior tenemos que el t#'ermino de mayor orden del entrelazamiento para bajas energ#'ias (ec. #eqref{p-eps}) esta determinado por la longitud de dispersi#'on $c_{0}$ y que la anisotrop#'ia del potencial no juega ning#'un papel a pesar de que como hemos mencionado, el entrelazamiento es un efecto de segundo orden.##

#paragraph{Conclusiones:## }
Hemos considerado la creaci#'on de entrelazamiento en un proceso de dispersi#'on a baja energ#'ia, de dos part#'iculas con masas $m_{1}$ y $m_{2}$ respectivamente, en tres dimensiones con la interacci#'on dada por un potencial que no es necesariamente sim#'etrico.## 
Inicialmente las part#'iculas se encuentran en un estado puro, el cual esta dado por el producto de dos estados Gaussianos normalizadas, con la misma varianza $#sigma$ pero con momento promedio opuesto. Medimos el entrelazamiento creado por el proceso de dispersi#'on de las part#'iculas a trav#'es de la pureza $P$ del estado de una de estas despu#'es de la interacci#'on. Antes del proceso de dispersi#'on cuando las part#'iculas se encuentran muy alejadas, la pureza del sistema es uno. Dimos un c#'alculo riguroso, con cota de error, del t#'ermino de mayor orden de la pureza $P$ para bajas energ#'ias; A saber, probamos que el t#'ermino principal de la pureza esta dado por:
$$ 1- #left( #frac{c_{0} #, #sigma}{#hbar} #right)^{2} #E $$
en donde $c_{0}$ es la longitud de dispersi#'on y el coeficiente $#E$ solo depende de la masa de las part#'iculas. ##
As#'i tambi#'en, probamos que el entrelazamiento es m#'inimo cuando las masas son iguales y que este crece r#'apidamente conforme la diferencia de las masas aumenta. Como hicimos notar anteriormente, no hay t#'erminos de orden $#sigma/#hbar$ en el termino principal de la pureza, lo cual muestra que para bajas energ#'ias el entrelazamiento es un fen#'omeno de segundo orden. Aunque los efectos de la anisotrop#'ia del potencial aparecen a segundo orden en el limite de bajas energ#'ias, se encontr#'o que estos efectos no tienen ninguna contribuci#'on en el termino principal de la pureza y por tanto, la anisotrop#'ia del potencial no juega ning#'un papel en este desarrollo, a pesar del hecho de que el entrelazamiento es un efecto de segundo orden.

%section{creacion de entrelazamiento}

#part{Ap#'endice}

#appendix

#chapter{Introducci#'on de $#hbar$ y $m$ en el teorema de Kato - Jensen}

En el teorema de Kato-Jensen, el cual se puede encontrar en el articulo [19], se da una expansi#'on para la matriz de dispersi#'on en el espacio de configuraci#'on de momentos en t#'erminos de operadores de rango finito. Este teorema se desarrolla en unidades en las cuales en Hamiltoniano de la ecuaci#'on de Schr#"odinger tienen la forma:
$$ H=- #Delta + Q $$ 
en donde $Q$ es un potencial que satisface la suposici#'on 1, que se da en el capitulo 17 de la introducci#'on. As#'i, en estas unidades los operadores de onda $#O_{#pm}$ se definen como:
$$ #slim_{t #rar #pm #infty} e^{-it( -#Delta +Q )} #, e^{it#Delta} $$
y ya que estos operadores tienen el mismo significado que en las secciones 14 y 15 de la introducci#'on, el operador de dispersi#'on $#S$, esta dado por:
$$ #S= #O_{+}^{*} #O_{-} $$

En estas unidades la #emph{longitud de dispersi#'on} se define como:
#begin{equation}%#label{scat-l}
c_{0}= #frac{1}{4 #pi} #left( Q ( 1-G_{0} Q)^{-1}#, 1 ,1 #right)
#end{equation}
en donde $G_{0}$ es el operador integra cuyo n#'ucleo es la funci#'on de Green a energ#'ia cero del Hamiltoniano. De esta forma las funciones $Y_{0}(w)$ y $Y_{1}(w)$ quedan definidas como:
$$ Y_{0}(w)= #frac{1}{#sqrt{4 #pi}} $$
y 
#begin{equation}#label{Y1}
Y_{1}(w)= #frac{1}{4#pi^{3/2}} #left( Q ( 1+G_{0}Q )^{-1}#, 1, x #cdot w  #right)
#end{equation}
Para $w #in S^{2}$ y $x#in #R^{3}$.##
De esta forma, el teorema de Kato - Jensen da la siguiente expansi#'on de la matriz de dispersi#'on, para $#beta >5$:
#begin{equation}#label{S-lamb}
S(#lambda) = I + i #lambda^{1/2}#Sigma_{1}^{0} - #lambda #Sigma_{2}^{0} + o(#lambda)
#end{equation}
si $#beta >7$ se puede remplazar $o(#lambda)$ por $O(#lambda^{3/2})$.##

En este caso, la transformada de Fourier y su inversa est#'an dadas en $L^{2}(#R^{3})$ por:
$$ #F #varphi= #frac{1}{(2#pi)^{3/2}} #int_{#R^{3}} e^{-i #q #cdot #x}#, #varphi(#x) #, d#x $$
$$ #F^{-1} #hat{#varphi}= #frac{1}{(2#pi)^{3/2}} #int_{#R^{3}} e^{i #q #cdot #x}#, #hat{#varphi}(#q) #, d#q $$
As#'i, el operador de dispersi#'on en el espacio de momentos respecto a la transformada de Fourier anterior esta dado por:
#begin{equation}#label{S-hat}
#hat{#S}=#F #, #S #, #F^{-1}
#end{equation}

Luego, sabemos que $#S$ se puede descomponer en t#'erminos de operadores sobre el espacio de Hilbert $L^{2}(S^{2})$ de donde obtenemos la matriz de dispersi#'on:
$$ #hat{#S}=#{ #S(#lambda) #} $$

En donde $#S(#lambda): L^{2}(S^{2}) #rar L^{2}(S^{2})$, para cada $#lambda #in [0,#infty)$. De esta manera podemos representar a $#hat{#S}$ de forma integral como:
#begin{equation}#label{Shat-phi}
#hat{#S}#varphi(#q)= #int_{S^{2}} #rho(#lambda,w,w' )#, #varphi(#lambda^{1/2} w') #, dw'
#end{equation}
con $w #in S^{2}$, $ #lambda =|#q|^{2} $ y $ #q= #lambda^{1/2}w $.##

El desarrollo anterior se puede entender como el caso en el que toma $#hbar=1$ y $m=1/2$.##

Por otro lado, tomemos al Hamiltoniano de la ecuaci#'on de Schr#"odinger en las unidades f#'isicas convencionales:
$$ H= -#frac{#hbar^{2}}{2m} #Delta + V(#x) $$
en donde $V(#x)$ es un potencial que satisface la condici#'on 1 que se da en el capitulo 17 de la introducci#'on. Luego, este Hamiltoniano se puede escribir como:
$$ H= #frac{#hbar^{2}}{2m}(-#Delta +Q(#x)) $$
en donde $Q:= #frac{2m}{#hbar^{2}}#, V(#x)$, y ya que solo tiene un factor constante extra esta funci#'on sigue cumpliendo la condici#'on sobre el potencial. El Hamiltoniano libre (o no perturbado) esta dado por:
$$ H_{0}= -#frac{#hbar^{2}}{2m} #Delta $$
De esta forma, definimos a los operadores de onda $W_{#pm}$ como:
$$ W_{#pm}= #slim_{t #rar #infty} e^{i #frac{t}{#hbar}H} #, e^{-i #frac{t}{#hbar}H_{0} } $$
Sin embargo observamos que:
$$ W_{#pm}= #slim_{t #rar #pm#infty} #exp #left( i#frac{t}{#hbar}#,#frac{#hbar^{2}}{2m}(-#Delta +Q) #right) #, #exp #left( i #frac{t}{#hbar}#,#frac{#hbar^{2}}{2m} #Delta #right)   $$
$$ =#slim_{t #rar #pm#infty} #exp #left( it#,#frac{#hbar}{2m}(-#Delta +Q) #right) #, #exp #left( it#,#frac{#hbar}{2m} #Delta #right) $$
$$ =#slim_{t #rar #pm#infty} #exp #left( it#,(-#Delta +Q) #right) #, #exp #left( it#,#Delta #right) = #O_{#pm} $$
Luego observamos que el operador de dispersi#'on esta dado por:
#begin{equation}#label{S-equiv}
#S= W_{+}^{*}W_{-} = #O_{+}^{*}#O_{-}
#end{equation}

Ahora, denotaremos a la transformada de Fourier y a su inversa en estas unidades como:
$$ #F_{#hbar}(#varphi)  = #frac{1}{(2 #pi #hbar)^{3/2}} #int_{#R^{3}} e^{-i#frac{#P}{#hbar}#cdot #x} #, #varphi(#x)#, d#x $$
$$ #F_{#hbar}^{-1}(#hat{#varphi})  = #frac{1}{(2 #pi #hbar)^{3/2}} #int_{#R^{3}} e^{i#frac{#P}{#hbar}#cdot #x} #, #hat{#varphi}(#P)#, d#P $$
a $#F_{#hbar}$ se le conoce tambi#'en como la transformada de Fourier f#'isica en $L^{2}(#R^{3})$ ya que esta toma en cuenta a la constante $#hbar$.##
Para $#varphi #in L^{2}(#R^{3})$, denotamos a su transformada de Fourier f#'isica como:
$$ #varphi_{M}(#P)= ( #F_{#hbar} #varphi )(#P) $$

Ahora definimos a $E= #frac{|#P|^{2}}{2m}$ y hacemos el cambio de coordenadas $#P #rar (E,w)$ en donde $w #in S^{2}$. Este cambio lo hacemos a trav#'es del operador unitario $U: L^{2}(#R^{3}) #rar L^{2}( [0,#infty),L^{2}(S^{2}) ) $. As#'i definimos:
$$ #varphi_{#xi}(E,w) =(U #varphi_{M} ) (E,w) $$
en donde $U$ act#'ua de la forma:
$$ (U #varphi_{M}) (E,w) = m^{1/2} (2mE)^{1/4}#, #varphi_{M} #left( #sqrt{2mE}#,w #right) $$
Para probar que $U$ es efectivamente un operador unitario basta ver que $U^{*}=U^{-1}$. Para hacer esto primero debemos de encontrar como act#'ua $U^{*}$. Tomemos a $#varphi,#psi #in L^{2}(#R^{3})$, luego:
$$ ( U^{*}#varphi,#psi ) = ( #varphi,U#psi )=#int_{0}^{#infty} #int_{S^{2}} #varphi(E,w)#, m^{1/2} (2mE)^{1/4} #, #ol{#psi}#left( #sqrt{2mE}#,w #right) #, dw#, dE $$
$$ = #int_{0}^{#infty} #int_{S^{2}} #varphi #left(#frac{|#P|^{2}}{2m},w #right) #, #ol{#psi} (|#P| w) #, m^{1/2}#,|#P|^{1/2} #, #frac{|#P|}{m} #, dw #, d|#P| $$
$$ = #int_{0}^{#infty} #int_{S^{2}} #left[ #frac{1}{m^{1/2}} #, #frac{1}{|#P|^{1/2}}#, #varphi #left(#frac{|#P|^{2}}{2m},w #right) #right] #, #ol{#psi}(|#P| w)#, |#P|^{2} #, dw #,d|#P|    $$
lo anterior implica que:
$$ U^{*} #varphi(#P)= #frac{1}{m^{1/2}} #, #frac{1}{|#P|^{1/2}}#, #varphi #left(#frac{|#P|^{2}}{2m},w #right) $$
Luego,
$$ U^{*}U #varphi= U^{*} #, m^{1/2}#, (2mE)^{1/4} #, #varphi ( #sqrt{2mE}#,w ) = #frac{ m^{1/2}#, |#P|^{1/2} }{m^{1/2}#, |#P|^{1/2}} #varphi( #sqrt{2mE}#,w ) = #varphi( #P ) $$
por tanto $U$ es unitario.##
En las coordenadas $(E,w)$ la matriz de dispersi#'on esta dada por:
$$ ( U#,F_{#hbar}#,#S F_{#hbar}^{-1} U^{-1} ) (#varphi_{#xi})(E,w)= ( U#, S_{M} #,U^{-1} ) (#varphi_{#xi})(E,w) =#S_{#xi}(E) #, #varphi_{#xi}(E,w)  $$
en donde introdujimos a $ S_{M}= F_{#hbar}#,#S F_{#hbar}^{-1}$. Es claro que, $#S_{#xi}(E): L^{2}(S^{2}) #rar L^{2}(S^{2}) $, para toda $E #in [0,#infty)$. Luego, $#S_{#xi}(E)$, se puede representar de forma integral como:
$$ #S_{#xi}(E) #, #varphi_{#xi}(E,w)= #int_{S^{2}} #S(E,w,w')#, #varphi_{#xi}(E,w')#, dw' $$
Ahora en la representaci#'on de momentos, calculemos $#S_{M}#varphi_{M}$:
$$ #S_{M} #varphi_{M}= U^{-1} (#S_{#xi}) U #varphi_{M} = U^{-1} #S_{#xi}#, m^{1/2} #, (2mE)^{1/4} #, #varphi_{M}( #sqrt{2mE}w ) $$
$$= U^{-1} #, #int_{S^{2}} #S(E,w,w')#, m^{1/2} #, (2mE)^{1/4} #, #varphi_{M}( #sqrt{2mE}w' )#, dw' $$
$$ = #int_{S^{2}} #S#left( #frac{|#P|^{2}}{2m},w,w' #right)#, #varphi_{M}(|#P|w')#, dw' $$
de esta forma tenemos que:
#begin{equation}#label{Sm-pm}
#S_{M}#varphi_{M}(#P)= #int_{S^{2}} #S#left( #frac{|#P|^{2}}{2m},w,w' #right)#, #varphi_{M}(|#P|w')#, dw'
#end{equation}

Ahora, comparemos lo anterior con el operador de dispersi#'on del teorema de Kato - Jensen, con n#'ucleo integral $#rho( #lambda,w,w' )$, en el caso $#hbar=1$ y $m=1/2$.##

La ecuaci#'on #eqref{S-hat}, implica que $ #S= #F^{-1} #, #hat{#S} #, #F $. Tomando esto, el hecho de que $#varphi= #F_{#hbar}^{-1} #varphi_{M} $ y la ecuaci#'on #eqref{Shat-phi}, tenemos que:
$$ #S_{M}#varphi_{M}= #F_{#hbar}#, #S #, #F_{#hbar}^{-1}#, #F_{#hbar} #varphi = #F_{#hbar}#, #S #, #varphi =#F_{#hbar} #, #F^{-1}( #hat{#S} #, #F #, #varphi )= #F_{#hbar} #, #F^{-1} #,#hat{#S} #,#hat{#varphi} $$
$$= #F_{#hbar} #, #F^{-1} #, #int_{S^{2}} #rho(|#P|^{2},w,w') #, #hat{#varphi}(|#P|w')#, dw' $$

Lo anterior e puede hacer sin ninguna ambiguedad por la equivalencia que se expresa en #eqref{S-equiv}. Calculemos ahora $#F_{#hbar} #, #F^{-1} #hat{#psi}$, para $#hat{#psi} #in L^{2}(#R^{3})$:
$$ #F_{#hbar} #, #F^{-1} #hat{#psi}= #frac{1}{(2 #pi #hbar)^{3/2}} #int_{#R^{3}} e^{-i#frac{#P}{#hbar}#cdot #x } #frac{1}{(2 #pi)^{3/2}} #int_{#R^{3}} e^{i #q #cdot #x} #, #hat{#psi}(q) #, d#q #, d#x = #frac{1}{#hbar^{3/2}} #hat{#psi} #left( #frac{#P}{#hbar} #right) $$
de esta forma, tomando el hecho de que $#hat{#varphi}#left( #frac{|#P|}{#hbar}w' #right)= #varphi_{M}(|#P|w') $, tenemos que:
$$ #S_{M}#varphi_{M}= #int_{S^{2}} #rho#left(#frac{|#P|^{2}}{#hbar^{2}},w,w' #right) #, #varphi_{M}( |#P|w' )#, dw' $$
luego, por la ecuaci#'on #eqref{Sm-pm}, tenemos que:
$$ #rho#left(#frac{|#P|^{2}}{#hbar^{2}},w,w' #right)= #S #left(#frac{|#P|^{2}}{2m},w,w'  #right)= #rho#left(#lambda,w,w' #right) $$

#newpage

De esta forma, sustituyendo $#lambda$ por $#frac{|#P|^{2}}{#hbar^{2}}$ y $Q$ por $#frac{2m}{#hbar^{2}} #, V $, en las ecuaciones #eqref{scat-l}, #eqref{Y1} y #eqref{S-lamb}, obtenemos:
$$ c_{0}=#frac{-1}{4#pi} #left( #frac{2m}{#hbar^{2}}V #left(1 - #frac{2m}{#hbar^{2}}G_{0} V #right)^{-1}#, 1,1  #right) $$
$$ Y_{1}(w)= #frac{-1}{(4#pi)^{3/2}} #left( #frac{2m}{#hbar^{2}}V #left( 1+#frac{2m}{#hbar} G_{0}V #right)^{-1}#, 1, #x #cdot w  #right) $$
$$ #S #left( #left| #frac{#P}{#hbar} #right|^{2} #right)=I + i #left| #frac{#P}{#hbar} #right| #Sigma_{1}^{0} - #left| #frac{#P}{#hbar} #right|^{2} #Sigma_{2}^{0} + o(|#P/#hbar|^{2}) #,#,,#,#hbox{para } #beta>5 $$
$$ #S #left( #left| #frac{#P}{#hbar} #right|^{2} #right)=I + i #left| #frac{#P}{#hbar} #right| #Sigma_{1}^{0} - #left| #frac{#P}{#hbar} #right|^{2} #Sigma_{2}^{0} + O(|#P/#hbar|^{3}) #,#,,#,#hbox{para } #beta>7 $$

%*********************************************************************

#chapter{Integrales elementales}
En el resto de este ap#'endice estableceremos, por medio de c#'alculo explicito, algunos resultados que se utilizan en el capitulo de creaci#'on de entrelazamiento, a saber calcularemos las integrales $L(#mu_{1},1-#mu)$, $N(#mu_{1},1-#mu)$. De esta misma forma obtendremos los valores $J#left( #frac{1}{2} , #frac{1}{2} #right)$ y $J(1,0)$.##
Para esto, establecemos primero las siguientes integrales elementales:
#begin{equation}#label{A1}
#int_{0}^{#infty} e^{-ax^{2}} dx =#frac{1}{2}#sqrt{#frac{#pi}{a}} #,#,#,,#,#, a>0
#end{equation}

#begin{equation}#label{A2}
#int_{0}^{#infty} e^{-ax^{2}} x^{2} dx= #frac{1}{4a}#sqrt{#frac{#pi}{a}} #,#,#,,#,#, a>0
#end{equation}
La siguiente integral se sigue directamente de #eqref{A2} integrando por partes usando el hecho de que,##
$ x e^{-ax^{2}}= #frac{d}{dx} #left[ #frac{-1}{2a}#, e^{-ax^{2}} #right] $,
#begin{equation}#label{A2.1}
#int_{0}^{#infty} e^{-ax^{2}} x^{4} dx= #frac{3}{8}#, #frac{#sqrt{#pi}}{a^{5/2}} #,#,#,,#,#, a>0
#end{equation}

#begin{equation}#label{A3}
#int_{-#infty}^{#infty} e^{-ax^{2}-2bx} dx= #sqrt{#frac{#pi}{a}} #, e^{b^{2}/a} #,#,#,,#,#, a>0
#end{equation}

#begin{equation}#label{A4}
#int_{0}^{#infty} e^{-ax^{2}-2bx} dx= #frac{1}{2}#sqrt{#frac{#pi}{a}} #, e^{b^{2}/a} #left[ 1- #erf ( b/#sqrt{a}#, ) #right] #,#,#,,#,#, a>0
#end{equation}

#begin{equation}#label{A4.1}
#int_{0}^{#infty} x#, e^{-ax^{2}} dx= #frac{1}{2a}
#end{equation}

#begin{equation}#label{A8}
#int_{0}^{#pi} e^{-a #, #cos(#t)} #sen(#t) #, d#t= 2#,#frac{#senh(a)}{a} #,#,#,,#,#, a>0
#end{equation}
En donde, $#erf (x)$ es la funci#'on error:
#begin{equation}#label{erf}
#erf(x)= #frac{2}{#sqrt{#pi}} #int_{0}^{x} e^{-y^{2}} dy
#end{equation}
Con la propiedad,
#begin{equation}#label{erf-2}
#erf^{2}(x)= 1- #frac{4}{#pi} #int_{0}^{1} #frac{e^{-x^{2}(y^{2}+1)}}{y^{2}+1}#, dy
#end{equation}

#newpage

As#'i tambi#'en introducimos las siguientes integrales indefinidas:
#begin{equation}#label{A5}
#int #frac{1}{(1+x^{2})#, #sqrt{2+x^{2}} } #, dx= #arctan #left( #frac{1}{x} #right) + #arctan #left( #frac{x}{2-#sqrt{2+x^{2}}} #right) + C
#end{equation}

#begin{equation}#label{A6}
#int #frac{1}{(1+x^{2})#, (2+x^{2})^{3/2} } #, dx= #arctan #left( #frac{x}{#sqrt{2+x^{2}}} #right) - #frac{x}{2 #sqrt{2+x^{2}}} + C
#end{equation}

#begin{equation}#label{A7}
#int #frac{1}{(1+x^{2})#, (2+x^{2})^{5/2} } #, dx= #arctan #left( #frac{x}{#sqrt{2+x^{2}}} #right) - #frac{x ( 4x^{2}+9 )}{6(2+y^{2})^{3/2}}+ C
#end{equation}

%*****************************************************************

#chapter{C#'alculo de $L(#mu_{1},1-#mu_{1})$}
Primero, en las coordenadas $#q$ - $#q_{cm}$, $L(#mu_{1},#mu_{2})$ est#'a dada por: 
#begin{align}#label{L}
L(#mu_{1},#mu_{2})= 
#frac{1}{#pi^{3}}  #int_{#R^{3}} #int_{#R^{3}}
|#q|#, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2#,|#q|^{2}
 #right] 
#nonumber##
#qquad  #vphantom{#int_{#R^{6}}}
#cdot #exp[ -(#mu_{1}-#mu_{2})#q_{cm}#cdot #q ] 
#cdot #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| }
#,d#q #, d#q_{cm} 
#end{align}
y denotamos por,
#begin{align}#label{I1}
I_{1}(#q_{cm},#mu_{1},#mu_{2}):= 
#int_{#R^{3}}
|#q|#, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2#,|#q|^{2}
 #right] 
#nonumber##
#qquad  #vphantom{#int_{#R^{6}}}
#cdot #exp[ -(#mu_{1}-#mu_{2})#q_{cm}#cdot #q ] 
#cdot #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| }
#,d#q 
#end{align}
Es claro que, 
#begin{equation}#label{LI}
L(#mu_{1},#mu_{2})= #frac{1}{#pi^{3}} #int_{#R^{3}} I_{1}(#q_{cm},#mu_{1},#mu_{2}) #, d#q_{cm} 
#end{equation}
Haciendo el cambio a coordenadas esf#'ericas $#q #rightarrow (#l, #t,#p) $ en un sistema de referencia en el que el eje $z$ es paralelo al vector $#q_{cm}$, tenemos que por la ecuaci#'on #eqref{A8},
#begin{align}
I_{1}(#q_{cm},#mu_{1},#mu_{2})= 
#int_{0}^{#infty} #int_{0}^{2#pi} #int_{0}^{#pi}
#l #, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2#, #l^{2}-(#mu_{1}-#mu_{2}) |#q_{cm}|#l #cos(#theta)
 #right] 
#nonumber##
#qquad  #vphantom{#int_{#R^{3}}}
#cdot #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l }
#, #l^{2} #sen(#t) #, d#t #, d#p #,d #l  #nonumber
#end{align}
$$= 2#pi #int_{0}^{#infty} #l #, #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l }#, e^{-(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2 #l^{2}} #left[#int_{0}^{#pi} e^{-(#mu_{1}-#mu_{2}) |#q_{cm}|#l #cos(#theta)} #sen (#t) #, d#t #right] #l^{2}#, d#l $$
#begin{equation}#label{intI1}
= 4#pi #int_{0}^{#infty} #l #, #left[ #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|} #right]^{2} #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2 #l^{2} #right] d#l
#end{equation}
Sustituyendo #eqref{intI1} en la ecuaci#'on #eqref{LI} desarrollando el termino cuadr#'atico obtenemos,
$$L(#mu_{1},#mu_{2})= #frac{4}{#pi^{2}} #int_{#R^{3}} #int_{0}^{#infty} #frac{#l #, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#q_{cm}^{2} -2#l^{2} #right] }{(#mu_{1}-#mu_{2})^{2} |#q_{cm}|^{2}} #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#,|#q_{cm}|#l #,] }{2} - #frac{1}{2} #right]  #,d#l #, d#q_{cm}  $$

#newpage

Haciendo el cambio a coordenadas esf#'ericas $#q_{cm} #rightarrow (#r,#hat{#t},#hat{#p}) $ e integrando la parte angular, obtenemos:
$$L(#mu_{1},#mu_{2})= #frac{16}{#pi} #int_{0}^{#infty} #int_{0}^{#infty} #frac{#l #, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#q_{cm}^{2} -2#l^{2} #right] }{(#mu_{1}-#mu_{2})^{2} |#q_{cm}|^{2}} #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#, #r #, #l #,] }{2} - #frac{1}{2} #right] #, d#r #,d#l   $$
$$=#frac{16}{#pi (#mu_{1}-#mu_{2})^{2}} #int_{0}^{#infty} #l #, e^{-2#l^{2}} #int_{0}^{#infty} e^{-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2} } #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#, #r #, #l #,] }{2} - #frac{1}{2} #right] #, d#r #, d#l $$
#begin{equation}#label{LI-2}
= #frac{16}{#pi (#mu_{1}-#mu_{2})^{2}} #int_{0}^{#infty} #l #, e^{-2#l^{2}} I_{2}(#l ,#mu_{1},#mu_{2})   #, d#l
#end{equation}
en donde,
$$ I_{2}(#l ,#mu_{1},#mu_{2}) := #int_{0}^{#infty} e^{-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2} } #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#, #r #, #l #,] }{2} - #frac{1}{2} #right] #, d#r $$
Desarrollando a $I_{2}(#l ,#mu_{1},#mu_{2})$ en t#'erminos de exponenciales, obtenemos:
$$ I_{2}(#l ,#mu_{1},#mu_{2})= #frac{1}{4}  #int_{0}^{#infty}#exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}+ 2(#mu_{1}-#mu_{2})#, #r #, #l  ]#,d#r $$
#begin{equation}#label{I2exp}
+#,  #frac{1}{4} #int_{0}^{#infty}#exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}- 2(#mu_{1}-#mu_{2})#, #r #, #l  ]#,d#r #, - #frac{1}{2} #int_{0}^{#infty} #exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}] #, d#r
#end{equation}
Luego observamos que,
$$ #frac{1}{4}  #int_{0}^{#infty}#exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}+ 2(#mu_{1}-#mu_{2})#, #r #, #l  ]#,d#r= 
#frac{1}{4}  #int_{-#infty}^{0}#exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}- 2(#mu_{1}-#mu_{2})#, #r #, #l  ]#,d#r$$ 
$$#frac{1}{2} #int_{0}^{#infty} #exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}] #, d#r= #frac{1}{4} #int_{-#infty}^{#infty} #exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}] #, d#r $$
Sustituyendo en #eqref{I2exp} obtenemos,
$$I_{2}(#l ,#mu_{1},#mu_{2})=#frac{1}{4} #int_{-#infty}^{#infty} #exp[-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2}- 2(#mu_{1}-#mu_{2})#, #r #, #l  ] - #exp [ -(#mu_{1}^{2}+#mu_{2}^{2})#r^{2} ]#, d#r $$  
de esta forma, por las ecuaciones #eqref{A1} y #eqref{A3} obtenemos que,
#begin{equation}#label{I2-res}
I_{2}(#l ,#mu_{1},#mu_{2})=#frac{1}{4} #sqrt{ #frac{#pi}{#mu_{1}^{2}+#mu_{2}^{2}}} #left[ #exp#left[ #frac{(#mu_{1}-#mu_{2})^{2} #l^{2}}{#mu_{1}^{2}+#mu_{2}^{2}} #right] - 1 #right] 
#end{equation}
Sustituyendo #eqref{I2-res} en la ecuaci#'on #eqref{LI-2} obtenemos,
$$ L(#mu_{1},#mu_{2})=#frac{4}{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }} #int_{0}^{#infty} #l e^{-2#l^{2}} #left[ #exp#left[ #frac{(#mu_{1}-#mu_{2})^{2} #l^{2}}{#mu_{1}^{2}+#mu_{2}^{2}} #right] - 1 #right] #, d#l $$
y por la ecuaci#'on #eqref{A4.1} tenemos que
$$ L(#mu_{1},#mu_{2})= #frac{4}{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }} #left[#frac{1}{2} #left(2-#frac{(#mu_{1}-#mu_{2})^{2}}{#mu_{1}^{2}+#mu_{2}^{2}} #right)^{-1} -#frac{1}{4} #right] $$
desarrollando t#'erminos cuadr#'aticos y utilizando el hecho de que $#mu_{1}+#mu_{2}=1$ es f#'acil ver que,
$$ #frac{1}{2} #left(2-#frac{(#mu_{1}-#mu_{2})^{2}}{#mu_{1}^{2}+#mu_{2}^{2}} #right)^{-1}= #frac{ #mu_{1}^{2}+#mu_{2}^{2} }{2} $$
As#'i obtenemos,
$$ L(#mu_{1},#mu_{2})= #frac{ 2#mu_{1}^{2}+2#mu_{2}^{2}-1 }{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }} $$
M#'as aun, usando el hecho de que $#mu_{2}=1-#mu_{1}$ y desarrollando los t#'erminos cuadr#'aticos tenemos que,
$$2#mu_{1}^{2}+2#mu_{2}^{2}-1= (#mu_{1}-#mu_{2})^{2}= 4#mu_{1}^{2} -4 #mu_{1}+1 $$
y que,
$$ #mu_{1}^{2}+#mu_{2}^{2}= 2#mu_{1}^{2} -2 #mu_{1}+1= #left(#sqrt{2}#, #mu_{1} - #frac{1}{#sqrt{2}} #right)^{2} +#frac{1}{2} = #frac{1}{2} #left[ #left(2#, #mu_{1} - 1 #right)^{2} + 1 #right] $$
y de esta forma obtenemos,
$$ L(#mu_{1},1-#mu_{1})= #sqrt{#frac{2}{#pi}}  #left[ (2#, #mu_{1} - 1)^{2} +1 #right]^{-1/2} $$

%*********************************************************************

#chapter{C#'alculo de $N(#mu_{1},1-#mu_{1})$}
Primero, en las coordenadas $#q$ - $#q_{cm}$, $N(#mu_{1},#mu_{2})$ est#'a dada por: 
#begin{align}#label{L}
N(#mu_{1},#mu_{2})= 
#frac{1}{#pi^{3}}  #int_{#R^{3}} #int_{#R^{3}}
|#q|^{2}#, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2#,|#q|^{2}
 #right] 
#nonumber##
#qquad  #vphantom{#int_{#R^{6}}}
#cdot #exp[ -(#mu_{1}-#mu_{2})#q_{cm}#cdot #q ] 
#cdot #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| }
#,d#q #, d#q_{cm} 
#end{align}

y denotamos por,
#begin{align}#label{In1}
I_{1}(#q_{cm},#mu_{1},#mu_{2}):= 
#int_{#R^{3}}
|#q|^{2}#, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2#,|#q|^{2}
 #right] 
#nonumber##
#qquad  #vphantom{#int_{#R^{6}}}
#cdot #exp[ -(#mu_{1}-#mu_{2})#q_{cm}#cdot #q ] 
#cdot #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot |#q| }
#,d#q 
#end{align}
Es claro que, 
#begin{equation}#label{LnI}
N(#mu_{1},#mu_{2})= #frac{1}{#pi^{3}} #int_{#R^{3}} I_{1}(#q_{cm},#mu_{1},#mu_{2}) #, d#q_{cm} 
#end{equation}
Haciendo el cambio a coordenadas esf#'ericas $#q #rightarrow (#l, #t,#p) $ en un sistema de referencia en el que el eje $z$ es paralelo al vector $#q_{cm}$, tenemos que por la ecuaci#'on #eqref{A8},
#begin{align}
I_{1}(#q_{cm},#mu_{1},#mu_{2})= 
#int_{0}^{#infty} #int_{0}^{2#pi} #int_{0}^{#pi}
#l^{2} #, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2#, #l^{2}-(#mu_{1}-#mu_{2}) |#q_{cm}|#l #cos(#theta)
 #right] 
#nonumber##
#qquad  #vphantom{#int_{#R^{3}}}
#cdot #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l }
#, #l^{2} #sen(#t) #, d#t #, d#p #,d #l  #nonumber
#end{align}
$$= 2#pi #int_{0}^{#infty} #l^{2}#, #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l }#, e^{-(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2 #l^{2}} #left[#int_{0}^{#pi} e^{-(#mu_{1}-#mu_{2}) |#q_{cm}|#l #cos(#theta)} #sen (#t) #, d#t #right] #l^{2}#, d#l $$
#begin{equation}#label{intI1n}
= 4#pi #int_{0}^{#infty} #l^{2}  #left[ #frac{ #senh #left[#, (#mu_{1}-#mu_{2})#, |#q_{cm}|#cdot #l #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{cm}|} #right]^{2} #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#, |#q_{cm}|^{2} - 2 #l^{2} #right] d#l
#end{equation}
Sustituyendo #eqref{intI1n} en la ecuaci#'on #eqref{LnI} desarrollando el termino cuadr#'atico obtenemos,
$$N(#mu_{1},#mu_{2})= #frac{4}{#pi^{2}} #int_{#R^{3}} #int_{0}^{#infty} #frac{#l^{2} #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#q_{cm}^{2} -2#l^{2} #right] }{(#mu_{1}-#mu_{2})^{2} |#q_{cm}|^{2}} #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#,|#q_{cm}|#l #,] }{2} - #frac{1}{2} #right]  #,d#l #, d#q_{cm}  $$

#newpage

Haciendo el cambio a coordenadas esf#'ericas $#q_{cm} #rightarrow (#r,#hat{#t},#hat{#p}) $ e integrando la parte angular, obtenemos:
$$N(#mu_{1},#mu_{2})= #frac{16}{#pi} #int_{0}^{#infty} #int_{0}^{#infty} #frac{#l^{2} #, #exp #left[ -(#mu_{1}^{2}+#mu_{2}^{2})#q_{cm}^{2} -2#l^{2} #right] }{(#mu_{1}-#mu_{2})^{2} |#q_{cm}|^{2}} #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#, #r #, #l #,] }{2} - #frac{1}{2} #right] #, d#r #,d#l   $$
$$=#frac{16}{#pi (#mu_{1}-#mu_{2})^{2}} #int_{0}^{#infty} #l^{2} #, e^{-2#l^{2}} #int_{0}^{#infty} e^{-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2} } #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#, #r #, #l #,] }{2} - #frac{1}{2} #right] #, d#r #, d#l $$
#begin{equation}#label{LnI-2}
= #frac{16}{#pi (#mu_{1}-#mu_{2})^{2}} #int_{0}^{#infty} #l^{2} #, e^{-2#l^{2}} I_{2}(#l ,#mu_{1},#mu_{2})   #, d#l
#end{equation}
en donde,
$$ I_{2}(#l ,#mu_{1},#mu_{2}) := #int_{0}^{#infty} e^{-(#mu_{1}^{2}+#mu_{2}^{2})#r^{2} } #left[ #frac{ #cosh [#, 2(#mu_{1}-#mu_{2})#, #r #, #l #,] }{2} - #frac{1}{2} #right] #, d#r $$
y observamos que este t#'ermino es el mismos que aparece en el desarrollo de $L(#mu_{1},1-#mu_{1})$,
$$I_{2}(#l ,#mu_{1},#mu_{2}) =#frac{1}{4} #sqrt{ #frac{#pi}{#mu_{1}^{2}+#mu_{2}^{2}}} #left[ #exp#left[ #frac{(#mu_{1}-#mu_{2})^{2} #l^{2}}{#mu_{1}^{2}+#mu_{2}^{2}} #right] - 1 #right] $$
Sustituyendo #eqref{I2-res} en la ecuaci#'on #eqref{LnI-2} obtenemos,
$$ N(#mu_{1},#mu_{2})=#frac{4}{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }} #int_{0}^{#infty} #l^{2} #,e^{-2#l^{2}} #left[ #exp#left[ #frac{(#mu_{1}-#mu_{2})^{2} }{#mu_{1}^{2}+#mu_{2}^{2}}#,#l^{2} #right] - 1 #right] #, d#l $$
$$=#frac{4}{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }} #left( #int_{0}^{#infty} #l^{2} #exp#left[ #frac{(#mu_{1}-#mu_{2})^{2} }{#mu_{1}^{2}+#mu_{2}^{2}}#, #l^{2} - 2#l^{2} #right]  d#l - #int_{0}^{#infty} #l^{2} e^{-2#l^{2}} #,d#l #right) $$
utilizando el hecho de que $ #mu_{2}= 1-#mu_{1} $ y desarrollando t#'erminos cuadr#'aticos, es f#'acil ver que,
$$ #frac{(#mu_{1}-#mu_{2})^{2} }{#mu_{1}^{2}+#mu_{2}^{2}} -2= #frac{-1}{#mu_{1}^{2}-#mu_{2}^{2}} $$
As#'i podemos reescribir a $N(#mu_{1},#mu_{2})$,
$$ N(#mu_{1},#mu_{2})= #frac{4}{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }} #left( #int_{0}^{#infty} #l^{2} #exp #left[ #frac{-#l^{2} }{#mu_{1}^{2}-#mu_{2}^{2}} #right]  d#l - #int_{0}^{#infty} #l^{2} e^{-2#l^{2}} #,d#l #right) $$
Luego, por la ecuaci#'on #eqref{A2} 
$$ N(#mu_{1},#mu_{2})= #frac{4}{(#mu_{1}-#mu_{2})^{2} #sqrt{#pi(#mu_{1}^{2}+#mu_{2}^{2}) }}  #left[  #frac{#mu_{1}^{2}+#mu_{2}^{2}}{4}#, #sqrt{#pi (#mu_{1}^{2}+#mu_{2}^{2})} - #frac{1}{8}#sqrt{#frac{#pi}{2}} #, #right] $$
$$= #frac{1}{(#mu_{1}-#mu_{2})^{2}}#, #frac{1}{#sqrt{#mu_{1}^{2}+#mu_{2}^{2}}} #,#left[  (#mu_{1}^{2}+#mu_{2}^{2})^{3/2} - #frac{1}{#sqrt{8}} #, #right]$$ 
utilizando el hecho $ #mu_{2}= 1-#mu_{1} $ y desarrollando t#'erminos cuadr#'aticos  tenemos que,
$$ N(#mu_{1},1-#mu_{1})= #frac{1}{2(2#mu_{1}-1)^{2}}#,#frac{1}{#sqrt{ 1+(2#mu_{1}-1)^{2} }} #left[ #,[ (2#mu_{1}-1)^{2}+1 ]^{3/2} -1 #right] $$

%*******************************************************************

#chapter{C#'alculo de $J #left(#frac{1}{2},#frac{1}{2} #right)$}
Primero, $J#left(#mu_{1},#mu_{2} #right)$ est#'a dada por,
$$J(#mu_{1},#mu_{2})= $$
#begin{align}#label{J0}
#frac{1}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}} 
|#mu_{2}#q_{1}-#mu_{1}#q_{2}|#, #exp #left[ -#frac{#mu_{1}^{2}+#mu_{2}^{2}}{2}#, |#q_{1}+#q_{2}|^{2} - |#mu_{2}#q_{1}-#mu_{1}#q_{2}|^{2}-#frac{|#q_{1}|^{2}}{2}#, #right]      
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{2}}}
#cdot #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }
#,d#q_{1} #right]^{2} d#q_{2}
#end{align}
con $0 #leq #mu_{1} #leq 1$ y $#mu_{2}= 1-#mu_{1}$. Luego tenemos que,
$$ J#left(1/2,1/2 #right)=  $$
#begin{align}#label{j.51}
#frac{1}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}} 
#frac{1}{2}|#q_{1}-#q_{2}|#, #exp #left[ -#frac{1}{4} #,|#q_{1}+#q_{2}|^{2} -#frac{1}{4} |#q_{1}-#q_{2}|^{2}-#frac{|#q_{1}|^{2}}{2}#, #right]      
#right.
#nonumber##
#qquad #left. #vphantom{#int_{#R^{2}}}
#cdot #lim_{(#mu_{1}-#mu_{2})#rightarrow 0} #left[ #frac{ #senh #left[ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| #, #right]}{ (#mu_{1}-#mu_{2})#, |#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| }  #right]
#,d#q_{1} #right]^{2} d#q_{2}
#end{align}
Haciendo $x=( #mu_{1}-#mu_{2} )$ y $#v{A} =|#q_{1}+#q_{2}|#cdot |#mu_{2}#q_{1}-#mu_{1}#q_{2}| $, tenemos por la regla de L'Hopital,
#begin{equation}#label{shl}
#lim_{x #rightarrow 0} #frac{#senh(x #v{A})}{x #v{A}}= #frac{#v{A} #cosh(x #v{A})}{#v{A}} #Big|_{x=0}=1
#end{equation}
Luego, por la ley del paralelogramo tenemos que,
#begin{equation}#label{par}
-#frac{1}{4} #left( |#q_{1} + #q_{2}|^{2} + |#q_{1} - #q_{2}|^{2} #right)= -#frac{1}{2} #left(|#q_{1}|^{2} + |#q_{2}|^{2}  #right)
#end{equation}
As#'i, sustituyendo las ecuaciones #eqref{shl} y #eqref{par} en la ecuaci#'on #eqref{j.51} obtenemos,
$$ J(1/2,1/2)= #frac{1}{#pi^{9/2}} #int_{#R^{3}} #left[#int_{#R^{3}} #frac{|#q_{1}-#q_{2}|}{2}#, #exp #left[ -|#q_{1}|^{2} -#frac{|#q_{2}|^{2}}{2} #right] #,d#q_{1} #right]^{2} d#q_{2} $$
#begin{equation}#label{j.52}
=#frac{1}{4 #pi^{9/2}} #int_{#R^{3}} e^{-|#q_{2}|^{2}} #left[#int_{#R^{3}} |#q_{1} - #q_{2}|#, e^{-|#q_{1}|^{2}} #,d#q_{1} #right]^{2}  #,d#q_{2}
#end{equation} 
denotando a $g(#q_{2})$  como,
#begin{equation}#label{gq2}
g(#q_{2})= #int_{#R^{3}} |#q_{1}-#q_{2}|#, e^{-|#q_{1}|^{2}}#, d#q_{1}
#end{equation}
podemos reescribir a #eqref{j.52} de la forma,
#begin{equation}#label{JGq}
J(1/2,1/2)= #frac{1}{4 #pi^{9/2}} #int_{#R^{3}} e^{-|#q_{2}|^{2}} g^{2}(#q_{2})  #,d#q_{2}
#end{equation}
En la siguiente secci#'on realizamos el c#'alculo de $g(#q_{2})$.##

#section{C#'alculo de $g(#q_{2})$}
Primero, tomemos un sistema de referencia en $#R^{3}$ tal que el vector $#q_{2}$ sea paralelo al eje $z$ y este dado en coordenadas cartesianas por $#q_{2}=(0,0,q_{2})$, es claro que $|#q_{2}|=q_{2}$. Denotamos por $#q_{0} := #q_{1} - #q_{2}$ y es claro que,
#begin{equation}
|#q_{1}|^{2}=(#q_{0}+#q_{2}) #cdot (#q_{0}+#q_{2}) =|#q_{0}|^{2} + 2#, #q_{0} #cdot #q_{2} + q_{2}^{2}
#end{equation}
m#'as aun, es claro que el Jacobiano de la transformaci#'on $ #q_{1} #rightarrow #q_{0} + #q_{2} $ es uno. As#'i haciendo el cambio de variable $ #q_{1} #rightarrow #q_{0} + #q_{2} $ en la ecuaci#'on #eqref{gq2} obtenemos,
#begin{equation}
g(#q_{2})= e^{-q_{2}^{2}} #int_{#R^{3}} |#q_{0}|#, e^{-|#q_{0}|^{2} -2 #q_{0} #cdot #q_{2}} #,d#q_{0}
#end{equation}
Ahora resolvamos esta ecuaci#'on.##
Haciendo el cambio a coordenadas esf#'ericas $#q_{0} #rightarrow (#r,#t,#p)$, tenemos por la ecuaci#'on #eqref{A8},
$$ g(#q_{2})= e^{-q_{2}^{2}} #int_{0}^{#infty} #int_{0}^{2#pi} #int_{0}^{#pi} #r^{3} #, #exp[-#r^{2}-2q_{2} #r #cos(#t)] #,  #sen(#t) #, d#t #, d#p #, d#r $$
$$= 2#pi #, e^{-q_{2}^{2}} #int_{0}^{#infty} #r^{3}e^{-#r^{2}} #left[#int_{0}^{#pi} e^{-2q_{2} #r #cos(#t)} #sen(#t) #, d#t #right] #, d#r= #frac{#pi}{q_{2}} #, e^{-q_{2}^{2}} #int_{0}^{#infty} #r^{2} e^{-#r^{2}} #left[ e^{2q_{2}#,#r} - e^{-2q_{2}#,#r} #right]#, d#r  $$
#begin{equation}#label{gq22}
#frac{#pi}{q_{2}} #, e^{-q_{2}^{2}} #int_{0}^{#infty} e^{q_{2}^{2}}#, #left[ #r^{2} #, e^{-(#r - q_{2})^{2}} - #r^{2} #, e^{-(#r + q_{2})^{2}} #right] = #frac{#pi}{q_{2}} #left[ #int_{0}^{#infty} #r^{2} #, e^{-(#r - q_{2})^{2}} - #int_{0}^{#infty} #r^{2} #, e^{-(#r + q_{2})^{2}} #right]
#end{equation}
Ahora resolvamos la integral $#int_{0}^{#infty} #r^{2} #, e^{-(#r - q_{2})^{2}}$. Haciendo el cambio de variable $(#r-#q_{2}) #rightarrow #l$, tenemos que,
$$ #int_{0}^{#infty} #r^{2} #, e^{-(#r - q_{2})^{2}} d#r = #int_{-q_{2}}^{#infty} (#l + q_{2})^{2}#, e^{-#l^{2}} d#l = #int_{-q_{2}}^{#infty} (#l^{2}+2q_{2}#l+q_{2}^{2}) #, e^{-#l^{2}} d#l $$
#begin{equation}#label{ints}
= #int_{-q_{2}}^{#infty}#l^{2} #, e^{-#l^{2}} d#l + 2q_{2} #int_{-q_{2}}^{#infty} #l #, e^{-#l^{2}} d#l + q_{2}^{2} #int_{-q_{2}}^{#infty}  e^{-#l^{2}} d#l
#end{equation}
Resolvemos la primera de las integrales de #eqref{ints} por partes,
$#l #, e^{-#l^{2}}= #frac{d}{d#l}#left[ -#frac{1}{2}#, e^{-#l^{2}} #right] $, de la siguiente forma:
$$ #int_{-q_{2}}^{#infty} #l^{2} #, e^{-#l^{2}} d#l = #int_{-q_{2}}^{#infty} #l #, #frac{d}{d#l}#left[ -#frac{1}{2}#, e^{-#l^{2}} #right]  #, d#l = -#frac{#l}{2}#, e^{-#l^{2}} #Big|_{-q_{2}}^{#infty} + #frac{1}{2} #int_{-q_{2}}^{#infty} e^{-#l^{2}} $$
$$ = -#frac{q_{2}}{2}#, e^{-q_{2}^{2}} + #frac{1}{2} #int_{0}^{q_{2}} e^{-#l^{2}} d#l + #frac{1}{2} #int_{0}^{#infty} e^{-#l^{2}} d#l $$
Luego por la ecuaci#'on #eqref{A1} tenemos que,
#begin{equation}#label{ints1}
#int_{-q_{2}}^{#infty} #l^{2} #, e^{-#l^{2}} d#l =  -#frac{q_{2}}{2}#, e^{-q_{2}^{2}} + #frac{1}{2} #int_{0}^{q_{2}} e^{-#l^{2}} d#l + #frac{#sqrt{#pi}}{4}
#end{equation}
As#'i tambi#'en, la segunda integral de #eqref{ints} se sigue directamente del hecho de que, $#l #, e^{-#l^{2}}= #frac{d}{d#l}#left[ -#frac{1}{2}#, e^{-#l^{2}} #right] $:
#begin{equation}#label{ints2}
2q_{2} #int_{-q_{2}}^{#infty} #l #, e^{-#l^{2}} d#l = - q_{2}#, e^{-#l^{2}} #Big|_{-q_{2}}^{#infty} = q_{2}#, e^{-q_{2}^{2}}
#end{equation}
y para la tercera integral tenemos, por la ecuaci#'on #eqref{A1}:
#begin{equation}#label{ints3}
q_{2}^{2} #int_{-q_{2}}^{#infty}  e^{-#l^{2}} d#l = q_{2}^{2}#, #left[ #int_{0}^{q_{2}} e^{-#l^{2}}#, d#l + #int_{0}^{#infty} e^{-#l^{2}}#, d#l #right]= q_{2}^{2} #int_{0}^{q_{2}} e^{-#l^{2}}#, d#l + #frac{q_{2}^{2}}{2} #sqrt{#pi} 
#end{equation}
As#'i, por #eqref{erf}, #eqref{ints1}, #eqref{ints2} y #eqref{ints3} tenemos que:
#begin{equation}#label{rho-}
#int_{0}^{#infty} #r^{2} #, e^{-(#r - q_{2})^{2}} d#l = #frac{q_{2}^{2}}{2} #, e^{-q_{2}^{2}} +#frac{#sqrt{#pi}}{2}#, #erf (q_{2}) #left[ #frac{1}{2}+q_{2}^{2} #right] + #frac{#sqrt{#pi}}{4}+ q_{2}^{2}#, #frac{#sqrt{#pi}}{2}
#end{equation}
De manera an#'aloga obtenemos que,
#begin{equation}#label{rho+}
#int_{0}^{#infty} #r^{2} #, e^{-(#r + q_{2})^{2}} d#r = -#frac{q_{2}^{2}}{2} #, e^{-q_{2}^{2}} -#frac{#sqrt{#pi}}{2}#, #erf (q_{2}) #left[ #frac{1}{2}+q_{2}^{2} #right] + #frac{#sqrt{#pi}}{4}+ q_{2}^{2}#, #frac{#sqrt{#pi}}{2}
#end{equation}
Luego, sustituyendo #eqref{rho-} y #eqref{rho+} en #eqref{gq22}, obtenemos finalmente que,
#begin{equation}#label{gq2-f}
g(#q_{2})= #frac{#pi^{3/2}}{2#,q_{2}}#, #erf (q_{2}) [ 1+2q_{2}^{2} ] + #pi e^{-q_{2}^{2}}
#end{equation}

#section{C#'alculo de $J#left(#frac{1}{2},#frac{1}{2} #right)$}

Sustituyendo #eqref{gq2-f} en la ecuaci#'on #eqref{JGq}, tenemos que,
$$ J#left( 1/2,1/2 #right)= #frac{1}{4#pi^{9/2}} #int_{#R^{3}} e^{-|#q_{2}|^{2}} #left[ #frac{#pi^{3/2}}{2#, |#q_{2}|}#, #erf (#q_{2}) [ 1+2|#q_{2}|^{2} ] + #pi e^{-|#q_{2}|^{2}} #right]^{2} #, d#q_{2} $$ 
M#'as aun, haciendo el cambio a coordenadas esf#'ericas $#q_{2} #rightarrow (#r,#t,#p)$ e integrando la parte angular obtenemos,
$$ J#left( 1/2,1/2 #right)= #frac{1}{4#pi^{9/2}} #int_{0}^{#infty} #int_{0}^{2#pi} #int_{0}^{#pi} e^{-#r^{2}} #left[ #frac{#pi^{3/2}}{2#r}#, #erf (#r)#, [ 1+2#r^{2}] +#pi e^{-#r^{2}}  #right]^{2} #r^{2} #sen(#t) #, d#t #, d#p #, d#r $$
$$= #frac{1}{#pi^{7/2}} #int_{0}^{#infty} #r^{2}#, e^{-#r^{2}} #left[ #frac{#pi^{3/2}}{2#r}#, #erf (#r)#, [ 1+2#r^{2}] +#pi e^{-#r^{2}}  #right]^{2}  #, d#r$$
Luego, desarrollando el t#'ermino cuadr#'atico y separando t#'erminos,
#begin{align}
J#left( 1/2,1/2 #right) = #frac{1}{4#sqrt{#pi}} &#int_{0}^{#infty} e^{-#r^{2}}#cdot #erf^{2}(#r)#, ( 1+4#r^{2}+4#r^{4} )#, d#r #label{J1} ##
+ #frac{1}{#pi} &#int_{0}^{#infty} #r #, e^{-2#r^{2}} #cdot #erf(#r)#, ( 1+2#r^{2})#, d#r #label{J2} ##
+ #frac{1}{#pi^{3/2}} &#int_{0}^{#infty} #r^{2} e^{-3#r^{2}} #, d#r #label{J3}
#end{align}
Ahora desarrollemos las integrales #eqref{J1}, #eqref{J2}, #eqref{J3}.##
#paragraph{Integral #eqref{J1}}
Es claro que,
$$ #frac{1}{4#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}}#cdot #erf^{2}(#r)#, ( 1+4#r^{2}+4#r^{4} )#, d#r $$
#begin{subequations}
#begin{align}
&=#frac{1}{4#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}}#cdot #erf^{2}(#r)#, d#r #label{J1A} ##
&+#frac{1}{#sqrt{#pi}} #int_{0}^{#infty} #r^{2} #,e^{-#r^{2}}#cdot #erf^{2}(#r)#, d#r #label{J1B} ##
&+#frac{1}{#sqrt{#pi}} #int_{0}^{#infty} #r^{4} #,e^{-#r^{2}}#cdot #erf^{2}(#r)#, d#r #label{J1C}
#end{align}
#end{subequations}
Para #eqref{J1A} tenemos por las ecuaciones #eqref{A1}, #eqref{erf-2} y #eqref{A5},
$$ #frac{1}{4#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}}#cdot #erf^{2}(#r)#, d#r = #frac{1}{4#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}} #left[1-#frac{4}{#pi} #int_{0}^{1} #frac{e^{-#r^{2}(y^{2}+1)}}{y^{2}+1}#, dy #right]  d#r  $$
$$= #frac{1}{4#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}} d#r -#frac{1}{#pi^{3/2}} #int_{0}^{1} #int_{0}^{#infty} #frac{e^{-#r^{2}(y^{2}+1)}}{y^{2}+1}#, d#r #, dy $$
$$ = #frac{1}{8} - #frac{1}{2#pi} #int_{0}^{1} #frac{1}{y^{2}+1}#cdot #frac{1}{#sqrt{y^{2}+2}}#, dy = #frac{1}{8}- #frac{1}{2#pi} #left[ #arctan#left( #frac{1}{2-#sqrt{3}} -#frac{#pi}{4} #right) #right] $$
#begin{equation}#label{r1}
= #frac{1}{8} - #frac{1}{12}
#end{equation}
Para #eqref{J1B} tenemos por las ecuaciones #eqref{A2}, #eqref{erf-2} y #eqref{A6},
$$ #frac{1}{#sqrt{#pi}} #int_{0}^{#infty} #r^{2} #,e^{-#r^{2}}#cdot #erf^{2}(#r)#, d#r= #frac{1}{#sqrt{#pi}} #int_{0}^{#infty}#r^{2} e^{-#r^{2}} #left[1-#frac{4}{#pi} #int_{0}^{1} #frac{e^{-#r^{2}(y^{2}+1)}}{y^{2}+1}#, dy #right]  d#r $$
$$ = #frac{1}{#sqrt{#pi}} #int_{0}^{#infty} #r^{2} e^{-#r^{2}} d#r -#frac{4}{#pi^{3/2}} #int_{0}^{1} #int_{0}^{#infty} #frac{#r^{2} e^{-#r^{2}(y^{2}+1)}}{y^{2}+1}#, d#r #, dy $$
$$= #frac{1}{4} -#frac{1}{#pi} #int_{0}^{1} #frac{1}{y^{2}+1}#cdot #frac{1}{(y^{2}+2)^{3/2}} = #frac{1}{4} -#frac{1}{#pi} #left[ #arctan #left( #frac{1}{#sqrt{3}} #right) - #frac{1}{2#sqrt{3}} #right] $$
#begin{equation}#label{r2}
= #frac{1}{4} -#frac{1}{6} + #frac{#sqrt{3}}{6#pi}
#end{equation}
Para #eqref{J1C} tenemos por las ecuaciones #eqref{A2.1}, #eqref{erf-2} y #eqref{A7},
$$ #frac{1}{#sqrt{#pi}} #int_{0}^{#infty} #r^{4} #,e^{-#r^{2}}#cdot #erf^{2}(#r)#, d#r =#frac{1}{#sqrt{#pi}} #int_{0}^{#infty}#r^{4} e^{-#r^{2}} #left[1-#frac{4}{#pi} #int_{0}^{1} #frac{e^{-#r^{2}(y^{2}+1)}}{y^{2}+1}#, dy #right]  d#r  $$
#begin{equation}#label{r3}
= #frac{3}{8} -#frac{3}{2#pi} #int_{0}^{1} #frac{1}{y^{2}+1}#cdot #frac{1}{(y^{2}+2)^{5/2}} = #frac{3}{8} - #frac{27}{108} + #frac{39 #sqrt{3}}{108 #pi}
#end{equation}
As#'i, sumando #eqref{r1}, #eqref{r2} y #eqref{r3} obtenemos el valor de la integral #eqref{J1},
#begin{equation}#label{Jr1}
#frac{1}{4#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}}#cdot #erf^{2}(#r)#, ( 1+4#r^{2}+4#r^{4} )#, d#r= #frac{1}{4} + #frac{19}{12 #sqrt{3}#, #pi} 
#end{equation}

#newpage

#paragraph{Integtral #eqref{J2}}
Es claro que,
$$ #frac{1}{#pi} #int_{0}^{#infty} #r #, e^{-2#r^{2}} #cdot #erf(#r)#, ( 1+2#r^{2})#, d#r $$
#begin{subequations}
#begin{align}
&=#frac{1}{#pi} #int_{0}^{#infty} #r #, e^{-2#r^{2}} #cdot #erf(#r)#,d#r #label{J2A}##
&+#frac{2}{#pi} #int_{0}^{#infty} #r^{3} e^{-2#r^{2}} #cdot #erf(#r)#,d#r #label{J2B}
#end{align}
#end{subequations}
Integramos a #eqref{J2A} por partes, as#'i usando la ecuaci#'on #eqref{A1} tenemos que,
$$ #frac{1}{#pi} #int_{0}^{#infty} #r #, e^{-2#r^{2}} #cdot #erf(#r)#,d#r= #frac{1}{#pi} #int_{0}^{#infty} #erf(#r) #cdot #frac{d}{d#r} #left[ -#frac{1}{4} e^{-2#r^{2}} #right] d#r $$
#begin{equation}#label{r2-1}
=-#frac{#erf(#r)}{4#pi}#, e^{-2#r^{2}} #Big|_{0}^{#infty} + #frac{1}{2#pi^{3/2}} #int_{0}^{#infty} e^{-3#r^{2}} d#r = #frac{1}{4#sqrt{3}#pi}
#end{equation}
As#'i tambi#'en, integramos a #eqref{J2B} por partes. Usando la ecuaci#'on #eqref{A2} tenemos que,
$$ #frac{2}{#pi} #int_{0}^{#infty} #r^{3} e^{-2#r^{2}} #cdot #erf(#r)#,d#r= #frac{2}{#pi} #int_{0}^{#infty} #r^{2}#, #erf(#r)#,#frac{d}{d#r} #left[ -#frac{1}{4}#, e^{-2#r^{2}} #right]#, d#r $$
$$ = -#frac{#r^{2}#,#erf(#r)}{2#pi}#, e^{-#r^{2}} #Big|_{0}^{#infty} + #frac{1}{2#pi} #int_{0}^{#infty} e^{-2#r^{2}} #left[ 2#r#,#erf(#r)+#frac{2#r^{2}}{#sqrt{#pi}}#, e^{-#r^{2}} #right]#, d#r  $$
$$ #frac{1}{#pi} #int_{0}^{#infty} #erf(#r)#, #r#,e^{-2#r^{2}} d#r + #frac{1}{#pi^{3/2}} #int_{0}^{#infty} #r^{2} e^{-3#r^{2}}#, d#r $$
$$ = #frac{1}{#pi} #int_{0}^{#infty} #erf(#r) #cdot #frac{d}{d#r}#left[ -#frac{1}{4}#, e^{-2#r^{2}} #right]#, d#r + #frac{1}{12#sqrt{3}#pi}  $$
$$= -#frac{#erf(#r)}{4#pi}#, e^{-2#r^{2}} #Big|_{0}^{#infty} +#frac{1}{2#pi^{3/2}}#int_{0}^{#infty} e^{-3#r^{2}}#, d#r  + #frac{1}{12#sqrt{3}#pi} $$
#begin{equation}#label{r2-2}
= #frac{1}{4#sqrt{3}#pi}+ #frac{1}{12#sqrt{3}#pi} = #frac{1}{3#sqrt{3}#pi}
#end{equation}
Luego, sumando #eqref{r2-1} y #eqref{r2-2} obtenemos el valor de la integral #eqref{J2},
#begin{equation}#label{Jr2}
#frac{1}{#pi} #int_{0}^{#infty} #r #, e^{-2#r^{2}} #cdot #erf(#r)#, ( 1+2#r^{2})#, d#r= #frac{7}{12#sqrt{3}#pi}
#end{equation}

#paragraph{Integtral #eqref{J3}}
Esta integral se sigue directamente de la ecuaci#'on #eqref{A2},
#begin{equation}#label{Jr3}
#frac{1}{#pi^{3/2}} #int_{0}^{#infty} #r^{2} e^{-3#r^{2}} #, d#r= #frac{1}{12#sqrt{3}#pi}
#end{equation}
De esta forma, sumando #eqref{Jr1}, #eqref{Jr2} y #eqref{Jr3} obtenemos el valor de $J(1/2,1/2)$:
$$J(1/2,1/2)= #frac{1}{4} + #frac{19}{12 #sqrt{3}#, #pi} + #frac{7}{12#sqrt{3}#pi}+#frac{1}{12#sqrt{3}#pi} $$
#begin{equation}
= #frac{1}{4}+#frac{3#sqrt{3}}{4#pi} = 0.663497
#end{equation}

%********************************************************************

#chapter{C#'alculo J(1,0)}
Por la ecuaci#'on #eqref{J0}, tenemos que $J(1,0)$ est#'a dada por,
$$J(1,0)= $$
#begin{equation}#label{J-10}
#frac{1}{#pi^{9/2}} #int_{#R^{3}} #left[ #int_{#R^{3}} |#q_{2}| #exp #left[ -#frac{|#q_{1}+#q_{2}|^{2}}{2} -|#q_{2}|^{2} -#frac{|#q_{1}|^{2}}{2} #right]#, #frac{#senh #left[ |#q_{1}+#q_{2}|#cdot |#q_{2}| #right] }{|#q_{1}+#q_{2}|#cdot |#q_{2}|} #,d#q_{1} #right]^{2} d#q_{2}
#end{equation}
Denotando por $h(#q_{2})$ a,
#begin{equation}#label{hq2}
h(#q_{2}):= #int_{#R^{3}} #exp #left[-#frac{|#q_{1}+#q_{2}|^{2}}{2} -#frac{|#q_{1}|^{2}}{2} #right] #, #frac{ e^{|#q_{1}+#q_{2}|#cdot|#q_{2}|} - e^{-|#q_{1}+#q_{2}|#cdot|#q_{2}|} }{|#q_{1}+#q_{2}|#cdot|#q_{2}|}
#end{equation}
podemos reescribir a #eqref{J-10} como:
#begin{equation}#label{Jh}
J(1,0)= #frac{1}{4#pi^{9/2}} #int_{#R^{3}} h^{2}(#q_{2}) #, e^{-2|#q_{2}|^{2}} |#q_{2}|^{2}#, d#q_{2}
#end{equation}
Ahora calculemos $h(#q_{2})$

#section{C#'alculo de $h(#q_{2})$}
Denotando, 
$$#q_{0}:= #q_{1}+#q_{2} $$
$$|#q_{0}|=q_{0}$$ 
$$|#q_{2}|=q_{2}$$ 
es claro que, $ |#q_{1}|^{2}= q_{0}^{2} - 2#q_{0}#cdot #q_{2} + q_{2}^{2}  $.##
Haciendo en cambio de variable $#q_{1} #rightarrow #q_{0}-#q_{2} $, en #eqref{hq2} obtenemos,
#begin{equation}#label{hq0}
h(#q_{2})= e^{-q_{2}^{2}/2} #int_{#R^{3}} e^{-q_{0}^{2}} #, #left[ #frac{e^{q_{0}#,q_{2}} - e^{-q_{0}#,q_{2}} }{q_{0}#, q_{2}} #right] e^{#q_{0}#cdot #q_{2}}#, d#q_{0}
#end{equation}
Tomando un sistema de referencia en el que $#q_{2}$ sea paralelo al eje $z$ y haciendo el cambio a coordenadas esf#'ericas $ #q_{0} #rightarrow (#r,#t,#p) $, tenemos que #eqref{hq0} se puede reescribir como,
#begin{equation}
h(#q_{2})= e^{-q_{2}^{2}/2} #int_{0}^{#infty} #int_{0}^{2#pi} #int_{0}^{#pi} e^{-#r^{2}}#, #left[ #frac{e^{#r#, #q_{2}} - e^{#r#, #q_{2}} }{#r #, #q_{2}} #right]#, e^{#r #, q_{2} #cos(#t)} #, #r^{2} #sen(#t) #, d#t #, d#p #, d#r
#end{equation}
Integrando respecto a $#p$ y por las ecuaciones #eqref{A1}, #eqref{A3} y #eqref{A8}, tenemos que, 
$$ h(#q_{2})= 2#pi #, e^{-q_{2}^{2}/2} #int_{0}^{#infty} e^{-#r^{2}}#, #left[ #frac{e^{#r#, q_{2}} - e^{-#r#, q_{2}} }{q_{2}} #right]^{2} d#r = #frac{2#pi}{q_{2}^{2}} #, e^{-q_{2}^{2}/2} #int_{0}^{#infty} e^{-#r^{2}}#, #left[ e^{2#r#, q_{2}} + e^{-2#r#, q_{2}} -2 #right] #, d#r $$
$$=#frac{2#pi}{q_{2}^{2}} #, e^{-q_{2}^{2}/2} #left[#int_{0}^{#infty} e^{-#r^{2}}#, e^{2#r q_{2}}#, d#r + #int_{0}^{#infty} e^{-#r^{2}}#, e^{-2#r q_{2}}#, d#r - #int_{-#infty}^{#infty} e^{-#r^{2}}#, d#r #right] $$
#begin{equation}#label{hq2r}
=#frac{2#pi}{q_{2}^{2}} #, e^{-q_{2}^{2}/2} #left[#int_{-#infty}^{#infty} e^{-#r^{2}}#, e^{2#r q_{2}}#, d#r - #int_{-#infty}^{#infty} e^{-#r^{2}}#, d#r #right]= #frac{2 #pi^{3/2}}{q_{2}^{2}} #, #left[e^{q_{2}^{2}/2} - e^{-q_{2}^{2}/2}  #right]
#end{equation}
Por #eqref{hq2r} tenemos,
#begin{equation}#label{hq22}
h^{2}(#q_{2})=#frac{4#pi^{3}}{q_{2}^{4}} #, #left[ e^{q_{2}^{2}} + e^{-q_{2}^{2}} -2 #right]
#end{equation}

#section{C#'alculo de $J(1,0)$}
Sustituyendo #eqref{hq22} en #eqref{Jh} obtenemos,
$$ J(1,0)= #frac{1}{#pi^{3/2}} #int_{#R^{3}} #frac{ e^{-2q_{2}^{2}} }{q_{2}^{2}} #left[ e^{q_{2}^{2}} + e^{-q_{2}^{2}} -2 #right]#, d#q_{2}    $$
Haciendo el cambio a coordenadas esf#'ericas $#q_{2} #rightarrow (#r,#t,#p)$ e integrando la parte angular, tenemos que,
$$ J(1,0)= #frac{4}{#sqrt{#pi}} #int_{0}^{#infty} e^{-#r^{2}} + e^{-3#r^{2}} - 2e^{-2#r^{2}} #, d#r $$
Por la ecuaci#'on #eqref{A1} tenemos que,
$$J(1,0)= #frac{4}{#sqrt{#pi}} #left[ #frac{#sqrt{#pi}}{2} +#frac{1}{2}#, #sqrt{#frac{#pi}{3}} -#sqrt{#frac{#pi}{2}} #right] $$
#begin{equation}#label{J10va}
=2 #left[ 1+#frac{1}{#sqrt{3}} -#sqrt{2} #right]= 0.32627
#end{equation}
#textcolor{red}{Completa las referencias, con n#'umero inicial y final de p#'aginas o n#'umero de art#'iculo. El anio entre par#'entisis}
#part{Referencias}

#n [1]: R.Weder, #emph{Entanglement creation in low-energy scattering}, ##
arXiv:1109.4005, Physical Review A Vol.84 2011.##

#n [2]: Einstein A, Podolsky B, Rosen N (1935). ##
#emph{Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?}. ##
Phys. Rev. 47.##

#n [3]: Schrodinger E; Born, M. (1935). #emph{Discussion of probability relations between separated systems}.##
Mathematical Proceedings of the Cambridge Philosophical Society 31.##

#n [4]: J. S. Bell (1964). #emph{On the Einstein- Poldolsky-Rosen paradox}. Physics.##

#n [5]: Freedman, Stuart J.; Clauser, John F. (1972). #emph{Experimental Test of Local Hidden-Variable Theories}. Physical Review Letters 28 (14).##

#n [6]: A. Aspect, P. Grangier, and G. Roger (1982). #emph{Experimental Realization of Einstein-Podolsky-Rosen-Bohm Gedankenexperiment: A New Violation of Bell's Inequalities}. ##
Physical Review Letters 49 (2).##

#n [7]: W. Rudin, #emph{Principles of Mathematical Analysis}, 2nd ed. McGraw-Hill Kogakusha, Japan (1964).##

#n [8]: M. Reed, B.Simon, #emph{Mehods of Mothern Mathematical Physiscs I. Functional analysis},## 
Academic Press, San Diego (1980).##

#n [9]: G. Teschl, #emph{Mathematical Methods in Quantum Mechanics}, American Mathematical## Society, Rhode island (2009).##

#n [10]: M.S. Birman, M.Z.Solomjak, #emph{Spectral Theory of Self-Adjoint Operators in Hilbert Space}, D.Reidel Publishing Company, Holland (1987).##

#n [11]: H. Brezis, #emph{Functional Analysis, Sobolev Spaces and Partial Differential Equations},## Springer, New York (2011).##

#n [12]: B. Thaller, #emph{Visual Quantum Mechanics, Springer-Verlag}, New York (2000).##

#n [13]: B. Thaller, #emph{Advanced Visual Quantum Mechanics}, Springer, New York (2005).##

#n [14]: Cohen-Tannoudji, B.Diu, F.Laloe, #emph{Quantum Mechanics Vol. 1}, Wiley, New York (1978).##

#n [15]: S.T.Thornton, J.B.Marion, #emph{Classical Dynamics of Particles and Systems 5th ed};## Thomson Brooks/Cole, Belmont CA. (2004).##

#n [16]: J.Stoer, R.Bulirsch, #emph{Introduction to Numerical Analysis 2nd ed};## 
Springer Verlag, New York (1993).##

#n [17]: W.O.Amrein, J.M. Jauch, K.B.Sinha, #emph{Scattering Theory in Quantum Mechanics}, W.A.## Benajmin Inc. New York (1977).##

#n [18]: A.Jensen and T.Kato, Duke Math. J. 46, 583 (1979).##

#n [19]: H.L.Royden, Real Analysis, Macmillan, New York (1968).##

#n [20]: R.A.Adams, J.J.Founier, #emph{Sobolev Spaces 2nd ed};## 
Elsevier-Academic Prees, Amsterdam (2003).##

#n [21]: M.Schechter, #emph{Spectra of Partial Differential Operators 2nd ed};## 
North Holland, Amsterdam (1986).##

#n [22]:  A.Einstein #emph{#"Uber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}, (#emph{On a Heuristic Point of View about the Creation and Conversion of Light}),  Annalen der Physik 17 (1905).##

#n [23]: Davisson, C.J. #emph{The Diffraction of Electrons by a Crystal of Nickel}. Bell System Tech. J. USA (1928).##

#n [24]: L. de Broglie, #emph{Recherches sur la th#'eorie des quanta} (#emph{Researches on the quantum theory}), Thesis, Paris (1924).##

#n [25]: W. Heisenberg, #emph{#"Uber den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik} (#emph{On the ideological content of quantum theoretical kinematics and mechanics}), Zeitschrift f#"ur Physik 43 (1927).##

#n [26]: Laue, Max von #emph{R#"ontgenstrahlinterferenzen} (#emph{X-ray interference}), Physikalische Zeitschrift 14 (22/23) (1913).##

#n [27]: M. Born, #emph{Zur Quantenmechanik der Sto$#beta$vorg#"ange} (#emph{The quantum mechanics of the collision processes}), Zeitschrift f#"ur Physik 37 (1926).##

#n [28]: E. Schr#"odinger, #emph{An Undulatory Theory of the Mechanics of Atoms and Molecules},  Physical Review 28 (1926).